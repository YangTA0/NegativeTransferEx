{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import copy\n",
    "from line_profiler import LineProfiler\n",
    "from option_critic.utils import *\n",
    "from matplotlib import pyplot as plt\n",
    "from option_critic.fourrooms import FourRooms\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | :\u001b[43m \u001b[0m: |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nHere\\'s our restructured problem statement (from Gym docs):\\n\\n\"There are 4 locations (labeled by different letters), and our job is to pick up the passenger \\nat one location and drop him off at another. We receive +20 points for a successful drop-off and \\nlose 1 point for every time-step it takes. There is also a 10 point penalty for illegal pick-up \\nand drop-off actions.\"\\n\\n- The filled square represents the taxi, which is yellow without a passenger and green with a passenger.\\n- The pipe (\"|\") represents a wall which the taxi cannot cross.\\n- R, G, Y, B are the possible pickup and destination locations. The blue letter represents the current passenger\\n  pick-up location, and the purple letter is the current destination.\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#[env.id for env in gym.envs.registry.all() if env.id.startswith('Taxi')]\n",
    "import Taxi_v0, Taxi_v1, Taxi_v4\n",
    "import Taxi_v00, Taxi_v01, Taxi_v03, Taxi_v04\n",
    "import Taxi_v000,Taxi_v001,Taxi_v003,Taxi_v004\n",
    "env = gym.make(\"Taxi-v3\")\n",
    "env.render()\n",
    "\"\"\"\n",
    "Here's our restructured problem statement (from Gym docs):\n",
    "\n",
    "\"There are 4 locations (labeled by different letters), and our job is to pick up the passenger \n",
    "at one location and drop him off at another. We receive +20 points for a successful drop-off and \n",
    "lose 1 point for every time-step it takes. There is also a 10 point penalty for illegal pick-up \n",
    "and drop-off actions.\"\n",
    "\n",
    "- The filled square represents the taxi, which is yellow without a passenger and green with a passenger.\n",
    "- The pipe (\"|\") represents a wall which the taxi cannot cross.\n",
    "- R, G, Y, B are the possible pickup and destination locations. The blue letter represents the current passenger\n",
    "  pick-up location, and the purple letter is the current destination.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70744e7a9cf346c19054a77106cda458",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "goals = [(3,2), (3,9), (9,2), (9,9)]\n",
    "four_room_envs =[None] * 4\n",
    "for i in range(len(goals)):\n",
    "    four_room_envs[i] = FourRooms()\n",
    "    four_room_envs[i].reset()\n",
    "    four_room_envs[i].goal =  four_room_envs[i].tostate[goals[i]]\n",
    "    clear_output(True)\n",
    "    fig = plt.subplot(2,2,i+1)\n",
    "    plt.imshow(four_room_envs[i].render(show_goal=True), cmap='Blues')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__enter__', '__eq__', '__exit__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_elapsed_steps', '_max_episode_steps', 'action_space', 'class_name', 'close', 'compute_reward', 'env', 'metadata', 'observation_space', 'render', 'reset', 'reward_range', 'seed', 'spec', 'step', 'unwrapped']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"Taxi-v003\")\n",
    "dir(env)\n",
    "print(dir(env))\n",
    "hasattr(env, 's')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arg_max(state_action):\n",
    "    max_index_list = []\n",
    "    max_value = state_action[0]\n",
    "    for index, value in enumerate(state_action):\n",
    "        if value > max_value:\n",
    "            max_index_list.clear()\n",
    "            max_value = value\n",
    "            max_index_list.append(index)\n",
    "        elif value == max_value:\n",
    "            max_index_list.append(index)\n",
    "    return random.choice(max_index_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_zero_state(q_table):\n",
    "    zero_state = 0\n",
    "    for arr_2d in q_table:\n",
    "         zero_state += np.sum(np.sum(arr_2d,axis = 1) == 0)\n",
    "    return zero_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R:\u001b[43m \u001b[0m| :G: |\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gym.make(\"Taxi-v004\").render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35m\u001b[43mR\u001b[0m\u001b[0m: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gym.make(\"Taxi-v003\").render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "| : | : : |\n",
      "| :\u001b[35mR\u001b[0m| :\u001b[34;1mG\u001b[0m: |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "| |Y: | :B|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gym.make(\"Taxi-v1\").render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | :\u001b[35mG\u001b[0m: |\n",
      "| : | : : |\n",
      "| : : :\u001b[43m \u001b[0m: |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gym.make(\"Taxi-v4\").render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODE_NUM = 4000\n",
    "MAX_EPISODE_LEN = 200\n",
    "REPEAT_TIMES = 5  # train agent REAPEAT_TIMES to get averaged learning curves\n",
    "EVALUATION_TIMES = 1 # evaluate target policy EVALUATION_TIMES after x updates in off-policy RL algorithms\n",
    "CLIST = ['b','c', 'g', 'k','m', 'r', 'y', 'w'] #colors to plot learning curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "|\u001b[43m \u001b[0m: | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "\n",
      "Action space = Discrete(6)\n",
      "State space = Discrete(500)\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "env.render()\n",
    "print(\"Action space = {}\".format(env.action_space))\n",
    "print(\"State space = {}\".format(env.observation_space))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "|\u001b[43m \u001b[0m: | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "env.s = state\n",
    "env.render()\n",
    "env = gym.make(\"Taxi-v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: 228\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| :\u001b[43m \u001b[0m: : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "state = env.encode(2, 1, 2, 0) # (taxi row, taxi column, passenger index, destination index)\n",
    "print(\"State:\", state)\n",
    "\n",
    "env.env.s = state # ( ( (taxi row*5)+taxi col) *5 + pass_index *4) + dest_index)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(1.0, 428, -1, False)],\n",
       " 1: [(1.0, 228, -1, False)],\n",
       " 2: [(1.0, 348, -1, False)],\n",
       " 3: [(1.0, 328, -1, False)],\n",
       " 4: [(1.0, 328, -10, False)],\n",
       " 5: [(1.0, 328, -10, False)]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.P[328] # {action: [(probability, nextstate, reward, done)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "def show_frames(env ,i, episode_rewards ):\n",
    "    clear_output(wait=True)\n",
    "    env.render()\n",
    "    print(f\"step {i} rewards={episode_rewards}\")\n",
    "    sleep(.1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(y, radius):\n",
    "    '''\n",
    "    smooth data y by averaging the values in each window [max{index-radius,0}, min{(index+radius), len(y)-1}] \n",
    "    \n",
    "    '''\n",
    "    if (len(y) < 2 * radius + 1):\n",
    "        return np.mean(y) * np.ones_like(y)\n",
    "    else:\n",
    "        convkernel = np.ones(2 * radius + 1)\n",
    "        out = np.convolve(y, convkernel, mode = 'same') / np.convolve(np.ones_like(y), convkernel, mode = 'same')\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluate(env, policy, times = 10):\n",
    "    # store data, make average and return\n",
    "    all_length, all_penalties, all_rewards = np.zeros(times), np.zeros(times), np.zeros(times)\n",
    "\n",
    "    for i in range(times):\n",
    "\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        episode_penalties, episode_rewards, episode_length = 0, 0, 0\n",
    "        \n",
    "        while not done:\n",
    "            action = arg_max(policy[state])\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            state = next_state\n",
    "            #update data for learning curve\n",
    "            if reward == -10:\n",
    "                episode_penalties +=1\n",
    "\n",
    "            episode_rewards += reward\n",
    "            episode_length += 1\n",
    "\n",
    "        all_rewards[i] = episode_rewards\n",
    "        all_penalties[i] = episode_penalties\n",
    "        all_length[i] = episode_length\n",
    "    return np.mean(all_rewards), np.mean(all_penalties), np.mean(all_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, train_episodes,init_q_table=None):\n",
    "    \"\"\"\n",
    "    Training the agent\n",
    "    Q(state,action)←(1−α)Q(state,action)+α(reward+γmaxaQ(next state,all actions))\n",
    "    \"\"\"  \n",
    "    \n",
    "    # Hyper parameters\n",
    "    alpha = 0.05  #(alpha) is the learning rate (0<α≤1) \n",
    "    gamma = 0.95  # (gamma) is the discount factor (0≤γ≤1) \n",
    "    epsilon = 0.1 # the rate to act randomly in the epsilon-greedy alg for expolration in Q-learning\n",
    "\n",
    "    timesteps =  train_episodes * MAX_EPISODE_LEN\n",
    "\n",
    "    # To plot learning curve\n",
    "    all_episodes_length = []\n",
    "    all_penalties = []\n",
    "    all_rewards = []\n",
    "    \n",
    "    if init_q_table is None:\n",
    "        q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "    else:\n",
    "        q_table = copy.deepcopy(init_q_table)\n",
    "    \n",
    "    env_copy = copy.deepcopy(env) # for policy_evaluate\n",
    "    i = 0\n",
    "    while i < timesteps:\n",
    "        state = env.reset()\n",
    "\n",
    "        episode_penalties, episode_rewards, episode_length = 0, 0, 0\n",
    "        done = False\n",
    "\n",
    "        while not done and i < timesteps:\n",
    "            # epsilon greedy alg balancing exporation and exploitation\n",
    "            if random.uniform(0,1)< epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = arg_max(q_table[state])\n",
    "\n",
    "            # step to next state\n",
    "            next_state, reward, done, info = env.step(action)       \n",
    "            \n",
    "            # update q-value\n",
    "            q_value = (1-alpha) * q_table[state, action] + alpha * (reward + gamma * np.max(q_table[next_state]))\n",
    "            q_table[state, action] = q_value\n",
    "\n",
    "            state = next_state\n",
    "            if i%200 == 0:\n",
    "                #evaluate policy for learning curve after each episode\n",
    "                episode_rewards, episode_penalties, episode_length = policy_evaluate(env_copy, q_table, EVALUATION_TIMES)\n",
    "                \n",
    "                all_episodes_length.append(episode_length)\n",
    "                all_penalties.append(episode_penalties)\n",
    "                all_rewards.append(episode_rewards)\n",
    "                \n",
    "                #show training progress\n",
    "                clear_output(wait=True)\n",
    "                print(f\"timesteps: {i}\")\n",
    "            i += 1\n",
    "         \n",
    "    print(f\"Training finished\")\n",
    "    \n",
    "    # return ndarray\n",
    "    all_episodes_length = np.array(all_episodes_length)\n",
    "    all_penalties = np.array(all_penalties)\n",
    "    all_rewards = np.array(all_rewards)\n",
    "    \n",
    "    return all_episodes_length, all_penalties, all_rewards, q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SARSA(env,  train_episodes,init_s_table = None):\n",
    "    \"\"\"Training the agent\n",
    "    Q(state,action)←(1−α)Q(state,action)+α(reward+γQ(next state,next action))\n",
    "\n",
    "    \"\"\"  \n",
    "    # Hyper parameters\n",
    "    alpha = 0.05  #(alpha) is the learning rate (0<α≤1) \n",
    "    gamma = 0.95  # (gamma) is the discount factor (0≤γ≤1) \n",
    "    epsilon = 0.1 # the rate to act randomly in the epsilon-greedy alg for expolration in Q-learning\n",
    "\n",
    "    train_episodes =  train_episodes \n",
    "\n",
    "    # To plot learning curve\n",
    "    all_episodes_length = []\n",
    "    all_penalties = []\n",
    "    all_rewards = []\n",
    "    \n",
    "    \n",
    "    if init_s_table is None:\n",
    "        s_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "    else:\n",
    "        s_table = copy.deepcopy(init_s_table)\n",
    "    env_copy = copy.deepcopy(env)\n",
    "    \n",
    "    for i in range(train_episodes):\n",
    "        state = env.reset()\n",
    "\n",
    "        episode_penalties, episode_rewards, episode_length = 0, 0, 0\n",
    "        done = False\n",
    "\n",
    "        # epsilon greedy alg balancing exporation and exploitation\n",
    "        if random.uniform(0,1)< epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = arg_max(s_table[state])\n",
    "\n",
    "        while not done:\n",
    "\n",
    "            # step to next state\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "\n",
    "            # choose next action\n",
    "            if random.uniform(0,1)< epsilon:\n",
    "                next_action = env.action_space.sample()\n",
    "            else:\n",
    "                next_action = arg_max(s_table[next_state])\n",
    "\n",
    "            # update q-value\n",
    "            q_value = (1-alpha) * s_table[state, action] + alpha * (reward + gamma * s_table[next_state,next_action])\n",
    "            s_table[state, action] = q_value\n",
    "\n",
    "            state = next_state\n",
    "            action = next_action\n",
    "\n",
    "        #evaluate policy for learning curve after each episode\n",
    "        episode_rewards, episode_penalties, episode_length = policy_evaluate(env_copy, q_table, EVALUATION_TIMES)\n",
    "\n",
    "        #record data for learning curve\n",
    "        all_episodes_length.append(episode_length)\n",
    "        all_penalties.append(episode_penalties)\n",
    "        all_rewards.append(episode_rewards)\n",
    "\n",
    "        #show training progress\n",
    "        if (i+1) % 100 == 0:\n",
    "            clear_output(wait=True)\n",
    "            print(f\"Episode {i}\")\n",
    "    print(\"Training finished \\n\")\n",
    "    \n",
    "    # return ndarray\n",
    "    all_episodes_length = np.array(all_episodes_length)\n",
    "    all_penalties = np.array(all_penalties)\n",
    "    all_rewards = np.array(all_rewards)\n",
    "    \n",
    "    return all_episodes_length, all_penalties, all_rewards, s_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.PRQL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prql(env, train_episodes, past_policy):\n",
    "        \n",
    "    # Hyper parameters\n",
    "    alpha = 0.05  #(alpha) is the learning rate (0<α≤1) \n",
    "    gamma = 0.95  # (gamma) is the discount factor (0≤γ≤1) \n",
    "    epsilon = 0.1 # the rate to act randomly in the epsilon-greedy alg for expolration in Q-learning\n",
    "    fi = 1 # the intial probability to reuse past policy in policy-reuse algo, decays after each step in one episode\n",
    "    mu = 0.95 # the decaying rate of fi\n",
    "    \n",
    "    # To plot learning curve\n",
    "    all_episodes_length = []\n",
    "    all_penalties = []\n",
    "    all_rewards = []\n",
    "\n",
    "    q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    " \n",
    "    past_table = copy.deepcopy(past_policy)\n",
    "    env_copy = copy.deepcopy(env)\n",
    "    \n",
    "    for i in range(train_episodes):\n",
    "        state = env.reset()\n",
    "\n",
    "        episode_penalties, episode_rewards, episode_length = 0, 0, 0\n",
    "        done = False\n",
    "        f = fi\n",
    "        while not done:\n",
    "            # epsilon greedy alg balancing exporation and exploitation\n",
    "            if random.uniform(0,1) < f:\n",
    "                action = arg_max(past_table[state])\n",
    "                #print(f\"f = {f} in episode {i} \\n\")\n",
    "            elif random.uniform(0,1) < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "                #print(\"random action \\n\")\n",
    "                \n",
    "            else:\n",
    "                action = arg_max(q_table[state])\n",
    "                #print(\"on the policy now \\n\")\n",
    "\n",
    "            # step to next state\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "\n",
    "            # update q-value\n",
    "            q_value = (1-alpha) * q_table[state, action] + alpha * (reward + gamma * np.max(q_table[next_state]))\n",
    "            q_table[state, action] = q_value\n",
    "\n",
    "            state = next_state\n",
    "            f = f*mu\n",
    "            \n",
    "        #evaluate policy for learning curve after each episode\n",
    "        episode_rewards, episode_penalties, episode_length = policy_evaluate(env_copy, q_table, EVALUATION_TIMES)\n",
    "\n",
    "        #record data for learning curve\n",
    "        all_episodes_length.append(episode_length)\n",
    "        all_penalties.append(episode_penalties)\n",
    "        all_rewards.append(episode_rewards)\n",
    "\n",
    "        #show training progress\n",
    "        if (i+1) % 100 == 0:\n",
    "            clear_output(wait=True)\n",
    "            print(f\"Episode {i}\")\n",
    "    print(\"Training finished \\n\")\n",
    "    \n",
    "    # return ndarray\n",
    "    all_episodes_length = np.array(all_episodes_length)\n",
    "    all_penalties = np.array(all_penalties)\n",
    "    all_rewards = np.array(all_rewards)\n",
    "    \n",
    "    return all_episodes_length, all_penalties, all_rewards, q_table    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. OPS-TL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OPS_TL(env,train_episodes,past_policies):\n",
    "    \"\"\"\n",
    "    Training the agent using algorithm in paper \"An Optimal Online Method of Selecting \n",
    "    Source Policies for Reinforcement Learning\"\n",
    "    \"\"\"\n",
    "    # Hyper parameters\n",
    "    alpha = 0.05 #(alpha) is the learning rate (0<α≤1) \n",
    "    gamma = 0.95 # (gamma) is the discount factor (0≤γ≤1)\n",
    "    epsilon = 0.1 # the rate to act randomly in the epsilon-greedy alg for expolration in Q-learning\n",
    "    fi = 0.95 # the intial probability to reuse past policy in policy-reuse algo, decays after each step in one episode\n",
    "    c = 0.0049 # the factor in UCB-tuned deciding the rate of exploration, a lager c will lead to a higher exploration rate\n",
    "    \n",
    "    # To plot learning curve\n",
    "    all_episodes_length = []\n",
    "    all_penalties = []\n",
    "    all_rewards = []\n",
    "   \n",
    "    #win = {'reuse': [], 'new': []}\n",
    "    timesteps = train_episodes * MAX_EPISODE_LEN\n",
    "   \n",
    "    q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "    env_copy = copy.deepcopy(env)\n",
    "    \n",
    "    # pt = 1-k/(k+1500)\n",
    "    \n",
    "    #initialize the expected reward of every bandit/past_policy\n",
    "    policy_expect = np.zeros([len(past_policies)])\n",
    "    policy_times = np.zeros([len(past_policies)])\n",
    "    for j in range(len(past_policies)):\n",
    "        t = 0\n",
    "        env.reset()\n",
    "        _,policy_expect[j],_,_= policy_reuse(timesteps, env,q_table, env.reset(), t,[],[],[],past_policies[j])\n",
    "        policy_times[j] += 1\n",
    "        \n",
    "    # training strat  \n",
    "    i = 0\n",
    "    while i < timesteps:\n",
    "        state = env.reset()\n",
    "        \n",
    "        episode_penalties, episode_rewards, episode_length = 0, 0, 0\n",
    "        done = False\n",
    "        \n",
    "        k = i // MAX_EPISODE_LEN  + 1\n",
    "        pt = 1-k/(k+1500)\n",
    "        \n",
    "        # use UCB1 to solve MAB(multi armed bandit problem\n",
    "        if random.uniform(0,1) < pt:\n",
    "            j = arg_max(policy_expect + np.sqrt(c * np.log(np.sum(policy_times)) / policy_times )) # UCB1-tuned\n",
    "            episode_penalties, episode_rewards, episode_length,i = policy_reuse(timesteps, env, q_table, state, i,\n",
    "                                                                              all_episodes_length,all_penalties,all_rewards,\n",
    "                                                                              past_policy = past_policies[j])\n",
    "            policy_expect[j] = (policy_expect[j]*policy_times[j] + episode_rewards)/( policy_times[j] +1)\n",
    "            policy_times[j] += 1\n",
    "#            if episode_length < 200:\n",
    "#                win['reuse'].append(k)\n",
    "        # follow episilon-greedy strategy    \n",
    "        else:\n",
    "            episode_penalties, episode_rewards, episode_length,i = epsilon_greedy(timesteps, env, q_table, state, i , \n",
    "                                                                                all_episodes_length,all_penalties,all_rewards,\n",
    "                                                                                epsilon, alpha, gamma)\n",
    "#            if episode_length < 200:\n",
    "#                win['new'].append(k)        \n",
    "\n",
    "\n",
    "        #show training progress\n",
    "        if (k) % 100 == 0:\n",
    "            clear_output(wait=True)\n",
    "            print(f\"Episode {k}\")\n",
    "    print(f\"Training finished\")\n",
    "    \n",
    "    # return ndarray\n",
    "    all_episodes_length = np.array(all_episodes_length)\n",
    "    all_penalties = np.array(all_penalties)\n",
    "    all_rewards = np.array(all_rewards)\n",
    "    \n",
    "    return all_episodes_length, all_penalties, all_rewards, q_table    \n",
    "            \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OPS_Norm(env,train_episodes,past_policies, c = 0.0049):\n",
    "    \"\"\"\n",
    "    Training the agent using algorithm in paper \"An Optimal Online Method of Selecting \n",
    "    Source Policies for Reinforcement Learning\"\n",
    "    \"\"\"\n",
    "    # Hyper parameters\n",
    "    alpha = 0.05 #(alpha) is the learning rate (0<α≤1) \n",
    "    gamma = 0.95 # (gamma) is the discount factor (0≤γ≤1)\n",
    "    epsilon = 0.1 # the rate to act randomly in the epsilon-greedy alg for expolration in Q-learning\n",
    "    fi = 0.95 # the intial probability to reuse past policy in policy-reuse algo, decays after each step in one episode\n",
    "    c = c # the factor in UCB-tuned deciding the rate of exploration, a lager c will lead to a higher exploration rate\n",
    "    \n",
    "    # To plot learning curve\n",
    "    all_episodes_length = []\n",
    "    all_penalties = []\n",
    "    all_rewards = []\n",
    "    all_frequency = np.zeros([len(past_policies)+1, train_episodes])\n",
    "#   win = {'reuse': [], 'new': []}\n",
    "    timesteps = train_episodes * MAX_EPISODE_LEN\n",
    "   \n",
    "    q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "    env_copy = copy.deepcopy(env)\n",
    "    \n",
    "    # pt = 1-k/(k+1500)\n",
    "    \n",
    "    #initialize the expected reward of every bandit/past_policy\n",
    "    frequency = np.zeros([len(past_policies)+1, train_episodes])\n",
    "    policy_expect = np.zeros([len(past_policies)])\n",
    "    policy_times = np.zeros([len(past_policies)])\n",
    "    for j in range(len(past_policies)):\n",
    "        t = 0\n",
    "        env.reset()\n",
    "        for _ in range(10):\n",
    "            _,temp,_,_= policy_reuse(timesteps, env,q_table, env.reset(), t,[],[],[],past_policies[j])\n",
    "            policy_expect[j] += temp\n",
    "        policy_expect[j] /= 10\n",
    "        policy_times[j] += 1\n",
    "        \n",
    "    # training strat  \n",
    "    i = 0\n",
    "    while i < timesteps:\n",
    "        state = env.reset()\n",
    "        \n",
    "        episode_penalties, episode_rewards, episode_length = 0, 0, 0\n",
    "        done = False\n",
    "        \n",
    "        k = i // MAX_EPISODE_LEN  + 1\n",
    "        pt = 1-k/(k+1500)\n",
    "        \n",
    "        # use UCB1 to solve MAB(multi armed bandit problem\n",
    "        if random.uniform(0,1) < pt:\n",
    "            j = arg_max(policy_expect/20 + np.sqrt(c * np.log(np.sum(policy_times)) / policy_times )) # UCB1-tuned\n",
    "            episode_penalties, episode_rewards, episode_length,i = policy_reuse(timesteps, env, q_table, state, i,\n",
    "                                                                              all_episodes_length,all_penalties,all_rewards,\n",
    "                                                                              past_policy = past_policies[j])\n",
    "            policy_expect[j] = (policy_expect[j]*policy_times[j] + episode_rewards)/( policy_times[j] +1)\n",
    "            policy_times[j] += 1\n",
    "            frequency[j][k-1] += 1\n",
    "#            if episode_length < 200:\n",
    "#                win['reuse'].append(k)\n",
    "        # follow episilon-greedy strategy    \n",
    "        else:\n",
    "            episode_penalties, episode_rewards, episode_length,i = epsilon_greedy(timesteps, env, q_table, state, i , \n",
    "                                                                                all_episodes_length,all_penalties,all_rewards,\n",
    "                                                                                epsilon, alpha, gamma)\n",
    "#            if episode_length < 200:\n",
    "#                win['new'].append(k)        \n",
    "            frequency[len(past_policies)][k-1] += 1\n",
    "\n",
    "        #show training progress\n",
    "        if (k) % 100 == 0:\n",
    "            clear_output(wait=True)\n",
    "            print(f\"Episode {k}\")\n",
    "    print(f\"Training finished\")\n",
    "    \n",
    "    # return ndarray\n",
    "    all_episodes_length = np.array(all_episodes_length)\n",
    "    all_penalties = np.array(all_penalties)\n",
    "    all_rewards = np.array(all_rewards)\n",
    "    for line in range(train_episodes):\n",
    "        for row in range(len(past_policies)+1):\n",
    "            all_frequency[row][line]=np.sum(frequency[row][:line+1], keepdims= True) /np.sum(np.sum(frequency[:,:line+1]))\n",
    "    return all_episodes_length, all_penalties, all_rewards, all_frequency, q_table    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.66666667, 0.6       ],\n",
       "       [0.        , 0.33333333, 0.2       ],\n",
       "       [0.        , 0.        , 0.2       ],\n",
       "       [0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequency =np.array([[1., 1., 1.],\n",
    "       [0., 1., 0.],\n",
    "       [0., 0., 1.],\n",
    "       [0., 0., 0.]])\n",
    "all_frequency = np.zeros([4,3])\n",
    "\n",
    "for line in range(3):\n",
    "    for row in range(4):\n",
    "        all_frequency[row][line]=  np.sum(frequency[row][:line+1]) /np.sum(np.sum(frequency[:,:line+1]))\n",
    "\n",
    "    #all_frequency[:][line]=frequency[:][line]/ np.sum(frequency[:][:line],axis = 1, keepdims= True)\n",
    "all_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=np.arange(12)\n",
    "print(a)\n",
    "#结果：[ 0  1  2  3  4  5  6  7  8  9 10 11]\n",
    "\n",
    "#reshape对一维数组进行修改形状 (4,3)修改为4行3列\n",
    "a=np.ones([4,3])\n",
    "a[:,:2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(timesteps, env, q_table, initial_state, i,\n",
    "                   all_episodes_length,all_penalties,all_rewards,epsilon = 0.1, alpha = 0.05,  gamma = 0.95 ):\n",
    "    done = False\n",
    "    state = initial_state\n",
    "    episode_penalties, episode_rewards, episode_length = 0, 0, 0\n",
    "    _env_copy = copy.deepcopy(env)\n",
    "    while not done and i < timesteps:\n",
    "        if random.uniform(0,1) < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = arg_max(q_table[state])\n",
    "            \n",
    "        # step to next state\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # update q-value\n",
    "        q_value = (1-alpha) * q_table[state, action] + alpha * (reward + gamma * np.max(q_table[next_state]))\n",
    "        q_table[state, action] = q_value\n",
    "\n",
    "        state = next_state\n",
    "      \n",
    "        #update data for learning curve\n",
    "        if reward == -10:\n",
    "            episode_penalties +=1\n",
    "\n",
    "        episode_rewards += reward\n",
    "        episode_length += 1\n",
    "        \n",
    "        if i%200 == 0:\n",
    "            #evaluate policy for learning curve after each episode\n",
    "            _episode_rewards, _episode_penalties, _episode_length = policy_evaluate(_env_copy, q_table, EVALUATION_TIMES)\n",
    "\n",
    "            all_episodes_length.append(_episode_length)\n",
    "            all_penalties.append(_episode_penalties)\n",
    "            all_rewards.append(_episode_rewards)\n",
    "\n",
    "            #show training progress\n",
    "            clear_output(wait=True)\n",
    "            print(f\"timesteps: {i}\")\n",
    "        i += 1\n",
    "\n",
    "    return episode_penalties, episode_rewards, episode_length,i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_reuse(timesteps,env, q_table, initial_state, i, \n",
    "                 all_episodes_length,all_penalties,all_rewards, past_policy,  fi = 0.95, mu=0.95, alpha = 0.05,  gamma = 0.95 ):\n",
    "    done = False\n",
    "    state = initial_state\n",
    "    episode_penalties, episode_rewards, episode_length = 0, 0, 0\n",
    "\n",
    "    _env_copy = copy.deepcopy(env)\n",
    "    while not done and i < timesteps:\n",
    "        if random.uniform(0,1) < fi:\n",
    "            action = arg_max(past_policy[state])\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "            \n",
    "        # step to next state\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # update q-value\n",
    "        q_value = (1-alpha) * q_table[state, action] + alpha * (reward + gamma * np.max(q_table[next_state]))\n",
    "        q_table[state, action] = q_value\n",
    "\n",
    "        state = next_state\n",
    "        fi = fi*mu\n",
    "        #update data for learning curve\n",
    "        if reward == -10:\n",
    "            episode_penalties +=1\n",
    "\n",
    "        episode_rewards += reward\n",
    "        episode_length += 1\n",
    "        \n",
    "        if i%200 == 0:\n",
    "            #evaluate policy for learning curve after each episode\n",
    "            _episode_rewards, _episode_penalties, _episode_length = policy_evaluate(_env_copy, q_table, EVALUATION_TIMES)\n",
    "\n",
    "            all_episodes_length.append(_episode_length)\n",
    "            all_penalties.append(_episode_penalties)\n",
    "            all_rewards.append(_episode_rewards)\n",
    "\n",
    "            #show training progress\n",
    "            clear_output(wait=True)\n",
    "            print(f\"timesteps: {i}\")\n",
    "        i += 1\n",
    "        \n",
    "    return episode_penalties, episode_rewards, episode_length,i\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tranform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform( target_rl_algo, episode_num, repeat_times, target_task, \n",
    "              source_task= None, source_rl_algo = None, policy_library = None):\n",
    "    \n",
    "    \"\"\"\n",
    "    Situation 1:\n",
    "    no source task available, train agent on target task from scratch \n",
    "    \n",
    "    Situation 2:\n",
    "    source task is available but source policy unavailable, train agent on source task to get source policy, \n",
    "    then train agent on target task using knowledge from source policy\n",
    "    \n",
    "    Situation 3:\n",
    "    source policies are availale, reuse source policies to train agent on target task\n",
    "    \"\"\"\n",
    "    train_episodes = episode_num    \n",
    "    # data collected during trainning\n",
    "    all_episodes_length = np.zeros(train_episodes)\n",
    "    all_penalties = np.zeros(train_episodes)\n",
    "    all_rewards = np.zeros(train_episodes)\n",
    "    if target_rl_algo is OPS_Norm and policy_library is not None:\n",
    "        all_frequency = np.zeros([len(policy_library[0])+1, train_episodes])\n",
    "    else:\n",
    "        all_frequency = None\n",
    "    \n",
    "    all_trans_knowledge = []\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in range(repeat_times):\n",
    "       \n",
    "        \n",
    "        #Situation 3    \n",
    "        if policy_library is not None: \n",
    "            \n",
    "            if target_task is str:\n",
    "                env = gym.make(target_task)\n",
    "            else:\n",
    "                env = target_task\n",
    "            past_policies = policy_library # policy_library = [ [[policy11][policy21]...] [[policy12][policy22]...]...]\n",
    "            if target_rl_algo is OPS_Norm:\n",
    "                episodes,penalties,rewards,frequency, *knowledge = target_rl_algo(env, train_episodes, past_policies[i]) \n",
    "            else:\n",
    "                episodes,penalties,rewards, *knowledge = target_rl_algo(env, train_episodes, past_policies[i]) \n",
    "        #Situation 2   \n",
    "        elif source_task is not None:\n",
    "            if source_rl_algo is None:\n",
    "                source_rl_algo = target_rl_algo\n",
    "            if source_task is str:\n",
    "                env = gym.make(source_task)\n",
    "            else:\n",
    "                env = source_task\n",
    "            _, _,_ ,*knowledge = source_rl_algo(env,  train_episodes)\n",
    "            \n",
    "            if target_task is str:\n",
    "                env = gym.make(target_task)\n",
    "            else:\n",
    "                env = target_task\n",
    "            episodes,penalties,rewards,*knowledge = target_rl_algo(env, train_episodes , *knowledge)\n",
    "         #Situation 1\n",
    "        else: \n",
    "            if target_task is str:\n",
    "                env = gym.make(target_task)\n",
    "            else:\n",
    "                env = target_task\n",
    "           \n",
    "            episodes,penalties,rewards,*knowledge = target_rl_algo(env, train_episodes)\n",
    "            \n",
    "\n",
    "        all_episodes_length += episodes\n",
    "        all_penalties += penalties\n",
    "        all_rewards += rewards\n",
    "        if target_rl_algo is OPS_Norm and policy_library is not None:\n",
    "            all_frequency += frequency\n",
    "        if len(knowledge) > 1:\n",
    "            all_trans_knowledge.append(knowledge)\n",
    "        else:\n",
    "            all_trans_knowledge.append(*knowledge)\n",
    "\n",
    "    all_episodes_length /= repeat_times\n",
    "    all_penalties /= repeat_times\n",
    "    all_rewards /= repeat_times\n",
    "    if target_rl_algo is OPS_Norm and policy_library is not None:\n",
    "        all_frequency /= repeat_times\n",
    "    \n",
    "    return all_episodes_length, all_penalties, all_rewards, all_frequency, all_trans_knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,*args = 3,4,5\n",
    "args[:]\n",
    "def add(a,b):\n",
    "    return a+b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 799800\n",
      "Training finished\n",
      "CPU times: user 10min 18s, sys: 1min 39s, total: 11min 57s\n",
      "Wall time: 10min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "qlearn_episodes_length_fourroom, qlearn_penalties_fourroom, qlearn_rewards_fourroom, qlearn_q_table_fourroom = [[None] * len(four_room_envs) for _ in range(4)]\n",
    "for i in range(len(four_room_envs)):\n",
    "    qlearn_episodes_length_fourroom[i], qlearn_penalties_fourroom[i], qlearn_rewards_fourroom[i], _, qlearn_q_table_fourroom[i] = transform(target_rl_algo = q_learning,\n",
    "                                                                                     episode_num = EPISODE_NUM, \n",
    "                                                                                   repeat_times = REPEAT_TIMES,\n",
    "                                                                                     target_task = four_room_envs[i],\n",
    "                                                                                     source_task = None\n",
    "                                                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<unknown>, line 5)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/home/cseadmin/anaconda3/envs/spinningup/lib/python3.6/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m2862\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \u001b[1;32m\"<ipython-input-32-5488db75b097>\"\u001b[0m, line \u001b[1;32m1\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n    get_ipython().run_cell_magic('time', '', '\\n# training Taxi-v003 from scratch\\nqlearn_episodes_length_v003, qlearn_penalties_v003, qlearn_rewards_v003, _, qlearn_q_table_v003 = transform(target_rl_algo = q_learning,\\n                                                                                     episode_num = EPISODE_NUM, \\n                                                       `                            repeat_times = REPEAT_TIMES,\\n                                                                                     target_task = \"Taxi-v003\",\\n                                                                                     source_task = None\\n                                                                                    )')\n",
      "  File \u001b[1;32m\"/home/cseadmin/anaconda3/envs/spinningup/lib/python3.6/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m2103\u001b[0m, in \u001b[1;35mrun_cell_magic\u001b[0m\n    result = fn(magic_arg_s, cell)\n",
      "  File \u001b[1;32m\"<decorator-gen-62>\"\u001b[0m, line \u001b[1;32m2\u001b[0m, in \u001b[1;35mtime\u001b[0m\n",
      "  File \u001b[1;32m\"/home/cseadmin/anaconda3/envs/spinningup/lib/python3.6/site-packages/IPython/core/magic.py\"\u001b[0m, line \u001b[1;32m187\u001b[0m, in \u001b[1;35m<lambda>\u001b[0m\n    call = lambda f, *a, **k: f(*a, **k)\n",
      "  File \u001b[1;32m\"/home/cseadmin/anaconda3/envs/spinningup/lib/python3.6/site-packages/IPython/core/magics/execution.py\"\u001b[0m, line \u001b[1;32m1179\u001b[0m, in \u001b[1;35mtime\u001b[0m\n    expr_ast = self.shell.compile.ast_parse(expr)\n",
      "\u001b[0;36m  File \u001b[0;32m\"/home/cseadmin/anaconda3/envs/spinningup/lib/python3.6/site-packages/IPython/core/compilerop.py\"\u001b[0;36m, line \u001b[0;32m99\u001b[0;36m, in \u001b[0;35mast_parse\u001b[0;36m\u001b[0m\n\u001b[0;31m    return compile(source, filename, symbol, self.flags | PyCF_ONLY_AST, 1)\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"<unknown>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    `                            repeat_times = REPEAT_TIMES,\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# training Taxi-v003 from scratch\n",
    "qlearn_episodes_length_v003, qlearn_penalties_v003, qlearn_rewards_v003, _, qlearn_q_table_v003 = transform(target_rl_algo = q_learning,\n",
    "                                                                                     episode_num = EPISODE_NUM, \n",
    "                                                       `                            repeat_times = REPEAT_TIMES,\n",
    "                                                                                     target_task = \"Taxi-v003\",\n",
    "                                                                                     source_task = None\n",
    "                                                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# training Taxi-v000 from scratch\n",
    "qlearn_episodes_length_v000, qlearn_penalties_v000, qlearn_rewards_v000, _, qlearn_q_table_v000 = transform(target_rl_algo = q_learning,\n",
    "                                                                                     episode_num = EPISODE_NUM, \n",
    "                                                                                     repeat_times = REPEAT_TIMES,\n",
    "                                                                                     target_task = \"Taxi-v000\",\n",
    "                                                                                     source_task = None\n",
    "                                                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# training Taxi-v001 from scratch\n",
    "qlearn_episodes_length_v001, qlearn_penalties_v001, qlearn_rewards_v001,_, qlearn_q_table_v001 = transform(target_rl_algo = q_learning,\n",
    "                                                                                     episode_num = EPISODE_NUM, \n",
    "                                                                                     repeat_times = REPEAT_TIMES,\n",
    "                                                                                     target_task = \"Taxi-v001\",\n",
    "                                                                                     source_task = None\n",
    "                                                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# training Taxi-v004 from scratch\n",
    "qlearn_episodes_length_v004, qlearn_penalties_v004, qlearn_rewards_v004,_, qlearn_q_table_v004 = transform(target_rl_algo = q_learning,\n",
    "                                                                                     episode_num = EPISODE_NUM, \n",
    "                                                                                     repeat_times = REPEAT_TIMES,\n",
    "                                                                                     target_task = \"Taxi-v004\",\n",
    "                                                                                     source_task = None\n",
    "                                                                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 PRQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3999\n",
      "Training finished \n",
      "\n",
      "CPU times: user 1min 50s, sys: 8.79 s, total: 1min 59s\n",
      "Wall time: 1min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pr_episodes_length_fourroom, pr_penalties_fourroom, pr_rewards_fourroom, pr_q_table_fourroom = [[None] * (len(four_room_envs)-1) for _ in range(4)]\n",
    "\n",
    "#pr_fourroom_v012_3\n",
    "for i in range(len(four_room_envs)-1):\n",
    "    pr_episodes_length_fourroom[i], pr_penalties_fourroom[i], pr_rewards_fourroom[i], _, pr_q_table_fourroom[i] = transform(target_rl_algo = prql,\n",
    "                                                                                     episode_num = EPISODE_NUM, \n",
    "                                                                                   repeat_times = REPEAT_TIMES,\n",
    "                                                                                     target_task = four_room_envs[3],\n",
    "                                                                                     policy_library = qlearn_q_table_fourroom[i]\n",
    "                                                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#pr_v014_003\n",
    "pr_episodes_length_v014_003, pr_penalties_v014_003,pr_rewards_v014_003, _, pr_q_table_v014_003 = transform(target_rl_algo= prql,\n",
    "                                                                                       episode_num = EPISODE_NUM,\n",
    "                                                                                       repeat_times = 5,\n",
    "                                                                                       target_task = \"Taxi-v003\",\n",
    "                                                                                       \n",
    "                                                                                       policy_library =  qlearn_q_table_v004)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3 OPS-TL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 799800\n",
      "Training finished\n",
      "CPU times: user 8min 43s, sys: 1min 38s, total: 10min 21s\n",
      "Wall time: 9min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tl_episodes_length_fourroom, tl_penalties_fourroom, tl_rewards_fourroom, tl_q_table_fourroom = [[None] * (len(four_room_envs)-1) for _ in range(4)]\n",
    "\n",
    "#tl_fourroom_v012_3\n",
    "for i in range(len(four_room_envs)-1):\n",
    "    tl_episodes_length_fourroom[i], tl_penalties_fourroom[i], tl_rewards_fourroom[i], _, tl_q_table_fourroom[i] = transform(target_rl_algo = OPS_TL,\n",
    "                                                                                     episode_num = EPISODE_NUM, \n",
    "                                                                                   repeat_times = REPEAT_TIMES,\n",
    "                                                                                     target_task = four_room_envs[3],\n",
    "                                                                                     policy_library =  [[q_func1,q_func2,q_func3 ] for q_func1,q_func2,q_func3 in zip(qlearn_q_table_fourroom[0],qlearn_q_table_fourroom[1],qlearn_q_table_fourroom[2])]\n",
    "                                                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#ops_v014_003\n",
    "tl_episodes_length_v014_003, tl_penalties_v014_003,tl_rewards_v014_003,tl_fre_v014_003, tl_q_table_v014_003 = transform(target_rl_algo= OPS_TL,\n",
    "                                                                                       episode_num = EPISODE_NUM,\n",
    "                                                                                       repeat_times = REPEAT_TIMES,\n",
    "                                                                                       target_task = \"Taxi-v003\",\n",
    "                                                                                      \n",
    "                                                                                       policy_library = [[q_func1,q_func2,q_func3 ] for q_func1,q_func2,q_func3 in zip(qlearn_q_table_v000, qlearn_q_table_v001, qlearn_q_table_v004)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 CAPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 799800\n",
      "Training finished\n",
      "CPU times: user 8min 46s, sys: 1min 22s, total: 10min 9s\n",
      "Wall time: 9min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "ops_episodes_length_fourroom, ops_penalties_fourroom, ops_rewards_fourroom, ops_q_table_fourroom = [[None] * (len(four_room_envs)-1) for _ in range(4)]\n",
    "\n",
    "#ops_fourroom_v012_3\n",
    "for i in range(len(four_room_envs)-1):\n",
    "    ops_episodes_length_fourroom[i], ops_penalties_fourroom[i], ops_rewards_fourroom[i], _, ops_q_table_fourroom[i] = transform(target_rl_algo = OPS_Norm,\n",
    "                                                                                     episode_num = EPISODE_NUM, \n",
    "                                                                                   repeat_times = REPEAT_TIMES,\n",
    "                                                                                     target_task = four_room_envs[3],\n",
    "                                                                                     policy_library =  [[q_func1,q_func2,q_func3 ] for q_func1,q_func2,q_func3 in zip(qlearn_q_table_fourroom[0],qlearn_q_table_fourroom[1],qlearn_q_table_fourroom[2])]\n",
    "                                                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#ops_v014_003\n",
    "ops_episodes_length_v014_003, ops_penalties_v014_003,ops_rewards_v014_003,ops_fre_v014_003, ops_q_table_v014_003 = transform(target_rl_algo= OPS_Norm,\n",
    "                                                                                       episode_num = EPISODE_NUM,\n",
    "                                                                                       repeat_times = REPEAT_TIMES,\n",
    "                                                                                       target_task = \"Taxi-v003\",\n",
    "                                                                                      \n",
    "                                                                                       policy_library = [[q_func1,q_func2,q_func3 ] for q_func1,q_func2,q_func3 in zip(qlearn_q_table_v000, qlearn_q_table_v001, qlearn_q_table_v004)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 option-critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Taxi_v000,Taxi_v001,Taxi_v003,Taxi_v004\n",
    "import copy\n",
    "option_policies, option_terminations, policy_over_options, nrewards = [ [None]*4 for i in range(4) ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_list = [\"Taxi-v000\", \"Taxi-v001\", \"Taxi-v003\", \"Taxi-v004\"]\n",
    "\n",
    "for i in range(len(env_list)):\n",
    "    option_policies[i], option_terminations[i], policy_over_options[i], nrewards[i] = option_critic(env = gym.make(env_list[i]),nruns = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trans_rewards = [None]*4\n",
    "trans_termination = [None]*4\n",
    "for i in [0,1,3]:\n",
    "    _, trans_termination[i], _, trans_rewards[i]= option_critic( gym.make(\"Taxi-v003\"), 2, option_policies[i], option_terminations[i], policy_over_options[i])\n",
    "qlearn_q_table_fourroom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = 'a b'\n",
    "args.partition(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "level = [0,1,3,4]\n",
    "nepisodes = 4000\n",
    "plt.xlabel('episodes')\n",
    "plt.ylabel('rewards')\n",
    "plt.plot(range(nepisodes), smooth(nrewards[1][0:4000],500),label ='oc_v3')\n",
    "for i in [0,1,3]:\n",
    "   plt.plot(range(nepisodes), smooth(trans_rewards[i],500),label ='oc_v'+str(level[i])+'_3')\n",
    "\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%matplotlib widget\n",
    "RATIO = 1\n",
    "SMOOTH_RADIUS = 50\n",
    "sns.set(style = \"darkgrid\")\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(qlearn_rewards_v003, SMOOTH_RADIUS), label = \"qlearn_v003\", color='k')\n",
    "#plt.plot(range(EPISODE_NUM), smooth(qlearn_rewards_v5, 100), label = \"qlearn_v5\")\n",
    "\n",
    "#plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v000_003,SMOOTH_RADIUS), label = \"ops_norm_v0_003\",color='b')\n",
    "#plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v001_003,SMOOTH_RADIUS), label = \"ops_norm_v1_003\",color='m')\n",
    "#plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v004_003,SMOOTH_RADIUS), label = \"ops_norm_v4_003\",color='y')\n",
    "\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(pr_rewards_v014_003,SMOOTH_RADIUS), label = \"pr_v014_003\",color='c')\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(tl_rewards_v014_003,SMOOTH_RADIUS), label = \"ops_v014_003\",color='g')\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v014_003,SMOOTH_RADIUS), label = \"caps_v014_003\",color='r')\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(trans_rewards[3],SMOOTH_RADIUS), label = \"oc_v014_003\",color='m')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.title(\"0-4000 episode-reward graph\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"rewards\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RATIO = 4\n",
    "SMOOTH_RADIUS = 50\n",
    "sns.set(style = \"darkgrid\")\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(qlearn_rewards_v003[:EPISODE_NUM//RATIO], SMOOTH_RADIUS), label = \"qlearn_v003\", color='k')\n",
    "#plt.plot(range(EPISODE_NUM), smooth(qlearn_rewards_v5, 100), label = \"qlearn_v5\")\n",
    "\n",
    "#plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v000_003,SMOOTH_RADIUS), label = \"ops_norm_v0_003\",color='b')\n",
    "#plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v001_003,SMOOTH_RADIUS), label = \"ops_norm_v1_003\",color='m')\n",
    "#plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v004_003,SMOOTH_RADIUS), label = \"ops_norm_v4_003\",color='y')\n",
    "\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(pr_rewards_v014_003[:EPISODE_NUM//RATIO],SMOOTH_RADIUS), label = \"pr_v014_003\",color='c')\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(tl_rewards_v014_003[:EPISODE_NUM//RATIO],SMOOTH_RADIUS), label = \"ops_v014_003\",color='g')\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v014_003[:EPISODE_NUM//RATIO],SMOOTH_RADIUS), label = \"caps_v014_003\",color='r')\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(trans_rewards[3][:EPISODE_NUM//RATIO],SMOOTH_RADIUS), label = \"oc_v014_003\",color='m')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.title(\"0-1000 episode-reward graph\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"rewards\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "RATIO = 1\n",
    "SMOOTH_RADIUS = 100\n",
    "sns.set(style = \"darkgrid\")\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_fre_v01_003[0],SMOOTH_RADIUS), label = 'ploicy_v001')\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_fre_v01_003[1],SMOOTH_RADIUS), label = 'ploicy_v004')\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_fre_v01_003[2],SMOOTH_RADIUS), label = 'target policy')\n",
    "plt.legend()\n",
    "plt.xlabel('episodes')\n",
    "plt.ylabel('frequency')\n",
    "plt.title(\"frequency of policy selection\")\n",
    "plt.show()\n",
    "#plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_fre_v04_003,SMOOTH_RADIUS), label = \"ops_norm_v04_003\",color='g')\n",
    "#plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_fre_v14_003,SMOOTH_RADIUS), label = \"ops_norm_v14_003\",color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RATIO = 10\n",
    "SMOOTH_RADIUS = 20\n",
    "\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(qlearn_rewards_v003[:EPISODE_NUM//RATIO], SMOOTH_RADIUS), label = \"qlearn_v003\", color='k')\n",
    "#plt.plot(range(EPISODE_NUM), smooth(qlearn_rewards_v5, 100), label = \"qlearn_v5\")\n",
    "\n",
    "#plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v000_003[:EPISODE_NUM//RATIO],SMOOTH_RADIUS), label = \"ops_norm_v0_003\",color='b')\n",
    "#plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v001_003[:EPISODE_NUM//RATIO],SMOOTH_RADIUS), label = \"ops_norm_v1_003\",color='m')\n",
    "#plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v004_003[:EPISODE_NUM//RATIO],SMOOTH_RADIUS), label = \"ops_norm_v4_003\",color='y')\n",
    "\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v01_003[:EPISODE_NUM//RATIO],SMOOTH_RADIUS), label = \"caps_v01_003\",color='c')\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v04_003[:EPISODE_NUM//RATIO],SMOOTH_RADIUS), label = \"caps_v04_003\",color='g')\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v14_003[:EPISODE_NUM//RATIO],SMOOTH_RADIUS), label = \"caps_v14_003\",color='r')\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v014_003[:EPISODE_NUM//RATIO], SMOOTH_RADIUS), label = \"caps_v014_003\", color='navy')\n",
    "\n",
    "plt.title(\"first 400 episode-reward graph\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"rewards\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#ops_v1_003\n",
    "ops_episodes_length_v1_003, ops_penalties_v1_003,ops_rewards_v1_003, ops_q_table_v1_003 = transform(target_rl_algo= OPS_TL,\n",
    "                                                                                       episode_num = EPISODE_NUM,\n",
    "                                                                                       repeat_times = REPEAT_TIMES,\n",
    "                                                                                       target_task = \"Taxi-v003\",\n",
    "                                                                                       source_task = \"Taxi-v1\",\n",
    "                                                                                       policy_library = [[q_func] for q_func in qlearn_q_table_v1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#ops_v4_003\n",
    "ops_episodes_length_v4_003, ops_penalties_v4_003,ops_rewards_v4_003, ops_q_table_v4_003 = transform(target_rl_algo= OPS_TL,\n",
    "                                                                                       episode_num = EPISODE_NUM,\n",
    "                                                                                       repeat_times = REPEAT_TIMES,\n",
    "                                                                                       target_task = \"Taxi-v003\",\n",
    "                                                                                       source_task = \"Taxi-v4\",\n",
    "                                                                                       policy_library = [[q_func] for q_func in qlearn_q_table_v4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#ops_v14_003\n",
    "ops_episodes_length_v14_003, ops_penalties_v14_003,ops_rewards_v14_003, ops_q_table_v14_003 = transform(target_rl_algo= OPS_TL,\n",
    "                                                                                       episode_num = EPISODE_NUM,\n",
    "                                                                                       repeat_times = REPEAT_TIMES,\n",
    "                                                                                       target_task = \"Taxi-v003\",\n",
    "                                                                                       source_task = \"Taxi-v4\",\n",
    "                                                                                       policy_library = [[q_func1,q_func2 ] for q_func1,q_func2 in zip(qlearn_q_table_v1, qlearn_q_table_v4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#ops_norm_v14_003\n",
    "ops_norm_episodes_length_v14_003, ops__norm_penalties_v14_003,ops_norm_rewards_v14_003, ops_norm_q_table_v14_003 = transform(target_rl_algo= OPS_Norm,\n",
    "                                                                                       episode_num = EPISODE_NUM,\n",
    "                                                                                       repeat_times = 5,\n",
    "                                                                                       target_task = \"Taxi-v003\",\n",
    "                                                                                       source_task = \"Taxi-v4\",\n",
    "                                                                                       policy_library = [[q_func1,q_func2 ] for q_func1,q_func2 in zip(qlearn_q_table_v1, qlearn_q_table_v4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "#ops_norm_v14_003\n",
    "pl=[[q_func1,q_func2 ] for q_func1,q_func2 in zip(qlearn_q_table_v1, qlearn_q_table_v4)]\n",
    "ctuple = (0, 0.0049, 0.1, 0.2, 0.4,0.8, 1, 2, 4, 8, 16)\n",
    "ops_cnorm_episodes_length_v14_003,ops__cnorm_penalties_v14_003,ops_cnorm_rewards_v14_003,ops_cnorm_q_table_v14_003 = [0] * len(ctuple), [0]*len(ctuple),[0]*len(ctuple), [0]*len(ctuple)\n",
    "for i in range(len(ctuple)):\n",
    "    OPS_patial = functools.partial( OPS_Norm, c = ctuple[i])\n",
    "    ops_cnorm_episodes_length_v14_003[i], ops__cnorm_penalties_v14_003[i],ops_cnorm_rewards_v14_003[i], ops_cnorm_q_table_v14_003[i] = transform(target_rl_algo= OPS_patial,\n",
    "               episode_num = EPISODE_NUM,\n",
    "               repeat_times = REPEAT_TIMES,\n",
    "               target_task = \"Taxi-v003\",\n",
    "               policy_library = pl)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style = \"darkgrid\")\n",
    "plt.plot(range(EPISODE_NUM), smooth(qlearn_rewards_v003, 100), label = \"qlearn_v003\", color='k')\n",
    "for i in range(len(ctuple)):\n",
    "    plt.plot(range(EPISODE_NUM), smooth(ops_cnorm_rewards_v14_003[i], 100), label = \"c = \"+str(ctuple[i]))\n",
    "    \n",
    "plt.title(\"episode-reward graph\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"rewards\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RATIO = 20\n",
    "SMOOTH_RADIUS = 200\n",
    "\n",
    "\n",
    "for i in range(len(ctuple)):\n",
    "    plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_cnorm_rewards_v14_003[i][:EPISODE_NUM//RATIO], SMOOTH_RADIUS), label = \"c = \"+str(ctuple[i]))\n",
    "    \n",
    "plt.title(\"reward-epsidoe graph using OPS_norm different c\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"rewards\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ctuple)):\n",
    "    print(f\"c = {ctuple[i]}, reward = {smooth(ops_cnorm_rewards_v14_003[i][:EPISODE_NUM//20], 200)[0]}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.rcParams['font.sans-serif'] = ['SimHei']\n",
    "matplotlib.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "reward = [smooth( ops_cnorm_rewards_v14_003[i][:EPISODE_NUM//20], 200)[0] for i in range(len(ctuple)) ]\n",
    "\"\"\"\n",
    "绘制水平条形图方法barh\n",
    "参数一：y轴\n",
    "参数二：x轴\n",
    "\"\"\"\n",
    "plt.barh(range(len(ctuple)), reward, height=0.7, color='steelblue', alpha=0.8)      # 从下往上画\n",
    "plt.yticks(range(len(ctuple)), [f\"c={str(ctuple[i])}\" for i in range(len(ctuple))])\n",
    "plt.xlim(15,20)\n",
    "plt.xlabel(\"average reward \")\n",
    "plt.title(\"average reward over first 200 episode under different c\")\n",
    "for x, y in enumerate(reward):\n",
    "    plt.text(y + 0.2, x - 0.1, '%s' % y)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "sns.set(style = \"darkgrid\")\n",
    "\n",
    "\n",
    "plt.plot(range(EPISODE_NUM), smooth(qlearn_rewards_v003, 100), label = \"qlearn_v003\", color='k')\n",
    "#plt.plot(range(EPISODE_NUM), smooth(qlearn_rewards_v5, 100), label = \"qlearn_v5\")\n",
    "\n",
    "#plt.plot(range(EPISODE_NUM), smooth(qlearn_rewards_v1_3,100), label = \"qlearn_v1_3\",color='c')\n",
    "#plt.plot(range(EPISODE_NUM), smooth(qlearn_rewards_v4_3,100), label = \"qlearn_v4_3\",color='g')\n",
    "\n",
    "#plt.plot(range(EPISODE_NUM), smooth(pr_rewards_v4_3, 100), label = \"pr_v4_3\")\n",
    "#plt.plot(range(EPISODE_NUM), smooth(pr_rewards_v1_3, 100), label = \"pr_v1_3\")\n",
    "\n",
    "plt.plot(range(EPISODE_NUM), smooth(ops_rewards_v4_003, 100), label = \"ops_v4_003\", color='r')\n",
    "plt.plot(range(EPISODE_NUM), smooth(ops_rewards_v1_003, 100), label = \"ops_v1_003\", color='b')\n",
    "\n",
    "plt.plot(range(EPISODE_NUM), smooth(ops_rewards_v14_003, 100), label = \"ops_v14_003\", color='y')\n",
    "plt.plot(range(EPISODE_NUM), smooth(ops_norm_rewards_v14_003, 100), label = \"ops_norm_v14_003\", color='navy')\n",
    "\n",
    "plt.title(\"episode-reward graph\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"rewards\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "RATIO = 8\n",
    "SMOOTH_RADIUS = 25\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(qlearn_rewards_v003[:EPISODE_NUM//RATIO], SMOOTH_RADIUS), label = \"qlearn_v003\", color='k')\n",
    "#plt.plot(range(EPISODE_NUM), smooth(qlearn_rewards_v5, 100), label = \"qlearn_v5\")\n",
    "\n",
    "#plt.plot(range(EPISODE_NUM), smooth(qlearn_rewards_v1_3,100), label = \"qlearn_v1_3\",color='c')\n",
    "#plt.plot(range(EPISODE_NUM), smooth(qlearn_rewards_v4_3,100), label = \"qlearn_v4_3\",color='g')\n",
    "\n",
    "#plt.plot(range(EPISODE_NUM), smooth(pr_rewards_v4_3, 100), label = \"pr_v4_3\")\n",
    "#plt.plot(range(EPISODE_NUM), smooth(pr_rewards_v1_3, 100), label = \"pr_v1_3\")\n",
    "\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v4_003[:EPISODE_NUM//RATIO], SMOOTH_RADIUS), label = \"ops_v4_003\", color='r')\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v1_003[:EPISODE_NUM//RATIO], SMOOTH_RADIUS), label = \"ops_v1_003\", color='b')\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v14_003[:EPISODE_NUM//RATIO], SMOOTH_RADIUS), label = \"ops_v14_003\", color='y')\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_norm_rewards_v14_003[:EPISODE_NUM//RATIO], SMOOTH_RADIUS), label = \"ops_norm_v14_003\", color='navy')\n",
    "\n",
    "plt.title(\"episode-reward graph\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"rewards\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RATIO = 8\n",
    "SMOOTH_WINDOW = 20\n",
    "plt.plot(range(EPISODE_NUM//RATIO),smooth(qlearn_episodes_length_v003[:EPISODE_NUM//RATIO],SMOOTH_WINDOW), label = 'qlearn_v3')\n",
    "'''\n",
    "plt.plot(range(train_episodes),qlearn_episodes_length_v1_3, label = 'qlearn_v1_3')\n",
    "plt.plot(range(train_episodes),qlearn_episodes_length_v4_3, label = 'qlearn_v4_3')\n",
    "\n",
    "plt.plot(range(train_episodes),sarsa_episodes_length_v3, label = 'sarsa_v3')\n",
    "plt.plot(range(train_episodes),sarsa_episodes_length_v1_3, label = 'sarsa_v1_3')\n",
    "plt.plot(range(train_episodes),sarsa_episodes_length_v4_3, label = 'sarsa_v4_3')\n",
    "'''\n",
    "plt.plot(range(EPISODE_NUM//RATIO),smooth(ops_episodes_length_v1_003[:EPISODE_NUM//RATIO],SMOOTH_WINDOW), label = 'ops_v1_3')\n",
    "plt.plot(range(EPISODE_NUM//RATIO),smooth(ops_episodes_length_v4_003[:EPISODE_NUM//RATIO],SMOOTH_WINDOW), label = 'ops_v4_3')\n",
    "\n",
    "plt.title(\"episode-length graph\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"length\")\n",
    "plt.legend()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RATIO = 20\n",
    "SMOOTH_RADIUS = 20\n",
    "plt.plot(range(EPISODE_NUM//RATIO),smooth(qlearn_penalties_v03[:EPISODE_NUM//RATIO],SMOOTH_RADIUS), label = 'qlearn_v3')\n",
    "'''\n",
    "plt.plot(range(train_episodes),qlearn_episodes_length_v1_3, label = 'qlearn_v1_3')\n",
    "plt.plot(range(train_episodes),qlearn_episodes_length_v4_3, label = 'qlearn_v4_3')\n",
    "\n",
    "plt.plot(range(train_episodes),sarsa_episodes_length_v3, label = 'sarsa_v3')\n",
    "plt.plot(range(train_episodes),sarsa_episodes_length_v1_3, label = 'sarsa_v1_3')\n",
    "plt.plot(range(train_episodes),sarsa_episodes_length_v4_3, label = 'sarsa_v4_3')\n",
    "'''\n",
    "plt.plot(range(EPISODE_NUM//RATIO),smooth(ops_penalties_v1_03[:EPISODE_NUM//RATIO],SMOOTH_RADIUS), label = 'ops_v1_3')\n",
    "plt.plot(range(EPISODE_NUM//RATIO),smooth(ops_penalties_v4_03[:EPISODE_NUM//RATIO],SMOOTH_RADIUS), label = 'ops_v4_3')\n",
    "\n",
    "plt.title(\"episode-penalties graph\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"penalties\")\n",
    "plt.legend()\n",
    "plt.show\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def visit_all(dic):\n",
    "    for value in dic.values():\n",
    "        if value == 0:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "sum = 0\n",
    "times = 1000\n",
    "for i in range(times):    \n",
    "    dic = {i: 0 for i in range(100)}\n",
    "    while not visit_all(dic):\n",
    "        key = random.choice(range(len(dic)))\n",
    "        dic[key] = 1\n",
    "        sum += 1\n",
    "sum /= times\n",
    "print(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def div_add(n):\n",
    "    sum = 0\n",
    "    for i in range(1,n+1):\n",
    "        sum += 1/i\n",
    "        \n",
    "    return n * sum\n",
    "div_add(3000)/200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum = 0\n",
    "times = 1000\n",
    "for i in range(times):    \n",
    "    dic = {i: 0 for i in range(100)}\n",
    "    while dic[0] == 0:\n",
    "        key = random.choice(range(len(dic)))\n",
    "        dic[key] = 1\n",
    "        sum += 1\n",
    "sum /= times\n",
    "print(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_reuse(env, past_policy, train_episodes, fi = 1, mu = 0.95):\n",
    "        \n",
    "    # Hyper parameters\n",
    "    alpha = 0.05  #(alpha) is the learning rate (0<α≤1) \n",
    "    gamma = 0.95  # (gamma) is the discount factor (0≤γ≤1) \n",
    "    epsilon = 0.1 # the rate to act randomly in the epsilon-greedy alg for expolration in Q-learning\n",
    "    \n",
    "    # To plot learning curve\n",
    "    all_episodes_length = []\n",
    "    all_penalties = []\n",
    "    all_rewards = []\n",
    "\n",
    "    q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    " \n",
    "    past_table = copy.deepcopy(past_policy[0])\n",
    "    \n",
    "    \n",
    "    for i in range(train_episodes):\n",
    "        state = env.reset()\n",
    "\n",
    "        episode_penalties, episode_rewards, episode_length = 0, 0, 0\n",
    "        done = False\n",
    "        f = fi\n",
    "        while not done:\n",
    "            # epsilon greedy alg balancing exporation and exploitation\n",
    "            if random.uniform(0,1) < f:\n",
    "                action = arg_max(past_table[state])\n",
    "                #print(f\"f = {f} in episode {i} \\n\")\n",
    "            elif random.uniform(0,1) < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "                #print(\"random action \\n\")\n",
    "                \n",
    "            else:\n",
    "                action = arg_max(q_table[state])\n",
    "                #print(\"on the policy now \\n\")\n",
    "\n",
    "            # step to next state\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "\n",
    "            # update q-value\n",
    "            q_value = (1-alpha) * q_table[state, action] + alpha * (reward + gamma * np.max(q_table[next_state]))\n",
    "            q_table[state, action] = q_value\n",
    "\n",
    "            state = next_state\n",
    "            f = f*mu\n",
    "            #update data for learning curve\n",
    "            if reward == -10:\n",
    "                episode_penalties +=1\n",
    "\n",
    "            episode_rewards += reward\n",
    "            episode_length += 1\n",
    "\n",
    "        #record data for learning curve\n",
    "        all_episodes_length.append(episode_length)\n",
    "        all_penalties.append(episode_penalties)\n",
    "        all_rewards.append(episode_rewards)\n",
    "\n",
    "        #show training progress\n",
    "        if (i+1) % 100 == 0:\n",
    "            clear_output(wait=True)\n",
    "            print(f\"Episode {i}\")\n",
    "    print(\"Training finished \\n\")\n",
    "    \n",
    "    # return ndarray\n",
    "    all_episodes_length = np.array(all_episodes_length)\n",
    "    all_penalties = np.array(all_penalties)\n",
    "    all_rewards = np.array(all_rewards)\n",
    "    \n",
    "    return all_episodes_length, all_penalties, all_rewards, q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "sns.set(style = \"darkgrid\")\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(range(EPISODE_NUM), smooth(qlearn_rewards_v3, 100), label = \"qlearn_v3\", color='k')\n",
    "#plt.plot(range(EPISODE_NUM), smooth(qlearn_rewards_v5, 100), label = \"qlearn_v5\")\n",
    "\n",
    "#plt.plot(range(EPISODE_NUM), smooth(qlearn_rewards_v1_3,100), label = \"qlearn_v1_3\",color='c')\n",
    "#plt.plot(range(EPISODE_NUM), smooth(qlearn_rewards_v4_3,100), label = \"qlearn_v4_3\",color='g')\n",
    "\n",
    "#plt.plot(range(EPISODE_NUM), smooth(pr_rewards_v4_3, 100), label = \"pr_v4_3\")\n",
    "#plt.plot(range(EPISODE_NUM), smooth(pr_rewards_v1_3, 100), label = \"pr_v1_3\")\n",
    "\n",
    "plt.plot(range(EPISODE_NUM), smooth(ops_rewards_v4_3, 100), label = \"ops_v4_3\", color='r')\n",
    "plt.plot(range(EPISODE_NUM), smooth(ops_rewards_v1_3, 100), label = \"ops_v1_3\", color='b')\n",
    "\n",
    "plt.title(\"episode-reward graph\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"rewards\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "# training v03 from scratch\n",
    "\n",
    "qlearn_episodes_length_v03, qlearn_penalties_v03, qlearn_rewards_v03, qlearn_q_table_v03 = transform(target_rl_algo = q_learning,\n",
    "                                                                                     episode_num = EPISODE_NUM,  \n",
    "                                                                                     repeat_times = REPEAT_TIMES,\n",
    "                                                                                     target_task = \"Taxi-v03\",\n",
    "                                                                                     source_task = None\n",
    "                                                                                    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "# training v01 from scratch\n",
    "\n",
    "qlearn_episodes_length_v01, qlearn_penalties_v01, qlearn_rewards_v01, qlearn_q_table_v01 = transform(target_rl_algo = q_learning,\n",
    "                                                                                     episode_num = EPISODE_NUM,  \n",
    "                                                                                     repeat_times = REPEAT_TIMES,\n",
    "                                                                                     target_task = \"Taxi-v01\",\n",
    "                                                                                     source_task = None\n",
    "                                                                                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "# training v04 from scratch\n",
    "\n",
    "qlearn_episodes_length_v04, qlearn_penalties_v04, qlearn_rewards_v04, qlearn_q_table_v04 = transform(target_rl_algo = q_learning,\n",
    "                                                                                     episode_num = EPISODE_NUM,  \n",
    "                                                                                     repeat_times = REPEAT_TIMES,\n",
    "                                                                                     target_task = \"Taxi-v04\",\n",
    "                                                                                     source_task = None\n",
    "                                                                                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#ops_v04_03\n",
    "ops_episodes_length_v04_03, ops_penalties_v04_03,ops_rewards_v04_03, ops_q_table_v04_03 = transform(target_rl_algo= OPS_TL,\n",
    "                                                                                       episode_num = EPISODE_NUM,\n",
    "                                                                                       repeat_times = REPEAT_TIMES,\n",
    "                                                                                       target_task = \"Taxi-v03\",\n",
    "                                                                                       source_task = \"Taxi-v04\",\n",
    "                                                                                       policy_library = [[q_func] for q_func in qlearn_q_table_v04])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "#ops_v01_03\n",
    "ops_episodes_length_v01_03, ops_penalties_v01_03,ops_rewards_v01_03, ops_q_table_v01_03 = transform(target_rl_algo= OPS_TL,\n",
    "                                                                                       episode_num = EPISODE_NUM,\n",
    "                                                                                       repeat_times = REPEAT_TIMES,\n",
    "                                                                                       target_task = \"Taxi-v03\",\n",
    "                                                                                       source_task = \"Taxi-v01\",\n",
    "                                                                                       policy_library = [[q_func] for q_func in qlearn_q_table_v01])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(500), smooth(qlearn_rewards_v5[:500], 20), label = \"qlearn_v5\")\n",
    "#plt.plot(range(EPISODE_NUM), smooth(qlearn_rewards_v5, 100), label = \"qlearn_v5\")\n",
    "\n",
    "#plt.plot(range(EPISODE_NUM), smooth(qlearn_rewards_v1_3,100), label = \"qlearn_v1_3\")\n",
    "#plt.plot(range(EPISODE_NUM), smooth(qlearn_rewards_v4_3,100), label = \"qlearn_v4_3\")\n",
    "\n",
    "#plt.plot(range(EPISODE_NUM), smooth(pr_rewards_v4_3, 100), label = \"pr_v4_3\")\n",
    "#plt.plot(range(EPISODE_NUM), smooth(pr_rewards_v1_3, 100), label = \"pr_v1_3\")\n",
    "\n",
    "plt.plot(range(500), smooth(ops_rewards_v4_5[:500], 20), label = \"ops_v4_5\")\n",
    "plt.plot(range(500), smooth(ops_rewards_v1_5[:500], 20), label = \"ops_v1_5\")\n",
    "\n",
    "plt.title(\"episode-reward graph\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"rewards\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in qlearn_rewards_v3:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Taxi-v3\")\n",
    "r,_, _ = policy_evaluate(env, np.zeros([env.observation_space.n,env.action_space.n]), 1)\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(range(EPISODE_NUM//10 ),smooth(pr_rewards_v4_3[-EPISODE_NUM//10: ],100), label = 'pr_v4_3')\n",
    "plt.plot(range(EPISODE_NUM//10 ),smooth(pr_rewards_v4_3[-EPISODE_NUM//10: ],100), label = 'prql_v4_3')\n",
    "plt.plot(range(EPISODE_NUM//10), smooth(qlearn_rewards_v3[-EPISODE_NUM//10:], 100), label = \"qlearn_v3\")\n",
    "plt.plot(range(EPISODE_NUM//10), smooth(qlearn_rewards_v4[-EPISODE_NUM//10:], 100), label = \"qlearn_v4\")\n",
    "plt.plot(range(EPISODE_NUM//10), smooth(qlearn_rewards_v1_3[-EPISODE_NUM//10:],100), label = \"qlearn_v1_3\")\n",
    "\n",
    "plt.title(\"smoothed episode-reward graph: last {} episodes\".format(EPISODE_NUM//10))\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"reward\")\n",
    "plt.legend()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ops_episodes_length_v4_3, ops_penalties_v4_3,ops_rewards_v4_3, ops_q_table_v4_3 = transform(target_rl_algo= OPS_TL,\n",
    "                                                                                       episode_num = EPISODE_NUM,\n",
    "                                                                                       repeat_times = 10,\n",
    "                                                                                       target_task = \"Taxi-v3\",\n",
    "                                                                                       source_task = \"Taxi-v4\",\n",
    "                                                                                       policy_library = [[q_func] for q_func in qlearn_q_table_v4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "train_episodes = EPISODE_NUM\n",
    "\n",
    "plt.plot(range(train_episodes),smooth(qlearn_rewards_v3,100), 'k', label = 'qlearn_v3')\n",
    "plt.plot(range(train_episodes),smooth(qlearn_rewards_v1_3,100), label = 'qlearn_v1_3')\n",
    "plt.plot(range(train_episodes),smooth(qlearn_rewards_v4_3,100), label = 'qlearn_v4_3')\n",
    "\n",
    "\n",
    "plt.plot(range(train_episodes),smooth(pr_rewards_v1_3,100), label = 'pr_v1_3')\n",
    "plt.plot(range(train_episodes),smooth(pr_rewards_v4_3,100), label = 'pr_v4_3')\n",
    "\n",
    "\n",
    "plt.plot(range(train_episodes),smooth(ops_rewards_v1_3,100), label = 'ops_v1_3')\n",
    "plt.plot(range(train_episodes),smooth(ops_rewards_v4_3,100), label = 'ops_v4_3')\n",
    "\n",
    "plt.title(\"episode-reward graph\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"reward\")\n",
    "plt.legend()\n",
    "plt.show\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(train_episodes//10 ),smooth(qlearn_rewards_v3[-train_episodes//10: ],100), 'k',label = 'qlearn_v3')\n",
    "plt.plot(range(train_episodes//10 ),smooth(qlearn_rewards_v1_3[-train_episodes//10: ],100), label = 'qlearn_v1_3')\n",
    "plt.plot(range(train_episodes//10 ),smooth(qlearn_rewards_v4_3[-train_episodes//10: ],100), label = 'qlearn_v1_3')\n",
    "\n",
    "#plt.plot(range(train_episodes//10 ),smooth(sarsa_rewards_v3[-train_episodes//10: ],100), label = 'sarsa_v3')\n",
    "#plt.plot(range(train_episodes//10 ),smooth(sarsa_rewards_v1_3[-train_episodes//10: ],100), label = 'sarsa_v1_3')\n",
    "#plt.plot(range(train_episodes//10 ),smooth(sarsa_rewards_v4_3[-train_episodes//10: ],100), label = 'sarsa_v1_3')\n",
    "\n",
    "plt.plot(range(train_episodes//10 ),smooth(pr_rewards_v1_3[-train_episodes//10: ],100), label = 'pr_v1_3')\n",
    "plt.plot(range(train_episodes//10 ),smooth(pr_rewards_v4_3[-train_episodes//10: ],100), label = 'pr_v4_3')\n",
    "\n",
    "plt.plot(range(train_episodes//10 ),smooth(pr1_rewards_v1_3[-train_episodes//10: ],100), label = 'pr1_v1_3')\n",
    "plt.plot(range(train_episodes//10 ),smooth(pr1_rewards_v4_3[-train_episodes//10: ],100), label = 'pr1_v4_3')\n",
    "\n",
    "\n",
    "plt.title(\"smoothed episode-reward graph: last 1000 episodes\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"reward\")\n",
    "plt.legend()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(range(train_episodes//100),qlearn_rewards_v3[:train_episodes//100], label = 'qlearn_v3')\n",
    "plt.plot(range(train_episodes//100),qlearn_rewards_v1_3[:train_episodes//100], label = 'qlearn_v1_3')\n",
    "plt.plot(range(train_episodes//100),qlearn_rewards_v4_3[:train_episodes//100], label = 'qlearn_v4_3')\n",
    "\n",
    "plt.plot(range(train_episodes//100),sarsa_rewards_v3[:train_episodes//100], label = 'sarsa_v3')\n",
    "plt.plot(range(train_episodes//100),sarsa_rewards_v1_3[:train_episodes//100], label = 'sarsa_v1_3')\n",
    "plt.plot(range(train_episodes//100),sarsa_rewards_v4_3[:train_episodes//100], label = 'sarsa_v4_3')\n",
    "\n",
    "plt.title(\"episode-reward graph: first 1000 episodes\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"reward\")\n",
    "plt.legend()\n",
    "plt.show\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(train_episodes//100),smooth(qlearn_rewards_v3[:train_episodes//100],10), label = 'qlearn_v3')\n",
    "plt.plot(range(train_episodes//100),smooth(qlearn_rewards_v1_3[:train_episodes//100],10), label = 'qlearn_v1_3')\n",
    "plt.plot(range(train_episodes//100),smooth(qlearn_rewards_v4_3[:train_episodes//100],10), label = 'qlearn_v4_3')\n",
    "\n",
    "plt.plot(range(train_episodes//100),smooth(sarsa_rewards_v3[:train_episodes//100],10), label = 'sarsa_v3')\n",
    "plt.plot(range(train_episodes//100),smooth(sarsa_rewards_v1_3[:train_episodes//100],10), label = 'sarsa_v1_3')\n",
    "plt.plot(range(train_episodes//100),smooth(sarsa_rewards_v4_3[:train_episodes//100],10), label = 'sarsa_v4_3')\n",
    "\n",
    "plt.title(\"smoothed episode-reward graph: first 1000 episodes\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"reward\")\n",
    "plt.legend()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(train_episodes//100),smooth(qlearn_rewards_v3[:train_episodes//100],10), label = 'qlearn_v3')\n",
    "plt.plot(range(train_episodes//100),smooth(qlearn_rewards_v1_3[:train_episodes//100],10), label = 'qlearn_v1_3')\n",
    "plt.plot(range(train_episodes//100),smooth(qlearn_rewards_v4_3[:train_episodes//100],10), label = 'qlearn_v4_3')\n",
    "\n",
    "\n",
    "plt.title(\"q-learning smoothed episode-reward graph: first 1000 episodes\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"reward\")\n",
    "plt.legend()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(train_episodes//100),qlearn_episodes_length_v3[:train_episodes//100], label = 'qlearn_v3')\n",
    "plt.plot(range(train_episodes//100),qlearn_episodes_length_v1_3[:train_episodes//100], label = 'qlearn_v1_3')\n",
    "plt.plot(range(train_episodes//100),qlearn_episodes_length_v4_3[:train_episodes//100], label = 'qlearn_v4_3')\n",
    "\n",
    "plt.plot(range(train_episodes//100),sarsa_episodes_length_v3[:train_episodes//100], label = 'sarsa_v3')\n",
    "plt.plot(range(train_episodes//100),sarsa_episodes_length_v1_3[:train_episodes//100], label = 'sarsa_v1_3')\n",
    "plt.plot(range(train_episodes//100),sarsa_episodes_length_v4_3[:train_episodes//100], label = 'sarsa_v4_3')\n",
    "\n",
    "plt.title(\"episode-length graph: first 1000 episodes\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"length\")\n",
    "plt.legend()\n",
    "plt.show\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(train_episodes//100),smooth(qlearn_episodes_length_v3[:train_episodes//100],10), label = 'qlearn_v3')\n",
    "plt.plot(range(train_episodes//100),smooth(qlearn_episodes_length_v1_3[:train_episodes//100],10), label = 'qlearn_v1_3')\n",
    "plt.plot(range(train_episodes//100),smooth(qlearn_episodes_length_v4_3[:train_episodes//100],10), label = 'qlearn_v4_3')\n",
    "\n",
    "plt.plot(range(train_episodes//100),smooth(sarsa_episodes_length_v3[:train_episodes//100],10), label = 'sarsa_v3')\n",
    "plt.plot(range(train_episodes//100),smooth(sarsa_episodes_length_v1_3[:train_episodes//100],10), label = 'sarsa_v1_3')\n",
    "plt.plot(range(train_episodes//100),smooth(sarsa_episodes_length_v4_3[:train_episodes//100],10), label = 'sarsa_v4_3')\n",
    "\n",
    "plt.title(\"smoothed episode-length graph: first 1000 episodes\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"length\")\n",
    "plt.legend()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(train_episodes//100),smooth(qlearn_episodes_length_v3[:train_episodes//100],10), label = 'qlearn_v3')\n",
    "plt.plot(range(train_episodes//100),smooth(qlearn_episodes_length_v1_3[:train_episodes//100],10), label = 'qlearn_v1_3')\n",
    "plt.plot(range(train_episodes//100),smooth(qlearn_episodes_length_v4_3[:train_episodes//100],10), label = 'qlearn_v4_3')\n",
    "\n",
    "plt.title(\"q-learning smoothed episode-length graph: first 1000 episodes\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"length\")\n",
    "plt.legend()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(train_episodes//100),smooth(sarsa_episodes_length_v3[:train_episodes//100],10), label = 'sarsa_v3')\n",
    "plt.plot(range(train_episodes//100),smooth(sarsa_episodes_length_v1_3[:train_episodes//100],10), label = 'sarsa_v1_3')\n",
    "plt.plot(range(train_episodes//100),smooth(sarsa_episodes_length_v4_3[:train_episodes//100],10), label = 'sarsa_v4_3')\n",
    "\n",
    "plt.title(\"sarsa smoothed episode-length graph: first 1000 episodes\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"length\")\n",
    "plt.legend()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(train_episodes),qlearn_penalties_v3, label = 'qlearn_v3')\n",
    "plt.plot(range(train_episodes),qlearn_penalties_v1_3, label = 'qlearn_v1_3')\n",
    "plt.plot(range(train_episodes),qlearn_penalties_v4_3, label = 'qlearn_v4_3')\n",
    "\n",
    "plt.plot(range(train_episodes),sarsa_penalties_v3, label = 'sarsa_v3')\n",
    "plt.plot(range(train_episodes),sarsa_penalties_v1_3, label = 'sarsa_v1_3')\n",
    "plt.plot(range(train_episodes),sarsa_penalties_v4_3, label = 'sarsa_v4_3')\n",
    "\n",
    "plt.title(\"episode-penalties graph\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"penalties\")\n",
    "plt.legend()\n",
    "plt.show\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(train_episodes//100),qlearn_penalties_v3[:train_episodes//100], label = 'qlearn_v3')\n",
    "plt.plot(range(train_episodes//100),qlearn_penalties_v1_3[:train_episodes//100], label = 'qlearn_v1_3')\n",
    "plt.plot(range(train_episodes//100),qlearn_penalties_v4_3[:train_episodes//100], label = 'qlearn_v4_3')\n",
    "\n",
    "plt.plot(range(train_episodes//100),sarsa_penalties_v3[:train_episodes//100], label = 'sarsa_v3')\n",
    "plt.plot(range(train_episodes//100),sarsa_penalties_v1_3[:train_episodes//100], label = 'sarsa_v1_3')\n",
    "plt.plot(range(train_episodes//100),sarsa_penalties_v4_3[:train_episodes//100], label = 'sarsa_v4_3')\n",
    "\n",
    "plt.title(\"episode-penalties graph: first 1000 episodes\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"penalties\")\n",
    "plt.legend()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(train_episodes//100),smooth(qlearn_penalties_v3[:train_episodes//100],10), label = 'qlearn_v3')\n",
    "plt.plot(range(train_episodes//100),smooth(qlearn_penalties_v1_3[:train_episodes//100],10), label = 'qlearn_v1_3')\n",
    "plt.plot(range(train_episodes//100),smooth(qlearn_penalties_v4_3[:train_episodes//100],10), label = 'qlearn_v4_3')\n",
    "\n",
    "plt.plot(range(train_episodes//100),smooth(sarsa_penalties_v3[:train_episodes//100],10), label = 'sarsa_v3')\n",
    "plt.plot(range(train_episodes//100),smooth(sarsa_penalties_v1_3[:train_episodes//100],10), label = 'sarsa_v1_3')\n",
    "plt.plot(range(train_episodes//100),smooth(sarsa_penalties_v4_3[:train_episodes//100],10), label = 'sarsa_v4_3')\n",
    "\n",
    "plt.title(\"smoothed episode-penalties graph: first 1000 episodes\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"penalties \")\n",
    "plt.legend()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(train_episodes//100),smooth(qlearn_penalties_v3[:train_episodes//100],10), label = 'qlearn_v3')\n",
    "plt.plot(range(train_episodes//100),smooth(qlearn_penalties_v1_3[:train_episodes//100],10), label = 'qlearn_v1_3')\n",
    "plt.plot(range(train_episodes//100),smooth(qlearn_penalties_v4_3[:train_episodes//100],10), label = 'qlearn_v4_3')\n",
    "\n",
    "\n",
    "plt.title(\"q-learning smoothed episode-penalties graph: first 1000 episodes\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"penalties \")\n",
    "plt.legend()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(train_episodes//100),smooth(sarsa_penalties_v3[:train_episodes//100],10), label = 'sarsa_v3')\n",
    "plt.plot(range(train_episodes//100),smooth(sarsa_penalties_v1_3[:train_episodes//100],10), label = 'sarsa_v1_3')\n",
    "plt.plot(range(train_episodes//100),smooth(sarsa_penalties_v4_3[:train_episodes//100],10), label = 'sarsa_v4_3')\n",
    "\n",
    "plt.title(\"sarsa smoothed episode-penalties graph: first 1000 episodes\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"penalties \")\n",
    "plt.legend()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(run_times,policy_over_options,option_policies, q_table = None ,task = \"Taxi-v003\", mode = 'learning agent'):\n",
    "    \"\"\"Evaluate agent's performance after policy updates\"\"\"\n",
    "    env = gym.make(task)\n",
    "    if q_table is None:\n",
    "        list_length = 1\n",
    "    else:\n",
    "        list_length = len(q_table)\n",
    "    all_epochs, all_penalties, all_rewards, all_success_rate =  np.zeros(list_length), np.zeros(list_length),np.zeros(list_length),np.zeros(list_length)\n",
    "    all_epochs_std, all_penalties_std, all_rewards_std = np.zeros(list_length), np.zeros(list_length),np.zeros(list_length)\n",
    "    \n",
    "    \n",
    "        \n",
    "    for i in range( list_length ):\n",
    "        table_epochs, table_penalties, table_rewards = np.zeros(run_times), np.zeros(run_times),np.zeros(run_times)\n",
    "        if q_table is not None:\n",
    "            table = q_table[i]\n",
    "        success = 0\n",
    "        for j in range(run_times):\n",
    "            state = env.reset()\n",
    "            epochs, penalties, reward, sum_reward = 0, 0, 0, 0\n",
    "\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "                if mode == 'learning agent':\n",
    "                    action = arg_max(table[state])\n",
    "                elif mode == 'random agent':\n",
    "                    action = np.random.randint(0,env.action_space.n)\n",
    "                state, reward, done, info = env.step(action)\n",
    "\n",
    "                if reward == -10:\n",
    "                    penalties += 1\n",
    "                elif reward == 20:\n",
    "                    success += 1\n",
    "\n",
    "                epochs += 1\n",
    "                sum_reward += reward\n",
    "                show_frames(env,epochs,sum_reward )\n",
    "            table_penalties[j] = penalties\n",
    "            table_epochs[j] = epochs\n",
    "            table_rewards[j] = sum_reward\n",
    "\n",
    "        all_success_rate[i] = success/run_times    \n",
    "        all_epochs[i], all_penalties[i], all_rewards[i]  = np.mean(table_epochs), np.mean(table_penalties), np.mean(table_rewards)\n",
    "        all_epochs_std[i], all_penalties_std[i], all_rewards_std[i] = np.std(table_epochs), np.std(table_penalties), np.std(table_rewards)\n",
    "      \n",
    "    print(f\"Results after {run_times} runs:\")\n",
    "    print(f\"Success rate : {np.mean(all_success_rate)}\")\n",
    "    print(f\"Average  episode length : {np.mean(all_epochs)} ± {np.mean(all_epochs_std)}\")\n",
    "    print(f\"Average penalties per episode: {np.mean(all_penalties)} ± {np.mean(all_penalties_std)}\")\n",
    "    print(f\"Average rewards per episode: {np.mean(all_rewards)} ± {np.mean(all_rewards_std)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def op_evaluate(run_times,option_policies, option_terminations, policy_over_options, q_table = None ,task = \"Taxi-v003\", mode = 'learning agent'):\n",
    "    \"\"\"Evaluate agent's performance after policy updates\"\"\"\n",
    "    env = gym.make(task)\n",
    "    if q_table is None:\n",
    "        list_length = 1\n",
    "    else:\n",
    "        list_length = len(q_table)\n",
    "    all_epochs, all_penalties, all_rewards, all_success_rate =  np.zeros(list_length), np.zeros(list_length),np.zeros(list_length),np.zeros(list_length)\n",
    "    all_epochs_std, all_penalties_std, all_rewards_std = np.zeros(list_length), np.zeros(list_length),np.zeros(list_length)\n",
    "    \n",
    "    \n",
    "        \n",
    "    for i in range( list_length ):\n",
    "        table_epochs, table_penalties, table_rewards = np.zeros(run_times), np.zeros(run_times),np.zeros(run_times)\n",
    "        if q_table is not None:\n",
    "            table = q_table[i]\n",
    "        success = 0\n",
    "        for j in range(run_times):\n",
    "            state = env.reset()\n",
    "            epochs, penalties, reward, sum_reward = 0, 0, 0, 0\n",
    "\n",
    "            done = False\n",
    "            option = policy_over_options.evaluate(state)\n",
    "            while not done:\n",
    "                if mode == 'learning agent':\n",
    "                    if option_terminations[option].sample(state):\n",
    "                        option = policy_over_options.evaluate(state)\n",
    "\n",
    "                    action = option_policies[option].evaluate(state)\n",
    "                    \n",
    "                    \n",
    "                elif mode == 'random agent':\n",
    "                    action = np.random.randint(0,env.action_space.n)\n",
    "                state, reward, done, info = env.step(action)\n",
    "\n",
    "                if reward == -10:\n",
    "                    penalties += 1\n",
    "                elif reward == 20:\n",
    "                    success += 1\n",
    "\n",
    "                epochs += 1\n",
    "                sum_reward += reward\n",
    "                show_frames(env,epochs,sum_reward )\n",
    "            table_penalties[j] = penalties\n",
    "            table_epochs[j] = epochs\n",
    "            table_rewards[j] = sum_reward\n",
    "\n",
    "        all_success_rate[i] = success/run_times    \n",
    "        all_epochs[i], all_penalties[i], all_rewards[i]  = np.mean(table_epochs), np.mean(table_penalties), np.mean(table_rewards)\n",
    "        all_epochs_std[i], all_penalties_std[i], all_rewards_std[i] = np.std(table_epochs), np.std(table_penalties), np.std(table_rewards)\n",
    "      \n",
    "    print(f\"Results after {run_times} runs:\")\n",
    "    print(f\"Success rate : {np.mean(all_success_rate)}\")\n",
    "    print(f\"Average  episode length : {np.mean(all_epochs)} ± {np.mean(all_epochs_std)}\")\n",
    "    print(f\"Average penalties per episode: {np.mean(all_penalties)} ± {np.mean(all_penalties_std)}\")\n",
    "    print(f\"Average rewards per episode: {np.mean(all_rewards)} ± {np.mean(all_rewards_std)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i=1\n",
    "op_evaluate(3,option_policies[i], option_terminations[i], policy_over_options[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(1000,qlearn_q_table_v1_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(1000,qlearn_q_table_v4_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(1000,sarsa_q_table_v3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(1000,sarsa_q_table_v1_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(1000,sarsa_q_table_v4_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(1000, mode = 'random agent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# training Taxi-v3 reusing policy from Taxi-v1\n",
    "pr1_rewards_v1_3 = np.zeros(10000)\n",
    "for i in range(10):\n",
    "    pr1_episodes_length_v1_3, pr1_penalties_v1_3,rewards, pr1_q_table_v1_3 = prql(gym.make(\"Taxi-v3\"), qlearn_q_table_v1[i], 10000)\n",
    "    pr1_rewards_v1_3 += rewards\n",
    "pr1_rewards_v1_3/=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# training Taxi-v3 reusing policy from Taxi-v1\n",
    "pr1_rewards_v4_3 = np.zeros(10000)\n",
    "for i in range(10):\n",
    "    pr1_episodes_length_v4_3, pr1_penalties_v4_3,rewards, pr1_q_table_v4_3 = prql(gym.make(\"Taxi-v3\"), qlearn_q_table_v4[i], 10000)\n",
    "    pr1_rewards_v4_3 += rewards\n",
    "pr1_rewards_v4_3/=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "env = gym.make(\"Taxi-v4\")\n",
    "# training Taxi-v4 from scratch\n",
    "lprofiler = LineProfiler(q_learning)\n",
    "lprofiler.run('qlearn_episodes_length_v41, qlearn_penalties_v41, qlearn_rewards_v41, qlearn_q_table_v41 = q_learning(env,np.zeros([env.observation_space.n,env.action_space.n]),EPISODE_NUM)')\n",
    "lprofiler.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spinningup",
   "language": "python",
   "name": "spinningup"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
