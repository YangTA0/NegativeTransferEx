{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import copy\n",
    "import time\n",
    "import pandas as pd\n",
    "from line_profiler import LineProfiler\n",
    "from oc.utils import *\n",
    "from matplotlib import pyplot as plt\n",
    "from oc.fourrooms_copy import FourRooms\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nHere\\'s our restructured problem statement (from Gym docs):\\n\\n\"There are 4 locations (labeled by different letters), and our job is to pick up the passenger \\nat one location and drop him off at another. We receive +20 points for a successful drop-off and \\nlose 1 point for every time-step it takes. There is also a 10 point penalty for illegal pick-up \\nand drop-off actions.\"\\n\\n- The filled square represents the taxi, which is yellow without a passenger and green with a passenger.\\n- The pipe (\"|\") represents a wall which the taxi cannot cross.\\n- R, G, Y, B are the possible pickup and destination locations. The blue letter represents the current passenger\\n  pick-up location, and the purple letter is the current destination.\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#[env.id for env in gym.envs.registry.all() if env.id.startswith('Taxi')]\n",
    "import Taxi_v0, Taxi_v1, Taxi_v4\n",
    "import Taxi_v00, Taxi_v01, Taxi_v03, Taxi_v04\n",
    "import Taxi_v000,Taxi_v001,Taxi_v003,Taxi_v004\n",
    "env = gym.make(\"Taxi-v3\")\n",
    "env.render()\n",
    "\"\"\"\n",
    "Here's our restructured problem statement (from Gym docs):\n",
    "\n",
    "\"There are 4 locations (labeled by different letters), and our job is to pick up the passenger \n",
    "at one location and drop him off at another. We receive +20 points for a successful drop-off and \n",
    "lose 1 point for every time-step it takes. There is also a 10 point penalty for illegal pick-up \n",
    "and drop-off actions.\"\n",
    "\n",
    "- The filled square represents the taxi, which is yellow without a passenger and green with a passenger.\n",
    "- The pipe (\"|\") represents a wall which the taxi cannot cross.\n",
    "- R, G, Y, B are the possible pickup and destination locations. The blue letter represents the current passenger\n",
    "  pick-up location, and the purple letter is the current destination.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9dedb8539314dfe92696ede59ea2f01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "%autoreload 2\n",
    "#goals = [(8,8), (8,20), (20, 8), (20,20)]\n",
    "goals = [(27,27), (18,24), (24, 18), (20,20)]\n",
    "four_room_envs =[None] * 4\n",
    "for i in range(len(goals)):\n",
    "    four_room_envs[i] = FourRooms()\n",
    "    four_room_envs[i].reset()\n",
    "    four_room_envs[i].goal =  four_room_envs[i].tostate[goals[i]]\n",
    "    clear_output(True)\n",
    "    plt.subplot(2,2,i+1)\n",
    "    plt.imshow(four_room_envs[i].render(show_goal=True), cmap='Blues')\n",
    "    plt.axis('off')\n",
    "    plt.title('level ' + str(i))\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R:\u001b[43m \u001b[0m| : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env=gym.make(\"Taxi-v003\")\n",
    "env.seed(0)\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R:\u001b[43m \u001b[0m| : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gym.make(\"Taxi-v003\").render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "| : |\u001b[43m \u001b[0m: :\u001b[35mR\u001b[0m|\n",
      "| : | : :B|\n",
      "| : : : :\u001b[34;1mY\u001b[0m|\n",
      "| | : | : |\n",
      "| | : | :G|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gym.make(\"Taxi-v000\").render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | :\u001b[43m \u001b[0m:G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gym.make(\"Taxi-v3\").render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "\n",
      "Action space = Discrete(6)\n",
      "State space = Discrete(500)\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "env.render()\n",
    "print(\"Action space = {}\".format(env.action_space))\n",
    "print(\"State space = {}\".format(env.observation_space))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(1.0, 428, 0, False)],\n",
       " 1: [(1.0, 228, 0, False)],\n",
       " 2: [(1.0, 348, 0, False)],\n",
       " 3: [(1.0, 328, 0, False)],\n",
       " 4: [(1.0, 328, 0, False)],\n",
       " 5: [(1.0, 328, 0, False)]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.P[328] # {action: [(probability, nextstate, reward, done)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rng' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-47c79b39ac27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrng\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rng' is not defined"
     ]
    }
   ],
   "source": [
    "rng.choice(10)\n",
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 740,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 740,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map1 = gym.make(\"Taxi-v003\").desc.flatten()\n",
    "map2 = gym.make(\"Taxi-v001\").desc.flatten()\n",
    "from pyxdameraulevenshtein import damerau_levenshtein_distance, normalized_damerau_levenshtein_distance\n",
    "damerau_levenshtein_distance(map1, map2)  # expected result: 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 4s, sys: 0 ns, total: 1min 4s\n",
      "Wall time: 1min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyxdameraulevenshtein import damerau_levenshtein_distance, normalized_damerau_levenshtein_distance\n",
    "\n",
    "random.seed(1234)\n",
    "np.random.seed(1234)\n",
    "\n",
    "dl_list= []\n",
    "pass_locs = [(i,j*2) for i in range(6) for j in range(5)]\n",
    "obs_locs = [(i,j*2+1) for i in range(6) for j in range(4)]\n",
    "for i in range(10000):    \n",
    "    map1,map2 = np.zeros([6,10]),np.zeros([6,10])\n",
    "    for mapi in [map1,map2]:\n",
    "        for pos in  np.random.choice(range(30),4,replace = False):\n",
    "            pos = pass_locs[pos]\n",
    "            mapi[pos[0]][pos[1]] = 1\n",
    "        for pos in obs_locs:\n",
    "            mapi[pos[0]][pos[1]] = np.random.randint(2,4)\n",
    "    dl_list.append(damerau_levenshtein_distance(map1.flatten(), map2.flatten())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_P(MAP):\n",
    "    desc = np.asarray(MAP, dtype='c')\n",
    "    locR, locG, locY, locB = np.asarray(desc == b'R').nonzero(), np.asarray(desc == b'G').nonzero(),np.asarray(desc == b'Y').nonzero(),np.asarray(desc == b'B').nonzero()\n",
    "    #(loc[0][0] , loc[1][0] ) contains (array([x]),array([y])\n",
    "    locs = [(locR[0][0] - 1 , locR[1][0] //2), (locG[0][0] -1 , locG[1][0] //2 ), (locY[0][0] -1  , locY[1][0] //2), (locB[0][0] -1 , locB[1][0] //2)]\n",
    "\n",
    "    num_states = 500\n",
    "    num_rows = 5\n",
    "    num_columns = 5\n",
    "    max_row = num_rows - 1\n",
    "    max_col = num_columns - 1\n",
    "    num_actions = 6\n",
    "    P = {state: {action: []\n",
    "                 for action in range(num_actions)} for state in range(num_states)}\n",
    "    for row in range(num_rows):\n",
    "        for col in range(num_columns):\n",
    "            for pass_idx in range(len(locs) + 1):  # +1 for being inside taxi\n",
    "                for dest_idx in range(len(locs)):\n",
    "                    state = encode(row, col, pass_idx, dest_idx)\n",
    "                    for action in range(num_actions):\n",
    "                        # defaults\n",
    "                        new_row, new_col, new_pass_idx = row, col, pass_idx\n",
    "                        reward = 0  # default reward when there is no pickup/dropoff\n",
    "                        done = False\n",
    "                        taxi_loc = (row, col)\n",
    "\n",
    "                        if action == 0:\n",
    "                            new_row = min(row + 1, max_row)\n",
    "                        elif action == 1:\n",
    "                            new_row = max(row - 1, 0)\n",
    "                        if action == 2 and desc[1 + row, 2 * col + 2] == b\":\":\n",
    "                            new_col = min(col + 1, max_col)\n",
    "                        elif action == 3 and desc[1 + row, 2 * col] == b\":\":\n",
    "                            new_col = max(col - 1, 0)\n",
    "                        elif action == 4:  # pickup\n",
    "                            if (pass_idx < 4 and taxi_loc == locs[pass_idx]):\n",
    "                                new_pass_idx = 4\n",
    "                            else:  # passenger not at location\n",
    "                                reward = 0\n",
    "                        elif action == 5:  # dropoff\n",
    "                            if (taxi_loc == locs[dest_idx]) and pass_idx == 4:\n",
    "                                new_pass_idx = dest_idx\n",
    "                                done = True\n",
    "                                reward = 20\n",
    "                            elif (taxi_loc in locs) and pass_idx == 4:\n",
    "                                new_pass_idx = locs.index(taxi_loc)\n",
    "                            else:  # dropoff at wrong location\n",
    "                                reward = 0\n",
    "                        new_state = encode(\n",
    "                            new_row, new_col, new_pass_idx, dest_idx)\n",
    "                        P[state][action].append(\n",
    "                            (1.0, new_state, reward, done))\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode( taxi_row, taxi_col, pass_loc, dest_idx):\n",
    "    # (5) 5, 5, 4\n",
    "    i = taxi_row\n",
    "    i *= 5\n",
    "    i += taxi_col\n",
    "    i *= 5\n",
    "    i += pass_loc\n",
    "    i *= 4\n",
    "    i += dest_idx\n",
    "    return i\n",
    "\n",
    "def decode(i):\n",
    "    out = []\n",
    "    out.append(i % 4)\n",
    "    i = i // 4\n",
    "    out.append(i % 5)\n",
    "    i = i // 5\n",
    "    out.append(i % 5)\n",
    "    i = i // 5\n",
    "    out.append(i)\n",
    "    assert 0 <= i < 5\n",
    "    return reversed(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_metric(dist_table):\n",
    "    return np.sum(np.min(states_dis2,axis= 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_random_maps(num,seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    dl_list= []\n",
    "    pass_locs = [(i+1,j*2+1) for i in range(6) for j in range(5)]\n",
    "    obs_locs = [(i+1,j*2+2) for i in range(6) for j in range(4)]\n",
    "    obs = [':','|']\n",
    "    maps = []\n",
    "    for i in range(num):    \n",
    "        MAP =  [\n",
    "        \"+---------+\",\n",
    "        \"| : | : : |\",\n",
    "        \"| : | : : |\",\n",
    "        \"| : : : : |\",\n",
    "        \"| | : | : |\",\n",
    "        \"| | : | : |\",\n",
    "        \"+---------+\",\n",
    "        ]  \n",
    "        MAP = np.asarray(MAP,dtype = 'c')\n",
    "        for pos, loc in  zip( np.random.choice(range(30),4,replace = False), ['R','G','Y','B']):\n",
    "            pos = pass_locs[pos]\n",
    "            MAP[pos[0]][pos[1]] = loc\n",
    "        for pos in obs_locs:\n",
    "            MAP[pos[0]][pos[1]] = obs[np.random.randint(0,2)]\n",
    "        maps.append(MAP)\n",
    "    return maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'random' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-214de518d8d7>\u001b[0m in \u001b[0;36mmake_random_maps\u001b[0;34m(num, seed)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake_random_maps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdl_list\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mpass_locs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'random' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyxdameraulevenshtein import damerau_levenshtein_distance, normalized_damerau_levenshtein_distance\n",
    "dl_dists= []\n",
    "huasdorff_dists = []\n",
    "kt_dists = []\n",
    "maps = make_random_maps(20,2323)\n",
    "for i in range(len(maps)):\n",
    "    clear_output()\n",
    "    print(f\"iteration: {i}\")\n",
    "    for j in range(i+1, len(maps)):\n",
    "\n",
    "        next_states_dis, hausdorff_dis, kt_dis = state_dis(generate_P(maps[i]), generate_P(maps[j]))\n",
    "        huasdorff_dists.append(hausdorff_dis)\n",
    "        kt_dists.append(kt_dis)\n",
    "        dl_dists.append(damerau_levenshtein_distance(maps[i].flatten(), maps[j].flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 12/1000 [00:17<24:39,  1.50s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-336-488e9e354376>\u001b[0m in \u001b[0;36mstate_dis\u001b[0;34m(p1, p2, epsilon, nsteps)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                     \u001b[0memd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates_dis\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate_action1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate_action2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# d(s_i^{next_state} , s_j^{next_state})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                     \u001b[0mdis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_action1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstate_action2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0memd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;31m#                     if dis > 0:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m#                          print(i,j,dis)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dl_dists2= []\n",
    "huasdorff_dists2 = []\n",
    "kt_dists2 = []\n",
    "maps1 = make_random_maps(19,1234)\n",
    "maps2 = make_random_maps(10,4567)\n",
    "for i in range(len(maps1)):\n",
    "    clear_output(wait=True)\n",
    "    print(f\"iteration: {i}\")\n",
    "    for j in range(len(maps2)):\n",
    "        next_states_dis, hausdorff_dis, kt_dis = state_dis(generate_P(maps1[i]), generate_P(maps2[j]))\n",
    "        huasdorff_dists2.append(hausdorff_dis)\n",
    "        kt_dists2.append(kt_dis)\n",
    "        dl_dists2.append(damerau_levenshtein_distance(maps1[i].flatten(), maps2[j].flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: module://ipympl.backend_nbagg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b69091e1a8e5448cb9dea84eb9c13fb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'damerau levenshtein distance')"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib\n",
    "fig,ax = plt.subplots()\n",
    "ax.hist(dl_dists,10)\n",
    "ax.set_ylabel('counts')\n",
    "ax.set_xlabel('damerau levenshtein distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: module://ipympl.backend_nbagg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59ca3ba0beb14ad5aeb439ee19ab8f7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'damerau levenshtein distance')"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib\n",
    "fig,ax = plt.subplots()\n",
    "ax.hist(dl_dists2,10)\n",
    "ax.set_ylabel('counts')\n",
    "ax.set_xlabel('damerau levenshtein distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def state_dis(p1,p2,epsilon = 0.01, nsteps = 1000):\n",
    "    # p {state:{action:{[(1.0, new_state, reward, done)]}}} \n",
    "    assert len(p1[0]) == len(p2[0]),'transitions have different action spaces'\n",
    "    actions = range(len(p1[0]))\n",
    "    states_dis, next_states_dis = np.zeros([len(p1), len(p2)]), np.zeros([len(p1), len(p2)])\n",
    "    epsilon = epsilon\n",
    "    gamma = 0.95\n",
    "    step = 0\n",
    "    \n",
    "    for step in tqdm(range(nsteps)):\n",
    "        for i  in range(len(p1)):\n",
    "            for j in range(len(p2)):\n",
    "                # d(si,sj) = max{cr*|rsi - rsj| + ct*Tk(d)(si,sj)} with ct =gamma, cr = 1-gamma , Tkd A.K.A emd\n",
    "                dists = []\n",
    "                for action in actions:\n",
    "                    # print(i,j,action)\n",
    "                    state_action1=p1[i][action][0] # contains (1.0, new_state, reward, done)\n",
    "                    state_action2=p2[j][action][0]\n",
    "                    \n",
    "                    emd = abs(states_dis[state_action1[1]][state_action2[1]]) # d(s_i^{next_state} , s_j^{next_state})\n",
    "                    dis = (1-gamma)*abs(state_action1[2] - state_action2[2]) + gamma*emd \n",
    "#                     if dis > 0:\n",
    "#                          print(i,j,dis)\n",
    "                    dists.append(dis)\n",
    "                next_states_dis[i][j] = max(dists)\n",
    "        if np.max(abs(next_states_dis - states_dis) ) < epsilon:\n",
    "\n",
    "            break   \n",
    "        states_dis = next_states_dis\n",
    "        next_states_dis = np.zeros([len(p1), len(p2)])\n",
    "        step += 1 \n",
    "#         clear_output(wait=True)\n",
    "#         print(f\"step: {step}\")\n",
    "    kt_dis = np.trace(next_states_dis)\n",
    "    hausdorff_dis = max( np.max(np.min(next_states_dis,axis = 0)), np.max(np.min(next_states_dis,axis = 1)) )    \n",
    "    return next_states_dis, hausdorff_dis, kt_dis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/1000 [00:04<24:45,  1.49s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-334-cae90d7746be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0menv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Taxi-v001\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0menv2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Taxi-v003\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mstate_dis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0menv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-327-488e9e354376>\u001b[0m in \u001b[0;36mstate_dis\u001b[0;34m(p1, p2, epsilon, nsteps)\u001b[0m\n\u001b[1;32m     14\u001b[0m                 \u001b[0;31m# d(si,sj) = max{cr*|rsi - rsj| + ct*Tk(d)(si,sj)} with ct =gamma, cr = 1-gamma , Tkd A.K.A emd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                 \u001b[0mdists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m                     \u001b[0;31m# print(i,j,action)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                     \u001b[0mstate_action1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# contains (1.0, new_state, reward, done)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env1 = gym.make(\"Taxi-v001\")\n",
    "env2 = gym.make(\"Taxi-v003\")\n",
    "state_dis(env1.P,env2.P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_rate(lc_no_trans, lc_trans):\n",
    "    threshold = np.max(lc_no_trans)\n",
    "    \n",
    "    points = np.where(lc_trans >= threshold)\n",
    "    if len(*points) == 0:\n",
    "        return - threshold / lc_trans[-1] \n",
    "    else:\n",
    "        \n",
    "        time_trans = np.min( np.where(lc_trans >= threshold))\n",
    "    time_no_trans = np.min( np.where(lc_no_trans >= threshold) )\n",
    "    transfer_rate = (time_no_trans - time_trans) / time_no_trans\n",
    "    return transfer_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics_val(learning_curve_no_trans, learning_curve_trans):\n",
    "    \"\"\"\n",
    "    get the value of 5 metrics\n",
    "    1.jumpstart\n",
    "    2.asympotic performance \n",
    "    3.time to threshold\n",
    "    4.accumulated rewards\n",
    "    5.transfer rate\n",
    "    \"\"\"\n",
    "    lc_no_trans = learning_curve_no_trans\n",
    "    lc_trans = learning_curve_trans\n",
    "    \n",
    "    #jumpstart\n",
    "    jumpstart = lc_trans[0] - lc_no_trans[0]\n",
    "    \n",
    "    #asympotic performance \n",
    "    asympotic = lc_trans[-1] - lc_no_trans[-1]\n",
    "    \n",
    "    # time to threshold(convergence)\n",
    "    threshold = np.max(lc_no_trans)    \n",
    "    points = np.where(lc_trans >= threshold)\n",
    "    if len(*points) == 0:\n",
    "        time_trans = float('inf')\n",
    "    else:       \n",
    "        time_trans = np.min( np.where(lc_trans >= threshold))\n",
    "    time_no_trans = np.min( np.where(lc_no_trans >= threshold) )\n",
    "    time_to_threshold = time_no_trans - time_trans\n",
    "    \n",
    "    # accumulated rewards\n",
    "    acc_rewards = np.sum(lc_trans - lc_no_trans)\n",
    "    \n",
    "    #transfer rate \n",
    "    trans_rate = (time_no_trans - time_trans) / time_no_trans\n",
    "    \n",
    "    return trans_rate, jumpstart, asympotic, time_to_threshold, acc_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(lc_no_transfer, lc_transfer):\n",
    "    assert lc_no_transfer.shape[0] == lc_transfer.shape[0],\\\n",
    "    \"two learning curve must have same run times\"\n",
    "    nruns =  lc_no_transfer.shape[0]\n",
    "    trans_rate, jumpstart, asympotic, time_to_threshold, acc_rewards = [np.zeros(nruns), np.zeros(nruns),np.zeros(nruns),np.zeros(nruns),np.zeros(nruns)]\n",
    "    for i in range(nruns):\n",
    "        trans_rate[i], jumpstart[i], asympotic[i], time_to_threshold[i], acc_rewards[i] = get_metrics_val(lc_no_transfer[i], lc_transfer[i])\n",
    "    res = { 'Transfer rate': trans_rate,\n",
    "            'Jumpstart': jumpstart,\n",
    "            'Asympotic performace': asympotic,\n",
    "            'Time to threshold': time_to_threshold,\n",
    "            'Accumulated rewards': acc_rewards       \n",
    "           }\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_color_dict(data):\n",
    "    keys = [key for key in data.keys()]\n",
    "    \n",
    "    trans_settings = set()\n",
    "    algs = set()\n",
    "    for key in keys:\n",
    "        temp = key.split('_')\n",
    "        trans_settings.add(temp[0])\n",
    "        algs.add(temp[2])\n",
    "    trans_settings = list(trans_settings) \n",
    "    algs = list(algs)\n",
    "    trans_settings.sort(reverse = True)\n",
    "    algs.sort(reverse = True)\n",
    "    cmap_names =['Blues', 'Reds', 'Greens']\n",
    "    color_dict = {}\n",
    "    for i, trans_setting in enumerate(trans_settings):\n",
    "        for j, alg in enumerate(algs):\n",
    "            assert len(algs) > 0, 'data is empty'\n",
    "            color = plt.get_cmap(cmap_names[i%len(cmap_names)])(0.8 - 0.8 / len(algs) * j +0.1)\n",
    "            if trans_setting not in color_dict:\n",
    "                color_dict.update({trans_setting:{alg: color}})\n",
    "            elif alg not in color_dict[trans_setting]:\n",
    "                color_dict[trans_setting].update({alg: color})\n",
    "    return color_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_result(data, nepisodes, smooth_radius=50, combine_figures = True, average_group = True, show_std = True, figsize = None, title=\"episode-reward graph\", xlabel=\"episode\", ylabel = \"rewards\"):\n",
    "   \n",
    "    plt.rcParams['savefig.dpi'] = 600\n",
    "    data = copy.deepcopy(data)\n",
    "    color_dict = make_color_dict(data)\n",
    "    # plot n runs as only 1 curve\n",
    "    if average_group:\n",
    "        for label,rewards in data.items():\n",
    "            mean = np.mean(rewards, axis = 0)\n",
    "            std = np.std(rewards, axis = 0)\n",
    "            data[label]= {'mean': mean, 'std': std}\n",
    "            \n",
    "   \n",
    "    if combine_figures:\n",
    "        f, axarr = plt.subplots(1, 1, sharex=False, squeeze=False, figsize=figsize)\n",
    "        ax= axarr[0][0]\n",
    "        assert average_group == True, \"average_group should be True when combine_figures is True\"\n",
    "        for label in data.keys():\n",
    "                color = color_dict[label.split('_')[0]][label.split('_')[2]]\n",
    "                y = data[label]['mean'][:nepisodes]\n",
    "                ystd = data[label]['std'][:nepisodes]\n",
    "                #print(content.shape)\n",
    "                ax.plot(range(nepisodes), smooth(y,smooth_radius),color = color,label = label)\n",
    "                if show_std:\n",
    "                    ax.fill_between(range(nepisodes), smooth(y -  ystd,smooth_radius), smooth(y +  ystd,smooth_radius), color = color, alpha = 0.2)\n",
    "                ax.set_xlabel(xlabel)\n",
    "                ax.set_ylabel(ylabel)\n",
    "                if title == \"episode-reward graph\":\n",
    "                    ax.set_title(label.split('_')[2] +' ' + label.split('_')[-2] + \":  \" +title)\n",
    "                else:\n",
    "                    ax.set_title(title)\n",
    "                ax.legend()\n",
    "    else:\n",
    "        f, axarr = plt.subplots(len(data), 1, sharex=True, sharey=True, squeeze=False, figsize=figsize)\n",
    "        for idx,label in enumerate(data.keys()):\n",
    "            ax = axarr[idx][0]            \n",
    "            color = color_dict[label.split('_')[0]][label.split('_')[2]]\n",
    "            \n",
    "            \n",
    "            if average_group:\n",
    "               \n",
    "                y = data[label]['mean'][:nepisodes]\n",
    "                ystd = data[label]['std'][:nepisodes]\n",
    "                ax.plot(range(nepisodes), smooth(data[label]['mean'][:nepisodes],smooth_radius),color = color,label = label)\n",
    "                if show_std:\n",
    "                    ax.fill_between(range(nepisodes), smooth(y -  ystd,smooth_radius), smooth(y +  ystd,smooth_radius), color = color, alpha = 0.2)\n",
    "                ax.set_xlabel(xlabel)\n",
    "                ax.set_ylabel(ylabel)\n",
    "                ax.set_title(label+ \":  \" +title)\n",
    "                ax.legend()\n",
    "                \n",
    "            else:\n",
    "                for i,reward in enumerate(data[label]):\n",
    "                    if i==0:\n",
    "                        ax.plot(range(nepisodes),smooth(reward[:nepisodes],smooth_radius),label = label)\n",
    "                        ax.set_xlabel(xlabel)\n",
    "                        ax.set_ylabel(ylabel)\n",
    "                        ax.set_title(label+ \":  \" +title)\n",
    "                        ax.legend()\n",
    "                    else:\n",
    "                        ax.plot(range(nepisodes),smooth(reward[:nepisodes],smooth_radius),label = None)\n",
    "\n",
    "    plt.tight_layout()\n",
    "                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arg_max(state_action):\n",
    "    max_index_list = []\n",
    "    max_value = state_action[0]\n",
    "    for index, value in enumerate(state_action):\n",
    "        if value > max_value:\n",
    "            max_index_list.clear()\n",
    "            max_value = value\n",
    "            max_index_list.append(index)\n",
    "        elif value == max_value:\n",
    "            max_index_list.append(index)\n",
    "    return random.choice(max_index_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_zero_state(q_table):\n",
    "    zero_state = 0\n",
    "    for arr_2d in q_table:\n",
    "         zero_state += np.sum(np.sum(arr_2d,axis = 1) == 0)\n",
    "    return zero_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "def saveData(data, path):\n",
    "    output = open(path, 'wb')\n",
    "    # Pickle dictionary using protocol 0.\n",
    "    pickle.dump(data, output)\n",
    "    output.close()\n",
    "    \n",
    "def loadData(path):\n",
    "    pkl_file = open(path, 'rb')\n",
    "    segContent = pickle.load(pkl_file)\n",
    "    pkl_file.close()\n",
    "    return segContent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "process = subprocess.Popen(\"ls\", shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODE_NUM = 4000\n",
    "MAX_EPISODE_LEN = 200\n",
    "REPEAT_TIMES = 10  # train agent REAPEAT_TIMES to get averaged learning curves\n",
    "EVALUATION_TIMES = 10 # evaluate target policy EVALUATION_TIMES after x updates in off-policy RL algorithms\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "def show_frames(env,j ,i, episode_rewards ):\n",
    "    clear_output(wait=True)\n",
    "    env.render()\n",
    "    print(f\"episode {j+1}  step {i}  rewards={episode_rewards}\")\n",
    "    sleep(0.1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(y, radius):\n",
    "    '''\n",
    "    smooth data y by averaging the values in each window [max{index-radius,0}, min{(index+radius), len(y)-1}] \n",
    "    \n",
    "    '''\n",
    "    if (len(y) < 2 * radius + 1):\n",
    "        return np.mean(y) * np.ones_like(y)\n",
    "    else:\n",
    "        convkernel = np.ones(2 * radius + 1)\n",
    "        out = np.convolve(y, convkernel, mode = 'same') / np.convolve(np.ones_like(y), convkernel, mode = 'same')\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluate(env, policy, times = 10):\n",
    "    # store data, make average and return\n",
    "    all_length, all_penalties, all_rewards = np.zeros(times), np.zeros(times), np.zeros(times)\n",
    "\n",
    "    for i in range(times):\n",
    "\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        episode_penalties, episode_rewards, episode_length = 0, 0, 0\n",
    "        \n",
    "        for j in range(MAX_EPISODE_LEN):\n",
    "            action = arg_max(policy[state])\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            state = next_state\n",
    "            #update data for learning curve\n",
    "            if reward == -10:\n",
    "                episode_penalties +=1\n",
    "\n",
    "            episode_rewards += reward\n",
    "            episode_length += 1\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        all_rewards[i] = episode_rewards\n",
    "        all_penalties[i] = episode_penalties\n",
    "        all_length[i] = episode_length\n",
    "    return np.mean(all_rewards), np.mean(all_penalties), np.mean(all_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.randint(0,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qlearn(env, train_episodes, init_q_table=None):\n",
    "    \"\"\"\n",
    "    Training the agent\n",
    "    Q(state,action)←(1−α)Q(state,action)+α(reward+γmaxaQ(next state,all actions))\n",
    "    \"\"\"  \n",
    "    \n",
    "    # Hyper parameters\n",
    "    alpha = 0.05  #(alpha) is the learning rate (0<α≤1) \n",
    "    gamma = 0.95  # (gamma) is the discount factor (0≤γ≤1) \n",
    "    epsilon = 0.1 # the rate to act randomly in the epsilon-greedy alg for expolration in Q-learning\n",
    "\n",
    "    timesteps =  train_episodes * MAX_EPISODE_LEN\n",
    "\n",
    "    # To plot learning curve\n",
    "    all_episodes_length = []\n",
    "    all_penalties = []\n",
    "    all_rewards = []\n",
    "    \n",
    "    if init_q_table is None:\n",
    "        q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "    else:\n",
    "        q_table = copy.deepcopy(init_q_table)\n",
    "    \n",
    "    env_copy = copy.deepcopy(env) # for policy_evaluate\n",
    "    i = 0\n",
    "    while i < timesteps:\n",
    "        env.seed(random.randint(0,timesteps))\n",
    "        state = env.reset()\n",
    "\n",
    "        episode_penalties, episode_rewards, episode_length = 0, 0, 0\n",
    "        done = False\n",
    "\n",
    "        while not done and i < timesteps:\n",
    "            # epsilon greedy alg balancing exporation and exploitation\n",
    "            if random.uniform(0,1)< epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = arg_max(q_table[state])\n",
    "\n",
    "            # step to next state\n",
    "            next_state, reward, done, info = env.step(action)       \n",
    "            \n",
    "            # update q-value\n",
    "            q_value = (1-alpha) * q_table[state, action] + alpha * (reward + gamma * np.max(q_table[next_state]))\n",
    "            q_table[state, action] = q_value\n",
    "\n",
    "            state = next_state\n",
    "            if i%200 == 0:\n",
    "                #evaluate policy for learning curve after each episode\n",
    "                episode_rewards, episode_penalties, episode_length = policy_evaluate(env_copy, q_table, EVALUATION_TIMES)\n",
    "                \n",
    "                all_episodes_length.append(episode_length)\n",
    "                all_penalties.append(episode_penalties)\n",
    "                all_rewards.append(episode_rewards)\n",
    "                \n",
    "                #show training progress\n",
    "                clear_output(wait=True)\n",
    "                print(f\"timesteps: {i}\")\n",
    "            i += 1\n",
    "         \n",
    "    print(f\"Training finished\")\n",
    "    \n",
    "    # return ndarray\n",
    "    all_episodes_length = np.array(all_episodes_length)\n",
    "    all_penalties = np.array(all_penalties)\n",
    "    all_rewards = np.array(all_rewards)\n",
    "    \n",
    "    return all_episodes_length, all_penalties, all_rewards, q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(env,  train_episodes,init_s_table = None):\n",
    "    \"\"\"Training the agent\n",
    "    Q(state,action)←(1−α)Q(state,action)+α(reward+γQ(next state,next action))\n",
    "\n",
    "    \"\"\"  \n",
    "    # Hyper parameters\n",
    "    alpha = 0.05  #(alpha) is the learning rate (0<α≤1) \n",
    "    gamma = 0.95  # (gamma) is the discount factor (0≤γ≤1) \n",
    "    epsilon = 0.1 # the rate to act randomly in the epsilon-greedy alg for expolration in Q-learning\n",
    "\n",
    "    train_episodes =  train_episodes \n",
    "    timesteps =  train_episodes * MAX_EPISODE_LEN\n",
    "    # To plot learning curve\n",
    "    all_episodes_length = []\n",
    "    all_penalties = []\n",
    "    all_rewards = []\n",
    "    \n",
    "    \n",
    "    if init_s_table is None:\n",
    "        s_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "    else:\n",
    "        s_table = copy.deepcopy(init_s_table)\n",
    "    env_copy = copy.deepcopy(env)\n",
    "    \n",
    "    i = 0\n",
    "    while i < timesteps:\n",
    "        env.seed(random.randint(0,timesteps))\n",
    "        state = env.reset()\n",
    "\n",
    "        episode_penalties, episode_rewards, episode_length = 0, 0, 0\n",
    "        done = False\n",
    "\n",
    "        # epsilon greedy alg balancing exporation and exploitation\n",
    "        if random.uniform(0,1)< epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = arg_max(s_table[state])\n",
    "\n",
    "        while not done and i < timesteps:\n",
    "\n",
    "            # step to next state\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "\n",
    "            # choose next action\n",
    "            if random.uniform(0,1)< epsilon:\n",
    "                next_action = env.action_space.sample()\n",
    "            else:\n",
    "                next_action = arg_max(s_table[next_state])\n",
    "\n",
    "            # update q-value\n",
    "            q_value = (1-alpha) * s_table[state, action] + alpha * (reward + gamma * s_table[next_state,next_action])\n",
    "            s_table[state, action] = q_value\n",
    "\n",
    "            state = next_state\n",
    "            action = next_action\n",
    "\n",
    "            if i%200 == 0:\n",
    "                #evaluate policy for learning curve after each episode\n",
    "                episode_rewards, episode_penalties, episode_length = policy_evaluate(env_copy, s_table, EVALUATION_TIMES)\n",
    "                \n",
    "                all_episodes_length.append(episode_length)\n",
    "                all_penalties.append(episode_penalties)\n",
    "                all_rewards.append(episode_rewards)\n",
    "                \n",
    "                #show training progress\n",
    "                clear_output(wait=True)\n",
    "                print(f\"timesteps: {i}\")\n",
    "            i += 1\n",
    "    print(\"Training finished \\n\")\n",
    "    \n",
    "    # return ndarray\n",
    "    all_episodes_length = np.array(all_episodes_length)\n",
    "    all_penalties = np.array(all_penalties)\n",
    "    all_rewards = np.array(all_rewards)\n",
    "    \n",
    "    return all_episodes_length, all_penalties, all_rewards, s_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 PRQL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prql(env, train_episodes, past_policy):\n",
    "        \n",
    "    # Hyper parameters\n",
    "    alpha = 0.05  #(alpha) is the learning rate (0<α≤1) \n",
    "    gamma = 0.95  # (gamma) is the discount factor (0≤γ≤1) \n",
    "    epsilon = 0.1 # the rate to act randomly in the epsilon-greedy alg for expolration in Q-learning\n",
    "    fi = 1 # the intial probability to reuse past policy in policy-reuse algo, decays after each step in one episode\n",
    "    mu = 0.95 # the decaying rate of fi\n",
    "    \n",
    "    # To plot learning curve\n",
    "    all_episodes_length = []\n",
    "    all_penalties = []\n",
    "    all_rewards = []\n",
    "\n",
    "    timesteps =  train_episodes * MAX_EPISODE_LEN\n",
    "    q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    " \n",
    "    past_table = copy.deepcopy(past_policy)\n",
    "    env_copy = copy.deepcopy(env)\n",
    "    \n",
    "    i = 0\n",
    "    while i < timesteps:\n",
    "        env.seed(random.randint(0,timesteps))\n",
    "        state = env.reset()\n",
    "\n",
    "        episode_penalties, episode_rewards, episode_length = 0, 0, 0\n",
    "        done = False\n",
    "        f = fi\n",
    "        while not done and i < timesteps:\n",
    "            # epsilon greedy alg balancing exporation and exploitation\n",
    "            if random.uniform(0,1) < f:\n",
    "                action = arg_max(past_table[state])\n",
    "                #print(f\"f = {f} in episode {i} \\n\")\n",
    "            elif random.uniform(0,1) < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "                #print(\"random action \\n\")\n",
    "                \n",
    "            else:\n",
    "                action = arg_max(q_table[state])\n",
    "                #print(\"on the policy now \\n\")\n",
    "\n",
    "            # step to next state\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "\n",
    "            # update q-value\n",
    "            q_value = (1-alpha) * q_table[state, action] + alpha * (reward + gamma * np.max(q_table[next_state]))\n",
    "            q_table[state, action] = q_value\n",
    "\n",
    "            state = next_state\n",
    "            f = f*mu\n",
    "            \n",
    "            if i%200 == 0:\n",
    "                #evaluate policy for learning curve after each episode\n",
    "                episode_rewards, episode_penalties, episode_length = policy_evaluate(env_copy, q_table, EVALUATION_TIMES)\n",
    "                \n",
    "                all_episodes_length.append(episode_length)\n",
    "                all_penalties.append(episode_penalties)\n",
    "                all_rewards.append(episode_rewards)\n",
    "                \n",
    "                #show training progress\n",
    "                clear_output(wait=True)\n",
    "                print(f\"timesteps: {i}\")\n",
    "            i += 1\n",
    "    print(\"Training finished \\n\")\n",
    "    \n",
    "    # return ndarray\n",
    "    all_episodes_length = np.array(all_episodes_length)\n",
    "    all_penalties = np.array(all_penalties)\n",
    "    all_rewards = np.array(all_rewards)\n",
    "    \n",
    "    return all_episodes_length, all_penalties, all_rewards, q_table    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 OPS-TL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ops(env,train_episodes,past_policies):\n",
    "    \"\"\"\n",
    "    Training the agent using algorithm in paper \"An Optimal Online Method of Selecting \n",
    "    Source Policies for Reinforcement Learning\"\n",
    "    \"\"\"\n",
    "    # Hyper parameters\n",
    "    alpha = 0.05 #(alpha) is the learning rate (0<α≤1) \n",
    "    gamma = 0.95 # (gamma) is the discount factor (0≤γ≤1)\n",
    "    epsilon = 0.1 # the rate to act randomly in the epsilon-greedy alg for expolration in Q-learning\n",
    "    fi = 0.95 # the intial probability to reuse past policy in policy-reuse algo, decays after each step in one episode\n",
    "    c = 0.0049 # the factor in UCB-tuned deciding the rate of exploration, a lager c will lead to a higher exploration rate\n",
    "    \n",
    "    # To plot learning curve\n",
    "    all_episodes_length = []\n",
    "    all_penalties = []\n",
    "    all_rewards = []\n",
    "   \n",
    "    #win = {'reuse': [], 'new': []}\n",
    "    timesteps = train_episodes * MAX_EPISODE_LEN\n",
    "   \n",
    "    q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "    env_copy = copy.deepcopy(env)\n",
    "    \n",
    "    # pt = 1-k/(k+1500)\n",
    "    \n",
    "    #initialize the expected reward of every bandit/past_policy\n",
    "    policy_expect = np.zeros([len(past_policies)])\n",
    "    policy_times = np.zeros([len(past_policies)])\n",
    "    for j in range(len(past_policies)):\n",
    "        t = 0\n",
    "        env.reset()\n",
    "        _,policy_expect[j],_,_= policy_reuse(timesteps, env,q_table, env.reset(), t,[],[],[],past_policies[j])\n",
    "        policy_times[j] += 1\n",
    "        \n",
    "    # training strat  \n",
    "    i = 0\n",
    "    while i < timesteps:\n",
    "        env.seed(random.randint(0,timesteps))\n",
    "        state = env.reset()\n",
    "        \n",
    "        episode_penalties, episode_rewards, episode_length = 0, 0, 0\n",
    "        done = False\n",
    "        \n",
    "        k = i // MAX_EPISODE_LEN  + 1\n",
    "        pt = 1-k/(k+1500)\n",
    "        \n",
    "        # use UCB1 to solve MAB(multi armed bandit problem\n",
    "        if random.uniform(0,1) < pt:\n",
    "            j = arg_max(policy_expect + np.sqrt(c * np.log(np.sum(policy_times)) / policy_times )) # UCB1-tuned\n",
    "            episode_penalties, episode_rewards, episode_length,i = policy_reuse(timesteps, env, q_table, state, i,\n",
    "                                                                              all_episodes_length,all_penalties,all_rewards,\n",
    "                                                                              past_policy = past_policies[j])\n",
    "            policy_expect[j] = (policy_expect[j]*policy_times[j] + episode_rewards)/( policy_times[j] +1)\n",
    "            policy_times[j] += 1\n",
    "#            if episode_length < 200:\n",
    "#                win['reuse'].append(k)\n",
    "        # follow episilon-greedy strategy    \n",
    "        else:\n",
    "            episode_penalties, episode_rewards, episode_length,i = epsilon_greedy(timesteps, env, q_table, state, i , \n",
    "                                                                                all_episodes_length,all_penalties,all_rewards,\n",
    "                                                                                epsilon, alpha, gamma)\n",
    "#            if episode_length < 200:\n",
    "#                win['new'].append(k)        \n",
    "\n",
    "\n",
    "        #show training progress\n",
    "        if (k) % 100 == 0:\n",
    "            clear_output(wait=True)\n",
    "            print(f\"Episode {k}\")\n",
    "    print(f\"Training finished\")\n",
    "    \n",
    "    # return ndarray\n",
    "    all_episodes_length = np.array(all_episodes_length)\n",
    "    all_penalties = np.array(all_penalties)\n",
    "    all_rewards = np.array(all_rewards)\n",
    "    \n",
    "    return all_episodes_length, all_penalties, all_rewards, q_table    \n",
    "            \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(timesteps, env, q_table, initial_state, i,\n",
    "                   all_episodes_length,all_penalties,all_rewards,epsilon = 0.1, alpha = 0.05,  gamma = 0.95 ):\n",
    "    done = False\n",
    "    state = initial_state\n",
    "    episode_penalties, episode_rewards, episode_length = 0, 0, 0\n",
    "  \n",
    "    _env_copy = copy.deepcopy(env)\n",
    "    while not done and i < timesteps:\n",
    "        if random.uniform(0,1) < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = arg_max(q_table[state])\n",
    "            \n",
    "        # step to next state\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # update q-value\n",
    "        q_value = (1-alpha) * q_table[state, action] + alpha * (reward + gamma * np.max(q_table[next_state]))\n",
    "        q_table[state, action] = q_value\n",
    "\n",
    "        state = next_state\n",
    "      \n",
    "        #update data for learning curve\n",
    "        if reward == -10:\n",
    "            episode_penalties +=1\n",
    "\n",
    "        episode_rewards += reward\n",
    "        episode_length += 1\n",
    "        \n",
    "        if i%200 == 0:\n",
    "            #evaluate policy for learning curve after each episode\n",
    "            _episode_rewards, _episode_penalties, _episode_length = policy_evaluate(_env_copy, q_table, EVALUATION_TIMES)\n",
    "\n",
    "            all_episodes_length.append(_episode_length)\n",
    "            all_penalties.append(_episode_penalties)\n",
    "            all_rewards.append(_episode_rewards)\n",
    "\n",
    "            #show training progress\n",
    "            clear_output(wait=True)\n",
    "            print(f\"timesteps: {i}\")\n",
    "        i += 1\n",
    "\n",
    "    return episode_penalties, episode_rewards, episode_length,i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_reuse(timesteps,env, q_table, initial_state, i, \n",
    "                 all_episodes_length,all_penalties,all_rewards, past_policy,  fi = 0.95, mu=0.95, alpha = 0.05,  gamma = 0.95 ):\n",
    "    done = False\n",
    "    state = initial_state\n",
    "    episode_penalties, episode_rewards, episode_length = 0, 0, 0\n",
    "    \n",
    "    _env_copy = copy.deepcopy(env)\n",
    "    while not done and i < timesteps:\n",
    "        if random.uniform(0,1) < fi:\n",
    "            action = arg_max(past_policy[state])\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "            \n",
    "        # step to next state\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # update q-value\n",
    "        q_value = (1-alpha) * q_table[state, action] + alpha * (reward + gamma * np.max(q_table[next_state]))\n",
    "        q_table[state, action] = q_value\n",
    "\n",
    "        state = next_state\n",
    "        fi = fi*mu\n",
    "        #update data for learning curve\n",
    "        if reward == -10:\n",
    "            episode_penalties +=1\n",
    "\n",
    "        episode_rewards += reward\n",
    "        episode_length += 1\n",
    "        \n",
    "        if i%200 == 0:\n",
    "            #evaluate policy for learning curve after each episode\n",
    "            _episode_rewards, _episode_penalties, _episode_length = policy_evaluate(_env_copy, q_table, EVALUATION_TIMES)\n",
    "\n",
    "            all_episodes_length.append(_episode_length)\n",
    "            all_penalties.append(_episode_penalties)\n",
    "            all_rewards.append(_episode_rewards)\n",
    "\n",
    "            #show training progress\n",
    "            clear_output(wait=True)\n",
    "            print(f\"timesteps: {i}\")\n",
    "        i += 1\n",
    "        \n",
    "    return episode_penalties, episode_rewards, episode_length,i\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 caps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caps(env,train_episodes,past_policies):\n",
    "    \"\"\"\n",
    "    Training the agent using algorithm in paper \"Context-aware policy reuse\"\n",
    "    \"\"\"\n",
    "    # Hyper parameters\n",
    "    alpha = 0.05 #(alpha) is the learning rate (0<α≤1) \n",
    "    gamma = 0.95 # (gamma) is the discount factor (0≤γ≤1)\n",
    "    epsilon = 0.1 # the rate to act randomly in the epsilon-greedy alg for expolration in Q-learning\n",
    "    fi = 0.95 # the intial probability to reuse past policy in policy-reuse algo, decays after each step in one episode\n",
    "    c = 0.0049 # the factor in UCB-tuned deciding the rate of exploration, a lager c will lead to a higher exploration rate\n",
    "    \n",
    "    # To plot learning curve\n",
    "    all_episodes_length = []\n",
    "    all_penalties = []\n",
    "    all_rewards = []\n",
    "    all_frequency = np.zeros([len(past_policies)+1, train_episodes])\n",
    "#   win = {'reuse': [], 'new': []}\n",
    "    timesteps = train_episodes * MAX_EPISODE_LEN\n",
    "   \n",
    "    q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "    env_copy = copy.deepcopy(env)\n",
    "    \n",
    "    # pt = 1-k/(k+1500)\n",
    "    \n",
    "    #initialize the expected reward of every bandit/past_policy\n",
    "    frequency = np.zeros([len(past_policies)+1, train_episodes])\n",
    "    policy_expect = np.zeros([len(past_policies)])\n",
    "    policy_times = np.zeros([len(past_policies)])\n",
    "    for j in range(len(past_policies)):\n",
    "        t = 0\n",
    "        env.reset()\n",
    "        for _ in range(5):\n",
    "            _,temp,_,_= policy_reuse(timesteps, env,q_table, env.reset(), t,[],[],[],past_policies[j])\n",
    "            policy_expect[j] += temp\n",
    "        policy_expect[j] /= 10\n",
    "        policy_times[j] += 1\n",
    "        \n",
    "    # training strat  \n",
    "    i = 0\n",
    "    while i < timesteps:\n",
    "        env.seed(random.randint(0,timesteps))\n",
    "        state = env.reset()\n",
    "        \n",
    "        episode_penalties, episode_rewards, episode_length = 0, 0, 0\n",
    "        done = False\n",
    "        \n",
    "        k = i // MAX_EPISODE_LEN  + 1\n",
    "        pt = 1-k/(k+1500)\n",
    "        \n",
    "        # use UCB1 to solve MAB(multi armed bandit problem\n",
    "        if random.uniform(0,1) < pt:\n",
    "            j = arg_max(policy_expect/20 + np.sqrt(c * np.log(np.sum(policy_times)) / policy_times )) # UCB1-tuned\n",
    "            episode_penalties, episode_rewards, episode_length,i = policy_reuse(timesteps, env, q_table, state, i,\n",
    "                                                                              all_episodes_length,all_penalties,all_rewards,\n",
    "                                                                              past_policy = past_policies[j])\n",
    "            policy_expect[j] = (policy_expect[j]*policy_times[j] + episode_rewards)/( policy_times[j] +1)\n",
    "            policy_times[j] += 1\n",
    "            frequency[j][k-1] += 1\n",
    "#            if episode_length < 200:\n",
    "#                win['reuse'].append(k)\n",
    "        # follow episilon-greedy strategy    \n",
    "        else:\n",
    "            episode_penalties, episode_rewards, episode_length,i = epsilon_greedy(timesteps, env, q_table, state, i , \n",
    "                                                                                all_episodes_length,all_penalties,all_rewards,\n",
    "                                                                                epsilon, alpha, gamma)\n",
    "#            if episode_length < 200:\n",
    "#                win['new'].append(k)        \n",
    "            frequency[len(past_policies)][k-1] += 1\n",
    "\n",
    "\n",
    "    print(f\"Training finished\")\n",
    "    \n",
    "    # return ndarray\n",
    "    all_episodes_length = np.array(all_episodes_length)\n",
    "    all_penalties = np.array(all_penalties)\n",
    "    all_rewards = np.array(all_rewards)\n",
    "    for line in range(train_episodes):\n",
    "        for row in range(len(past_policies)+1):\n",
    "            all_frequency[row][line]=np.sum(frequency[row][:line+1], keepdims= True) /np.sum(np.sum(frequency[:,:line+1]))\n",
    "    return all_episodes_length, all_penalties, all_rewards, all_frequency, q_table    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 OC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oc(env ,episode_num ,  option_policies_lib = None, \n",
    "                  option_terminations_lib = None, policy_over_options = None, critic = None, noptions = 4,seed = 1 ):\n",
    "    \n",
    "    # Discount\n",
    "    discount = 0.99\n",
    "    \n",
    "\n",
    "    # Learning rates - termination, intra-option, critic\n",
    "    lr_term = 0.25\n",
    "    lr_intra = 0.25\n",
    "    lr_critic = 0.5\n",
    "\n",
    "    # Epsilon for epsilon-greedy for policy over options\n",
    "    epsilon = 1e-1\n",
    "\n",
    "    # Temperature for softmax\n",
    "    temperature = 0.01\n",
    "    \n",
    "    # To plot learning curve\n",
    "    all_episodes_length = []\n",
    "    all_penalties = []\n",
    "    all_rewards = []\n",
    "    \n",
    "\n",
    "    rng = np.random.RandomState(seed)\n",
    "    #trainning timesteps\n",
    "    timesteps =  episode_num * MAX_EPISODE_LEN\n",
    "\n",
    "    nstates = env.observation_space.n\n",
    "    nactions = env.action_space.n\n",
    "    \n",
    "    # for option_evaluate\n",
    "    env.seed(random.randint(0,timesteps))\n",
    "    state = env.reset()\n",
    "    env_copy = copy.deepcopy(env) \n",
    "\n",
    "    # Following three belong to the Actor\n",
    "\n",
    "    # 1. The intra-option policies - linear softmax functions\n",
    "    if option_policies_lib is None:\n",
    "        option_policies = [SoftmaxPolicy(rng, lr_intra, nstates, nactions, epsilon, temperature) for _ in range(noptions)]\n",
    "      \n",
    "       \n",
    "    else:\n",
    "        option_policies = [None] * noptions\n",
    "        for i in range(noptions):\n",
    "            option_policies[i] = copy.deepcopy(option_policies_lib[i])\n",
    "\n",
    "\n",
    "    # 2. The termination function - linear sigmoid function\n",
    "    if option_terminations_lib is None:\n",
    "        option_terminations = [SigmoidTermination(rng, lr_term, nstates) for _ in range(noptions)]\n",
    "        \n",
    "    else:\n",
    "        option_terminations =  [None] * noptions\n",
    "        for i in range(noptions):\n",
    "            option_terminations[i] = copy.deepcopy(option_terminations_lib[i])\n",
    "\n",
    "\n",
    "\n",
    "    # 3. The epsilon-greedy policy over options\n",
    "    if policy_over_options is None:\n",
    "        policy_over_options = EpsGreedyPolicy(rng, nstates, noptions, epsilon)\n",
    "\n",
    "        \n",
    "    else:\n",
    "        policy_over_options = copy.deepcopy(policy_over_options)\n",
    "\n",
    "\n",
    "    # Critic\n",
    "    if critic is None:\n",
    "        critic = Critic(lr_critic, discount, policy_over_options.Q_Omega_table, nstates, noptions, nactions)\n",
    "    else:\n",
    "        critic = copy.deepcopy(critic)\n",
    "\n",
    "   \n",
    "    \n",
    "    i=0\n",
    "    while i < timesteps:\n",
    "\n",
    "        # Change goal location after 1000 episodes \n",
    "        # Comment it for not doing transfer experiments\n",
    "#        if episode == 1000:\n",
    "#            env.goal = rng.choice(possible_next_goals)\n",
    "#            print('New goal: ', env.goal)\n",
    "\n",
    "        state = env.reset()\n",
    "\n",
    "        option = policy_over_options.sample(state)\n",
    "        action = option_policies[option].sample(state)\n",
    "\n",
    "        critic.cache(state, option, action)\n",
    "        \n",
    "        episode_penalties, episode_rewards, episode_length = 0, 0, 0\n",
    "        done = False\n",
    "        \n",
    "        \n",
    "        while not done and i < timesteps:\n",
    "            state, reward, done, _ = env.step(action)\n",
    "\n",
    "            # Termination might occur upon entering new state\n",
    "            if option_terminations[option].sample(state):\n",
    "                option = policy_over_options.sample(state)\n",
    "\n",
    "\n",
    "            action = option_policies[option].sample(state)\n",
    "\n",
    "            # Critic update\n",
    "            critic.update_Qs(state, option, action, reward, done, option_terminations)\n",
    "\n",
    "            # Intra-option policy update with baseline\n",
    "            Q_U = critic.Q_U(state, option, action)\n",
    "            Q_U = Q_U - critic.Q_Omega(state, option)\n",
    "            option_policies[option].update(state, action, Q_U)\n",
    "\n",
    "            # Termination condition update\n",
    "            option_terminations[option].update(state, critic.A_Omega(state, option))\n",
    "            \n",
    "          \n",
    "            if i%200 == 0:\n",
    "                #evaluate policy for learning curve after each episode\n",
    "                episode_rewards, episode_penalties, episode_length = option_evaluate(env_copy, option_policies, option_terminations, policy_over_options, EVALUATION_TIMES)\n",
    "                \n",
    "                all_episodes_length.append(episode_length)\n",
    "                all_penalties.append(episode_penalties)\n",
    "                all_rewards.append(episode_rewards)\n",
    "\n",
    "                #show training progress\n",
    "                clear_output(wait=True)\n",
    "                print(f\"timesteps: {i}\")\n",
    "            i += 1\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"Training finished\")\n",
    "    \n",
    "    # return ndarray\n",
    "    all_episodes_length = np.array(all_episodes_length)\n",
    "    all_penalties = np.array(all_penalties)\n",
    "    all_rewards = np.array(all_rewards)\n",
    "    \n",
    "    return all_episodes_length, all_penalties, all_rewards, option_policies, option_terminations, policy_over_options,critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def option_evaluate(env,  option_policies, option_terminations, policy_over_options , times):\n",
    "    # store data, make average and return\n",
    "    all_length, all_penalties, all_rewards = np.zeros(times), np.zeros(times), np.zeros(times)\n",
    "\n",
    "    for i in range(times):\n",
    "\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        episode_penalties, episode_rewards, episode_length = 0, 0, 0\n",
    "        \n",
    "        option = policy_over_options.evaluate(state)\n",
    "        action = option_policies[option].evaluate(state)\n",
    "       \n",
    "        for j in range(MAX_EPISODE_LEN):\n",
    "            # Termination might occur upon entering new state\n",
    "            if option_terminations[option].sample(state):\n",
    "                option = policy_over_options.evaluate(state)\n",
    "\n",
    "\n",
    "            action = option_policies[option].evaluate(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            state = next_state\n",
    "            #update data for learning curve\n",
    "            if reward == -10:\n",
    "                episode_penalties +=1\n",
    "\n",
    "            episode_rewards += reward\n",
    "            episode_length += 1\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        all_rewards[i] = episode_rewards\n",
    "        all_penalties[i] = episode_penalties\n",
    "        all_length[i] = episode_length\n",
    "    return np.mean(all_rewards), np.mean(all_penalties), np.mean(all_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  tranform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform( tl_algo, episode_num, repeat_times, target_task, \n",
    "              source_task= None, source_rl_algo = None, policy_library = None):\n",
    "    \n",
    "    \"\"\"\n",
    "    return all_episodes_length, all_penalties, all_rewards, all_frequency, all_trans_knowledge\n",
    "    \n",
    "    Situation 1:\n",
    "    no source task available, train agent on target task from scratch \n",
    "    \n",
    "    Situation 2:\n",
    "    source task is available but source policy unavailable, train agent on source task to get source policy, \n",
    "    then train agent on target task using knowledge from source policy\n",
    "    \n",
    "    Situation 3:\n",
    "    source policies are availale, reuse source policies to train agent on target task\n",
    "    \"\"\"\n",
    "    train_episodes = episode_num    \n",
    "    # data collected during trainning\n",
    "    all_episodes_length = np.zeros([repeat_times,train_episodes])\n",
    "    all_penalties = np.zeros([repeat_times,train_episodes])\n",
    "    all_rewards = np.zeros([repeat_times,train_episodes])\n",
    "    if tl_algo is caps and policy_library is not None:\n",
    "        all_frequency = np.zeros([len(policy_library[0])+1, train_episodes])\n",
    "    else:\n",
    "        all_frequency = None\n",
    "    \n",
    "    all_trans_knowledge = []\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in range(repeat_times):\n",
    "       \n",
    "        np.random.seed(i)\n",
    "        random.seed(i)\n",
    "        #Situation 3    \n",
    "        if policy_library is not None: \n",
    "            \n",
    "            if type(target_task) is str:\n",
    "                env = gym.make(target_task)\n",
    "            else:\n",
    "                env = target_task\n",
    "            past_policies = policy_library # policy_library = [ [[policy11][policy21]...] [[policy12][policy22]...]...]\n",
    "            if tl_algo is caps:\n",
    "                episodes,penalties,rewards,frequency, *knowledge = tl_algo(env, train_episodes, past_policies[i]) \n",
    "            else:\n",
    "                episodes,penalties,rewards, *knowledge = tl_algo(env, train_episodes,  past_policies[i]) \n",
    "        #Situation 2   \n",
    "        elif source_task is not None:\n",
    "            if source_rl_algo is None:\n",
    "                source_rl_algo = tl_algo\n",
    "            if type(source_task) is str:\n",
    "                env = gym.make(source_task)\n",
    "            else:\n",
    "                env = source_task\n",
    "            _, _,_ ,*knowledge = source_rl_algo(env,  train_episodes)\n",
    "            \n",
    "            if type(target_task) is str:\n",
    "                env = gym.make(target_task)\n",
    "            else:\n",
    "                env = target_task\n",
    "            episodes,penalties,rewards,*knowledge = tl_algo(env, train_episodes , *knowledge)\n",
    "         #Situation 1\n",
    "        else: \n",
    "            if type(target_task) is str:\n",
    "                env = gym.make(target_task)\n",
    "            else:\n",
    "                env = target_task\n",
    "                \n",
    "           \n",
    "            episodes,penalties,rewards,*knowledge = tl_algo(env, train_episodes)\n",
    "            \n",
    "\n",
    "        all_episodes_length[i] = episodes\n",
    "        all_penalties[i] = penalties\n",
    "        all_rewards[i] = rewards\n",
    "        if tl_algo is caps and policy_library is not None:\n",
    "            all_frequency += frequency\n",
    "        if len(knowledge) > 1:\n",
    "            all_trans_knowledge.append(knowledge)\n",
    "        else:\n",
    "            all_trans_knowledge.append(*knowledge)\n",
    "\n",
    "    \n",
    "    if tl_algo is caps and policy_library is not None:\n",
    "        all_frequency /= repeat_times\n",
    "    \n",
    "    return all_episodes_length, all_penalties, all_rewards, all_frequency, all_trans_knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0.])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a= np.zeros([2,3])\n",
    "a[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CAPS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-abc5b5252d29>\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(target_rl_algo, episode_num, repeat_times, target_task, source_task, source_rl_algo, policy_library)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mall_penalties\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrepeat_times\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_episodes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mall_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrepeat_times\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_episodes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mtarget_rl_algo\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mCAPS\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mpolicy_library\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mall_frequency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_library\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_episodes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CAPS' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "qlearn_episodes_length_fourroom, qlearn_penalties_fourroom, qlearn_rewards_fourroom, qlearn_q_table_fourroom = [[None] * len(four_room_envs) for _ in range(4)]\n",
    "for i in range(len(four_room_envs)):\n",
    "    qlearn_episodes_length_fourroom[i], qlearn_penalties_fourroom[i], qlearn_rewards_fourroom[i], _, qlearn_q_table_fourroom[i] = transform(tl_algo = q_learning,\n",
    "                                                                                     episode_num = EPISODE_NUM, \n",
    "                                                                                   repeat_times = REPEAT_TIMES,\n",
    "                                                                                     target_task = four_room_envs[i],\n",
    "                                                                                     source_task = None\n",
    "                                                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CAPS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-abc5b5252d29>\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(target_rl_algo, episode_num, repeat_times, target_task, source_task, source_rl_algo, policy_library)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mall_penalties\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrepeat_times\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_episodes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mall_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrepeat_times\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_episodes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mtarget_rl_algo\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mCAPS\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mpolicy_library\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mall_frequency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_library\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_episodes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CAPS' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# training Taxi-v003 from scratch\n",
    "qlearn_episodes_length_v003, qlearn_penalties_v003, qlearn_rewards_v003, _, qlearn_q_table_v003 = transform(tl_algo = q_learning,\n",
    "                                                                                     episode_num = EPISODE_NUM, \n",
    "                                                                                     repeat_times = REPEAT_TIMES,\n",
    "                                                                                     target_task = \"Taxi-v003\",\n",
    "                                                                                     source_task = None\n",
    "                                                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 799800\n",
      "Training finished\n",
      "CPU times: user 12min 15s, sys: 1min 8s, total: 13min 24s\n",
      "Wall time: 14min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# training Taxi-v000 from scratch\n",
    "qlearn_episodes_length_v000, qlearn_penalties_v000, qlearn_rewards_v000, _, qlearn_q_table_v000 = transform(tl_algo = q_learning,\n",
    "                                                                                     episode_num = EPISODE_NUM, \n",
    "                                                                                     repeat_times = REPEAT_TIMES,\n",
    "                                                                                     target_task = \"Taxi-v000\",\n",
    "                                                                                     source_task = None\n",
    "                                                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 799800\n",
      "Training finished\n",
      "CPU times: user 12min 4s, sys: 1min 14s, total: 13min 19s\n",
      "Wall time: 14min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# training Taxi-v001 from scratch\n",
    "qlearn_episodes_length_v001, qlearn_penalties_v001, qlearn_rewards_v001,_, qlearn_q_table_v001 = transform(tl_algo = q_learning,\n",
    "                                                                                     episode_num = EPISODE_NUM, \n",
    "                                                                                     repeat_times = REPEAT_TIMES,\n",
    "                                                                                     target_task = \"Taxi-v001\",\n",
    "                                                                                     source_task = None\n",
    "                                                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 799800\n",
      "Training finished\n",
      "CPU times: user 12min 55s, sys: 1min 27s, total: 14min 22s\n",
      "Wall time: 15min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# training Taxi-v004 from scratch\n",
    "qlearn_episodes_length_v004, qlearn_penalties_v004, qlearn_rewards_v004,_, qlearn_q_table_v004 = transform(tl_algo = q_learning,\n",
    "                                                                                     episode_num = EPISODE_NUM, \n",
    "                                                                                     repeat_times = REPEAT_TIMES,\n",
    "                                                                                     target_task = \"Taxi-v004\",\n",
    "                                                                                     source_task = None\n",
    "                                                                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 799800\n",
      "Training finished \n",
      "\n",
      "CPU times: user 10min 37s, sys: 1min 44s, total: 12min 21s\n",
      "Wall time: 12min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# training Taxi-v003 from scratch\n",
    "sarsa_episodes_length_v003, sarsa_penalties_v003, sarsa_rewards_v003, _, sarsa_q_table_v003 = transform(tl_algo = sarsa,\n",
    "                                                                                     episode_num = EPISODE_NUM, \n",
    "                                                                                     repeat_times = REPEAT_TIMES,\n",
    "                                                                                     target_task = \"Taxi-v003\",\n",
    "                                                                                     source_task = None\n",
    "                                                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 799800\n",
      "Training finished \n",
      "\n",
      "CPU times: user 7min 21s, sys: 2min 3s, total: 9min 24s\n",
      "Wall time: 8min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# training Taxi-v001 from scratch\n",
    "sarsa_episodes_length_v001, sarsa_penalties_v001, sarsa_rewards_v001,_, sarsa_q_table_v001 = transform(tl_algo = sarsa,\n",
    "                                                                                     episode_num = EPISODE_NUM, \n",
    "                                                                                     repeat_times = REPEAT_TIMES,\n",
    "                                                                                     target_task = \"Taxi-v001\",\n",
    "                                                                                     source_task = None\n",
    "                                                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 799800\n",
      "Training finished \n",
      "\n",
      "CPU times: user 7min 58s, sys: 2min 29s, total: 10min 27s\n",
      "Wall time: 8min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# training Taxi-v004 from scratch\n",
    "sarsa_episodes_length_v004, sarsa_penalties_v004, sarsa_rewards_v004,_, sarsa_q_table_v004 = transform(tl_algo = sarsa,\n",
    "                                                                                     episode_num = EPISODE_NUM, \n",
    "                                                                                     repeat_times = REPEAT_TIMES,\n",
    "                                                                                     target_task = \"Taxi-v004\",\n",
    "                                                                                     source_task = None\n",
    "                                                                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 PRQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'qlearn_q_table_fourroom' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'qlearn_q_table_fourroom' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pr_episodes_length_fourroom, pr_penalties_fourroom, pr_rewards_fourroom, pr_q_table_fourroom = [[None] * (len(four_room_envs)-1) for _ in range(4)]\n",
    "\n",
    "#pr_fourroom_v012_3\n",
    "for i in range(len(four_room_envs)-1):\n",
    "    pr_episodes_length_fourroom[i], pr_penalties_fourroom[i], pr_rewards_fourroom[i], _, pr_q_table_fourroom[i] = transform(tl_algo = prql,\n",
    "                                                                                     episode_num = EPISODE_NUM, \n",
    "                                                                                   repeat_times = REPEAT_TIMES,\n",
    "                                                                                     target_task = four_room_envs[3],\n",
    "                                                                                     policy_library = qlearn_q_table_fourroom[i]\n",
    "                                                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "randint() missing 1 required positional argument: 'b'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-abc5b5252d29>\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(target_rl_algo, episode_num, repeat_times, target_task, source_task, source_rl_algo, policy_library)\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0mepisodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpenalties\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfrequency\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mknowledge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_rl_algo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_episodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast_policies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m                 \u001b[0mepisodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpenalties\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mknowledge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_rl_algo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_episodes\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mpast_policies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;31m#Situation 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0msource_task\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-f3c5f7f96693>\u001b[0m in \u001b[0;36mprql\u001b[0;34m(env, train_episodes, past_policy)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtimesteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimesteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: randint() missing 1 required positional argument: 'b'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#pr_v014_003\n",
    "pr_episodes_length_v014_003, pr_penalties_v014_003,pr_rewards_v014_003, _, pr_q_table_v014_003 = transform(tl_algo= prql,\n",
    "                                                                                       episode_num = EPISODE_NUM,\n",
    "                                                                                       repeat_times = REPEAT_TIMES,\n",
    "                                                                                       target_task = \"Taxi-v003\",\n",
    "                                                                                       \n",
    "                                                                                       policy_library =  qlearn_q_table_v004)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3 OPS-TL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'qlearn_q_table_fourroom' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'qlearn_q_table_fourroom' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "ops_episodes_length_fourroom, ops_penalties_fourroom, ops_rewards_fourroom, ops_q_table_fourroom = [[None] * (len(four_room_envs)-1) for _ in range(4)]\n",
    "\n",
    "#ops_fourroom_v012_3\n",
    "\n",
    "ops_episodes_length_fourroom, ops_penalties_fourroom, ops_rewards_fourroom, _, ops_q_table_fourroom = transform(tl_algo = OPS_TL,\n",
    "                                                                                     episode_num = EPISODE_NUM, \n",
    "                                                                                   repeat_times = REPEAT_TIMES,\n",
    "                                                                                     target_task = four_room_envs[3],\n",
    "                                                                                     policy_library =  [[q_func1,q_func2,q_func3 ] for q_func1,q_func2,q_func3 in zip(qlearn_q_table_fourroom[0],qlearn_q_table_fourroom[1],qlearn_q_table_fourroom[2])]\n",
    "                                                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'qlearn_q_table_v000' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'qlearn_q_table_v000' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#ops_v014_003\n",
    "ops_episodes_length_v014_003, ops_penalties_v014_003,ops_rewards_v014_003,ops_fre_v014_003, ops_q_table_v014_003 = transform(tl_algo= OPS_TL,\n",
    "                                                                                       episode_num = EPISODE_NUM,\n",
    "                                                                                       repeat_times = REPEAT_TIMES,\n",
    "                                                                                       target_task = \"Taxi-v003\",\n",
    "                                                                                      \n",
    "                                                                                       policy_library = [[q_func1,q_func2,q_func3 ] for q_func1,q_func2,q_func3 in zip(qlearn_q_table_v000, qlearn_q_table_v001, qlearn_q_table_v004)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 caps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'qlearn_q_table_fourroom' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'qlearn_q_table_fourroom' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "#caps_fourroom_v012_3\n",
    "\n",
    "caps_episodes_length_fourroom, caps_penalties_fourroom, caps_rewards_fourroom, _, caps_q_table_fourroom = transform(tl_algo = caps,\n",
    "                                                                                     episode_num = EPISODE_NUM, \n",
    "                                                                                   repeat_times = REPEAT_TIMES,\n",
    "                                                                                     target_task = four_room_envs[3],\n",
    "                                                                                     policy_library =  [[q_func1,q_func2,q_func3 ] for q_func1,q_func2,q_func3 in zip(qlearn_q_table_fourroom[0],qlearn_q_table_fourroom[1],qlearn_q_table_fourroom[2])]\n",
    "                                                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'qlearn_q_table_v000' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'qlearn_q_table_v000' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#caps_v014_003\n",
    "caps_episodes_length_v014_003, caps_penalties_v014_003,caps_rewards_v014_003,caps_fre_v014_003, caps_q_table_v014_003 = transform(tl_algo= caps,\n",
    "                                                                                       episode_num = EPISODE_NUM,\n",
    "                                                                                       repeat_times = REPEAT_TIMES,\n",
    "                                                                                       target_task = \"Taxi-v003\",\n",
    "                                                                                      \n",
    "                                                                                       policy_library = [[q_func1,q_func2,q_func3 ] for q_func1,q_func2,q_func3 in zip(qlearn_q_table_v000, qlearn_q_table_v001, qlearn_q_table_v004)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 OC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Line magic function `%autoreload` not found.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%autoreload 2\n",
    "option_policies, option_terminations, policy_over_options, critic, nrewards = [ [None]*4 for i in range(5) ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 799800\n",
      "Training finished\n",
      "CPU times: user 13min 44s, sys: 2min 30s, total: 16min 15s\n",
      "Wall time: 14min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for i in [0,1,2,3]:\n",
    "    _, _ ,nrewards[i] ,option_policies[i], option_terminations[i], policy_over_options[i],critic[i]= oc( four_room_envs[i], 4000, noptions=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-253-07e25a4c40f4>\u001b[0m in \u001b[0;36moption_critic\u001b[0;34m(env, episode_num, option_policies_lib, option_terminations_lib, policy_over_options, critic, noptions)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0moption_policies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnoptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0moption_policies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moption_policies_lib\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trans_rewards,trans_terminations, trans_policies = [ [None]*4 for _ in range(3)]\n",
    " \n",
    "for i in [0,1,2]:\n",
    "    _,_, trans_rewards[i], trans_policies[i], trans_terminations[i], _,_= oc( four_room_envs[3], 4000, option_policies[i], option_terminations[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 799800\n",
      "Training finished\n"
     ]
    }
   ],
   "source": [
    "source_lib = [1,2]\n",
    "_,_, trans_rewards_012, _, _, _,_= oc( four_room_envs[3], 4000, [option_policies[i][0] for i in source_lib], noptions=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc810a14b62f456bb382c2c8a2b95a7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "import seaborn as sns\n",
    "RATIO = 1\n",
    "SMOOTH_RADIUS = 50\n",
    "sns.set(style = \"darkgrid\")\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(qlearn_rewards_fourroom[2], SMOOTH_RADIUS), label = \"NoneTransfer\")\n",
    "#plt.plot(range(EPISODE_NUM), smooth(qlearn_rewards_v5, 100), label = \"qlearn_v5\")\n",
    "\n",
    "#plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v000_003,SMOOTH_RADIUS), label = \"caps_v0_003\",color='b')\n",
    "#plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v001_003,SMOOTH_RADIUS), label = \"caps_v1_003\",color='m')\n",
    "#plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v004_003,SMOOTH_RADIUS), label = \"caps_v4_003\",color='y')\n",
    "i = 0\n",
    "#plt.plot(range(EPISODE_NUM//RATIO), smooth(pr_rewards_fourroom[i],SMOOTH_RADIUS), label = \"pr_v014_003\",color='c')\n",
    "#plt.plot(range(EPISODE_NUM//RATIO), smooth(tl_rewards_fourroom,SMOOTH_RADIUS), label = \"ops_v014_003\",color='g')\n",
    "#plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_fourroom,SMOOTH_RADIUS), label = \"caps_v014_003\",color='r')\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(trans_rewards_012,SMOOTH_RADIUS), label = \"oc_v01_3\")\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(nrewards[3], SMOOTH_RADIUS), label = \"oc_v3\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.title(\"0-4000 episode-reward graph\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"rewards\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc98862b686745548d8e84d04a8f09b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'pr_rewards_fourroom' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-242-e296c1cbaef0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v004_003,SMOOTH_RADIUS), label = \"ops_norm_v4_003\",color='y')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPISODE_NUM\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mRATIO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmooth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpr_rewards_fourroom\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mEPISODE_NUM\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mRATIO\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mSMOOTH_RADIUS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"pr_v014_003\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPISODE_NUM\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mRATIO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmooth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtl_rewards_fourroom\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mEPISODE_NUM\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mRATIO\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mSMOOTH_RADIUS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ops_v014_003\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'g'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPISODE_NUM\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mRATIO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmooth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops_rewards_fourroom\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mEPISODE_NUM\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mRATIO\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mSMOOTH_RADIUS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"caps_v014_003\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pr_rewards_fourroom' is not defined"
     ]
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "RATIO = 4\n",
    "SMOOTH_RADIUS = 100\n",
    "sns.set(style = \"darkgrid\")\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(qlearn_rewards_fourroom[3][:EPISODE_NUM//RATIO], SMOOTH_RADIUS), label = \"qlearn_v003\", color='k')\n",
    "#plt.plot(range(EPISODE_NUM), smooth(qlearn_rewards_v5, 100), label = \"qlearn_v5\")\n",
    "\n",
    "#plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v000_003,SMOOTH_RADIUS), label = \"caps_v0_003\",color='b')\n",
    "#plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v001_003,SMOOTH_RADIUS), label = \"caps_v1_003\",color='m')\n",
    "#plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v004_003,SMOOTH_RADIUS), label = \"caps_v4_003\",color='y')\n",
    "\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(pr_rewards_fourroom[0][:EPISODE_NUM//RATIO],SMOOTH_RADIUS), label = \"pr_v014_003\",color='c')\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(tl_rewards_fourroom[:EPISODE_NUM//RATIO],SMOOTH_RADIUS), label = \"ops_v014_003\",color='g')\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_fourroom[:EPISODE_NUM//RATIO],SMOOTH_RADIUS), label = \"caps_v014_003\",color='r')\n",
    "#plt.plot(range(EPISODE_NUM//RATIO), smooth(trans_rewards[3][:EPISODE_NUM//RATIO],SMOOTH_RADIUS), label = \"oc_v014_003\",color='m')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.title(\"0-1000 episode-reward graph\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"rewards\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pr_rewards_v014_003' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-372-8aae36421dd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v004_003,SMOOTH_RADIUS), label = \"ops_norm_v4_003\",color='y')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPISODE_NUM\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mRATIO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmooth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpr_rewards_v014_003\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mSMOOTH_RADIUS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"pr_v014_003\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPISODE_NUM\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mRATIO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmooth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtl_rewards_v014_003\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mSMOOTH_RADIUS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ops_v014_003\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'g'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPISODE_NUM\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mRATIO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmooth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops_rewards_v014_003\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mSMOOTH_RADIUS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"caps_v014_003\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pr_rewards_v014_003' is not defined"
     ]
    }
   ],
   "source": [
    "#%matplotlib widget\n",
    "RATIO = 1\n",
    "SMOOTH_RADIUS = 50\n",
    "sns.set(style = \"darkgrid\")\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(qlearn_rewards_v003, SMOOTH_RADIUS), label = \"qlearn_v003\", color='k')\n",
    "#plt.plot(range(EPISODE_NUM), smooth(qlearn_rewards_v5, 100), label = \"qlearn_v5\")\n",
    "\n",
    "#plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v000_003,SMOOTH_RADIUS), label = \"caps_v0_003\",color='b')\n",
    "#plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v001_003,SMOOTH_RADIUS), label = \"caps_v1_003\",color='m')\n",
    "#plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v004_003,SMOOTH_RADIUS), label = \"caps_v4_003\",color='y')\n",
    "\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(pr_rewards_v014_003,SMOOTH_RADIUS), label = \"pr_v014_003\",color='c')\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(tl_rewards_v014_003,SMOOTH_RADIUS), label = \"ops_v014_003\",color='g')\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v014_003,SMOOTH_RADIUS), label = \"caps_v014_003\",color='r')\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(trans_rewards[3],SMOOTH_RADIUS), label = \"oc_v014_003\",color='m')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.title(\"0-4000 episode-reward graph\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"rewards\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RATIO = 4\n",
    "SMOOTH_RADIUS = 50\n",
    "sns.set(style = \"darkgrid\")\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(qlearn_rewards_v003[:EPISODE_NUM//RATIO], SMOOTH_RADIUS), label = \"qlearn_v003\", color='k')\n",
    "#plt.plot(range(EPISODE_NUM), smooth(qlearn_rewards_v5, 100), label = \"qlearn_v5\")\n",
    "\n",
    "#plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v000_003,SMOOTH_RADIUS), label = \"caps_v0_003\",color='b')\n",
    "#plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v001_003,SMOOTH_RADIUS), label = \"caps_v1_003\",color='m')\n",
    "#plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v004_003,SMOOTH_RADIUS), label = \"caps_v4_003\",color='y')\n",
    "\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(pr_rewards_v014_003[:EPISODE_NUM//RATIO],SMOOTH_RADIUS), label = \"pr_v014_003\",color='c')\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(tl_rewards_v014_003[:EPISODE_NUM//RATIO],SMOOTH_RADIUS), label = \"ops_v014_003\",color='g')\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v014_003[:EPISODE_NUM//RATIO],SMOOTH_RADIUS), label = \"caps_v014_003\",color='r')\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(trans_rewards[3][:EPISODE_NUM//RATIO],SMOOTH_RADIUS), label = \"oc_v014_003\",color='m')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.title(\"0-1000 episode-reward graph\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"rewards\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "RATIO = 1\n",
    "SMOOTH_RADIUS = 100\n",
    "sns.set(style = \"darkgrid\")\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_fre_v01_003[0],SMOOTH_RADIUS), label = 'ploicy_v001')\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_fre_v01_003[1],SMOOTH_RADIUS), label = 'ploicy_v004')\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_fre_v01_003[2],SMOOTH_RADIUS), label = 'target policy')\n",
    "plt.legend()\n",
    "plt.xlabel('episodes')\n",
    "plt.ylabel('frequency')\n",
    "plt.title(\"frequency of policy selection\")\n",
    "plt.show()\n",
    "#plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_fre_v04_003,SMOOTH_RADIUS), label = \"caps_v04_003\",color='g')\n",
    "#plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_fre_v14_003,SMOOTH_RADIUS), label = \"caps_v14_003\",color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RATIO = 10\n",
    "SMOOTH_RADIUS = 20\n",
    "\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(qlearn_rewards_v003[:EPISODE_NUM//RATIO], SMOOTH_RADIUS), label = \"qlearn_v003\", color='k')\n",
    "#plt.plot(range(EPISODE_NUM), smooth(qlearn_rewards_v5, 100), label = \"qlearn_v5\")\n",
    "\n",
    "#plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v000_003[:EPISODE_NUM//RATIO],SMOOTH_RADIUS), label = \"caps_v0_003\",color='b')\n",
    "#plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v001_003[:EPISODE_NUM//RATIO],SMOOTH_RADIUS), label = \"caps_v1_003\",color='m')\n",
    "#plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v004_003[:EPISODE_NUM//RATIO],SMOOTH_RADIUS), label = \"caps_v4_003\",color='y')\n",
    "\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v01_003[:EPISODE_NUM//RATIO],SMOOTH_RADIUS), label = \"caps_v01_003\",color='c')\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v04_003[:EPISODE_NUM//RATIO],SMOOTH_RADIUS), label = \"caps_v04_003\",color='g')\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v14_003[:EPISODE_NUM//RATIO],SMOOTH_RADIUS), label = \"caps_v14_003\",color='r')\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v014_003[:EPISODE_NUM//RATIO], SMOOTH_RADIUS), label = \"caps_v014_003\", color='navy')\n",
    "\n",
    "plt.title(\"first 400 episode-reward graph\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"rewards\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#ops_v1_003\n",
    "ops_episodes_length_v1_003, ops_penalties_v1_003,ops_rewards_v1_003, ops_q_table_v1_003 = transform(tl_algo= OPS_TL,\n",
    "                                                                                       episode_num = EPISODE_NUM,\n",
    "                                                                                       repeat_times = REPEAT_TIMES,\n",
    "                                                                                       target_task = \"Taxi-v003\",\n",
    "                                                                                       source_task = \"Taxi-v1\",\n",
    "                                                                                       policy_library = [[q_func] for q_func in qlearn_q_table_v1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#ops_v4_003\n",
    "ops_episodes_length_v4_003, ops_penalties_v4_003,ops_rewards_v4_003, ops_q_table_v4_003 = transform(tl_algo= OPS_TL,\n",
    "                                                                                       episode_num = EPISODE_NUM,\n",
    "                                                                                       repeat_times = REPEAT_TIMES,\n",
    "                                                                                       target_task = \"Taxi-v003\",\n",
    "                                                                                       source_task = \"Taxi-v4\",\n",
    "                                                                                       policy_library = [[q_func] for q_func in qlearn_q_table_v4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#ops_v14_003\n",
    "ops_episodes_length_v14_003, ops_penalties_v14_003,ops_rewards_v14_003, ops_q_table_v14_003 = transform(tl_algo= OPS_TL,\n",
    "                                                                                       episode_num = EPISODE_NUM,\n",
    "                                                                                       repeat_times = REPEAT_TIMES,\n",
    "                                                                                       target_task = \"Taxi-v003\",\n",
    "                                                                                       source_task = \"Taxi-v4\",\n",
    "                                                                                       policy_library = [[q_func1,q_func2 ] for q_func1,q_func2 in zip(qlearn_q_table_v1, qlearn_q_table_v4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#caps_v14_003\n",
    "caps_episodes_length_v14_003, ops__norm_penalties_v14_003,caps_rewards_v14_003, caps_q_table_v14_003 = transform(tl_algo= caps,\n",
    "                                                                                       episode_num = EPISODE_NUM,\n",
    "                                                                                       repeat_times = 5,\n",
    "                                                                                       target_task = \"Taxi-v003\",\n",
    "                                                                                       source_task = \"Taxi-v4\",\n",
    "                                                                                       policy_library = [[q_func1,q_func2 ] for q_func1,q_func2 in zip(qlearn_q_table_v1, qlearn_q_table_v4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "#caps_v14_003\n",
    "pl=[[q_func1,q_func2 ] for q_func1,q_func2 in zip(qlearn_q_table_v1, qlearn_q_table_v4)]\n",
    "ctuple = (0, 0.0049, 0.1, 0.2, 0.4,0.8, 1, 2, 4, 8, 16)\n",
    "ops_cnorm_episodes_length_v14_003,ops__cnorm_penalties_v14_003,ops_cnorm_rewards_v14_003,ops_cnorm_q_table_v14_003 = [0] * len(ctuple), [0]*len(ctuple),[0]*len(ctuple), [0]*len(ctuple)\n",
    "for i in range(len(ctuple)):\n",
    "    OPS_patial = functools.partial( caps, c = ctuple[i])\n",
    "    ops_cnorm_episodes_length_v14_003[i], ops__cnorm_penalties_v14_003[i],ops_cnorm_rewards_v14_003[i], ops_cnorm_q_table_v14_003[i] = transform(tl_algo= OPS_patial,\n",
    "               episode_num = EPISODE_NUM,\n",
    "               repeat_times = REPEAT_TIMES,\n",
    "               target_task = \"Taxi-v003\",\n",
    "               policy_library = pl)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style = \"darkgrid\")\n",
    "plt.plot(range(EPISODE_NUM), smooth(qlearn_rewards_v003, 100), label = \"qlearn_v003\", color='k')\n",
    "for i in range(len(ctuple)):\n",
    "    plt.plot(range(EPISODE_NUM), smooth(ops_cnorm_rewards_v14_003[i], 100), label = \"c = \"+str(ctuple[i]))\n",
    "    \n",
    "plt.title(\"episode-reward graph\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"rewards\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RATIO = 20\n",
    "SMOOTH_RADIUS = 200\n",
    "\n",
    "\n",
    "for i in range(len(ctuple)):\n",
    "    plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_cnorm_rewards_v14_003[i][:EPISODE_NUM//RATIO], SMOOTH_RADIUS), label = \"c = \"+str(ctuple[i]))\n",
    "    \n",
    "plt.title(\"reward-epsidoe graph using caps different c\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"rewards\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ctuple)):\n",
    "    print(f\"c = {ctuple[i]}, reward = {smooth(ops_cnorm_rewards_v14_003[i][:EPISODE_NUM//20], 200)[0]}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.rcParams['font.sans-serif'] = ['SimHei']\n",
    "matplotlib.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "reward = [smooth( ops_cnorm_rewards_v14_003[i][:EPISODE_NUM//20], 200)[0] for i in range(len(ctuple)) ]\n",
    "\"\"\"\n",
    "绘制水平条形图方法barh\n",
    "参数一：y轴\n",
    "参数二：x轴\n",
    "\"\"\"\n",
    "plt.barh(range(len(ctuple)), reward, height=0.7, color='steelblue', alpha=0.8)      # 从下往上画\n",
    "plt.yticks(range(len(ctuple)), [f\"c={str(ctuple[i])}\" for i in range(len(ctuple))])\n",
    "plt.xlim(15,20)\n",
    "plt.xlabel(\"average reward \")\n",
    "plt.title(\"average reward over first 200 episode under different c\")\n",
    "for x, y in enumerate(reward):\n",
    "    plt.text(y + 0.2, x - 0.1, '%s' % y)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "sns.set(style = \"darkgrid\")\n",
    "\n",
    "\n",
    "plt.plot(range(EPISODE_NUM), smooth(qlearn_rewards_v003, 100), label = \"qlearn_v003\", color='k')\n",
    "#plt.plot(range(EPISODE_NUM), smooth(qlearn_rewards_v5, 100), label = \"qlearn_v5\")\n",
    "\n",
    "#plt.plot(range(EPISODE_NUM), smooth(qlearn_rewards_v1_3,100), label = \"qlearn_v1_3\",color='c')\n",
    "#plt.plot(range(EPISODE_NUM), smooth(qlearn_rewards_v4_3,100), label = \"qlearn_v4_3\",color='g')\n",
    "\n",
    "#plt.plot(range(EPISODE_NUM), smooth(pr_rewards_v4_3, 100), label = \"pr_v4_3\")\n",
    "#plt.plot(range(EPISODE_NUM), smooth(pr_rewards_v1_3, 100), label = \"pr_v1_3\")\n",
    "\n",
    "plt.plot(range(EPISODE_NUM), smooth(ops_rewards_v4_003, 100), label = \"ops_v4_003\", color='r')\n",
    "plt.plot(range(EPISODE_NUM), smooth(ops_rewards_v1_003, 100), label = \"ops_v1_003\", color='b')\n",
    "\n",
    "plt.plot(range(EPISODE_NUM), smooth(ops_rewards_v14_003, 100), label = \"ops_v14_003\", color='y')\n",
    "plt.plot(range(EPISODE_NUM), smooth(caps_rewards_v14_003, 100), label = \"caps_v14_003\", color='navy')\n",
    "\n",
    "plt.title(\"episode-reward graph\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"rewards\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "RATIO = 8\n",
    "SMOOTH_RADIUS = 25\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(qlearn_rewards_v003[:EPISODE_NUM//RATIO], SMOOTH_RADIUS), label = \"qlearn_v003\", color='k')\n",
    "#plt.plot(range(EPISODE_NUM), smooth(qlearn_rewards_v5, 100), label = \"qlearn_v5\")\n",
    "\n",
    "#plt.plot(range(EPISODE_NUM), smooth(qlearn_rewards_v1_3,100), label = \"qlearn_v1_3\",color='c')\n",
    "#plt.plot(range(EPISODE_NUM), smooth(qlearn_rewards_v4_3,100), label = \"qlearn_v4_3\",color='g')\n",
    "\n",
    "#plt.plot(range(EPISODE_NUM), smooth(pr_rewards_v4_3, 100), label = \"pr_v4_3\")\n",
    "#plt.plot(range(EPISODE_NUM), smooth(pr_rewards_v1_3, 100), label = \"pr_v1_3\")\n",
    "\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v4_003[:EPISODE_NUM//RATIO], SMOOTH_RADIUS), label = \"ops_v4_003\", color='r')\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v1_003[:EPISODE_NUM//RATIO], SMOOTH_RADIUS), label = \"ops_v1_003\", color='b')\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v14_003[:EPISODE_NUM//RATIO], SMOOTH_RADIUS), label = \"ops_v14_003\", color='y')\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(caps_rewards_v14_003[:EPISODE_NUM//RATIO], SMOOTH_RADIUS), label = \"caps_v14_003\", color='navy')\n",
    "\n",
    "plt.title(\"episode-reward graph\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"rewards\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RATIO = 8\n",
    "SMOOTH_WINDOW = 20\n",
    "plt.plot(range(EPISODE_NUM//RATIO),smooth(qlearn_episodes_length_v003[:EPISODE_NUM//RATIO],SMOOTH_WINDOW), label = 'qlearn_v3')\n",
    "'''\n",
    "plt.plot(range(train_episodes),qlearn_episodes_length_v1_3, label = 'qlearn_v1_3')\n",
    "plt.plot(range(train_episodes),qlearn_episodes_length_v4_3, label = 'qlearn_v4_3')\n",
    "\n",
    "plt.plot(range(train_episodes),sarsa_episodes_length_v3, label = 'sarsa_v3')\n",
    "plt.plot(range(train_episodes),sarsa_episodes_length_v1_3, label = 'sarsa_v1_3')\n",
    "plt.plot(range(train_episodes),sarsa_episodes_length_v4_3, label = 'sarsa_v4_3')\n",
    "'''\n",
    "plt.plot(range(EPISODE_NUM//RATIO),smooth(ops_episodes_length_v1_003[:EPISODE_NUM//RATIO],SMOOTH_WINDOW), label = 'ops_v1_3')\n",
    "plt.plot(range(EPISODE_NUM//RATIO),smooth(ops_episodes_length_v4_003[:EPISODE_NUM//RATIO],SMOOTH_WINDOW), label = 'ops_v4_3')\n",
    "\n",
    "plt.title(\"episode-length graph\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"length\")\n",
    "plt.legend()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RATIO = 20\n",
    "SMOOTH_RADIUS = 20\n",
    "plt.plot(range(EPISODE_NUM//RATIO),smooth(qlearn_penalties_v03[:EPISODE_NUM//RATIO],SMOOTH_RADIUS), label = 'qlearn_v3')\n",
    "'''\n",
    "plt.plot(range(train_episodes),qlearn_episodes_length_v1_3, label = 'qlearn_v1_3')\n",
    "plt.plot(range(train_episodes),qlearn_episodes_length_v4_3, label = 'qlearn_v4_3')\n",
    "\n",
    "plt.plot(range(train_episodes),sarsa_episodes_length_v3, label = 'sarsa_v3')\n",
    "plt.plot(range(train_episodes),sarsa_episodes_length_v1_3, label = 'sarsa_v1_3')\n",
    "plt.plot(range(train_episodes),sarsa_episodes_length_v4_3, label = 'sarsa_v4_3')\n",
    "'''\n",
    "plt.plot(range(EPISODE_NUM//RATIO),smooth(ops_penalties_v1_03[:EPISODE_NUM//RATIO],SMOOTH_RADIUS), label = 'ops_v1_3')\n",
    "plt.plot(range(EPISODE_NUM//RATIO),smooth(ops_penalties_v4_03[:EPISODE_NUM//RATIO],SMOOTH_RADIUS), label = 'ops_v4_3')\n",
    "\n",
    "plt.title(\"episode-penalties graph\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"penalties\")\n",
    "plt.legend()\n",
    "plt.show\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def visit_all(dic):\n",
    "    for value in dic.values():\n",
    "        if value == 0:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "sum = 0\n",
    "times = 1000\n",
    "for i in range(times):    \n",
    "    dic = {i: 0 for i in range(100)}\n",
    "    while not visit_all(dic):\n",
    "        key = random.choice(range(len(dic)))\n",
    "        dic[key] = 1\n",
    "        sum += 1\n",
    "sum /= times\n",
    "print(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def div_add(n):\n",
    "    sum = 0\n",
    "    for i in range(1,n+1):\n",
    "        sum += 1/i\n",
    "        \n",
    "    return n * sum\n",
    "div_add(3000)/200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum = 0\n",
    "times = 1000\n",
    "for i in range(times):    \n",
    "    dic = {i: 0 for i in range(100)}\n",
    "    while dic[0] == 0:\n",
    "        key = random.choice(range(len(dic)))\n",
    "        dic[key] = 1\n",
    "        sum += 1\n",
    "sum /= times\n",
    "print(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_reuse(env, past_policy, train_episodes, fi = 1, mu = 0.95):\n",
    "        \n",
    "    # Hyper parameters\n",
    "    alpha = 0.05  #(alpha) is the learning rate (0<α≤1) \n",
    "    gamma = 0.95  # (gamma) is the discount factor (0≤γ≤1) \n",
    "    epsilon = 0.1 # the rate to act randomly in the epsilon-greedy alg for expolration in Q-learning\n",
    "    \n",
    "    # To plot learning curve\n",
    "    all_episodes_length = []\n",
    "    all_penalties = []\n",
    "    all_rewards = []\n",
    "\n",
    "    q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    " \n",
    "    past_table = copy.deepcopy(past_policy[0])\n",
    "    \n",
    "    \n",
    "    for i in range(train_episodes):\n",
    "        state = env.reset()\n",
    "\n",
    "        episode_penalties, episode_rewards, episode_length = 0, 0, 0\n",
    "        done = False\n",
    "        f = fi\n",
    "        while not done:\n",
    "            # epsilon greedy alg balancing exporation and exploitation\n",
    "            if random.uniform(0,1) < f:\n",
    "                action = arg_max(past_table[state])\n",
    "                #print(f\"f = {f} in episode {i} \\n\")\n",
    "            elif random.uniform(0,1) < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "                #print(\"random action \\n\")\n",
    "                \n",
    "            else:\n",
    "                action = arg_max(q_table[state])\n",
    "                #print(\"on the policy now \\n\")\n",
    "\n",
    "            # step to next state\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "\n",
    "            # update q-value\n",
    "            q_value = (1-alpha) * q_table[state, action] + alpha * (reward + gamma * np.max(q_table[next_state]))\n",
    "            q_table[state, action] = q_value\n",
    "\n",
    "            state = next_state\n",
    "            f = f*mu\n",
    "            #update data for learning curve\n",
    "            if reward == -10:\n",
    "                episode_penalties +=1\n",
    "\n",
    "            episode_rewards += reward\n",
    "            episode_length += 1\n",
    "\n",
    "        #record data for learning curve\n",
    "        all_episodes_length.append(episode_length)\n",
    "        all_penalties.append(episode_penalties)\n",
    "        all_rewards.append(episode_rewards)\n",
    "\n",
    "        #show training progress\n",
    "        if (i+1) % 100 == 0:\n",
    "            clear_output(wait=True)\n",
    "            print(f\"Episode {i}\")\n",
    "    print(\"Training finished \\n\")\n",
    "    \n",
    "    # return ndarray\n",
    "    all_episodes_length = np.array(all_episodes_length)\n",
    "    all_penalties = np.array(all_penalties)\n",
    "    all_rewards = np.array(all_rewards)\n",
    "    \n",
    "    return all_episodes_length, all_penalties, all_rewards, q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "sns.set(style = \"darkgrid\")\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(range(EPISODE_NUM), smooth(qlearn_rewards_v3, 100), label = \"qlearn_v3\", color='k')\n",
    "#plt.plot(range(EPISODE_NUM), smooth(qlearn_rewards_v5, 100), label = \"qlearn_v5\")\n",
    "\n",
    "#plt.plot(range(EPISODE_NUM), smooth(qlearn_rewards_v1_3,100), label = \"qlearn_v1_3\",color='c')\n",
    "#plt.plot(range(EPISODE_NUM), smooth(qlearn_rewards_v4_3,100), label = \"qlearn_v4_3\",color='g')\n",
    "\n",
    "#plt.plot(range(EPISODE_NUM), smooth(pr_rewards_v4_3, 100), label = \"pr_v4_3\")\n",
    "#plt.plot(range(EPISODE_NUM), smooth(pr_rewards_v1_3, 100), label = \"pr_v1_3\")\n",
    "\n",
    "plt.plot(range(EPISODE_NUM), smooth(ops_rewards_v4_3, 100), label = \"ops_v4_3\", color='r')\n",
    "plt.plot(range(EPISODE_NUM), smooth(ops_rewards_v1_3, 100), label = \"ops_v1_3\", color='b')\n",
    "\n",
    "plt.title(\"episode-reward graph\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"rewards\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "# training v03 from scratch\n",
    "\n",
    "qlearn_episodes_length_v03, qlearn_penalties_v03, qlearn_rewards_v03, qlearn_q_table_v03 = transform(tl_algo = q_learning,\n",
    "                                                                                     episode_num = EPISODE_NUM,  \n",
    "                                                                                     repeat_times = REPEAT_TIMES,\n",
    "                                                                                     target_task = \"Taxi-v03\",\n",
    "                                                                                     source_task = None\n",
    "                                                                                    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "# training v01 from scratch\n",
    "\n",
    "qlearn_episodes_length_v01, qlearn_penalties_v01, qlearn_rewards_v01, qlearn_q_table_v01 = transform(tl_algo = q_learning,\n",
    "                                                                                     episode_num = EPISODE_NUM,  \n",
    "                                                                                     repeat_times = REPEAT_TIMES,\n",
    "                                                                                     target_task = \"Taxi-v01\",\n",
    "                                                                                     source_task = None\n",
    "                                                                                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "# training v04 from scratch\n",
    "\n",
    "qlearn_episodes_length_v04, qlearn_penalties_v04, qlearn_rewards_v04, qlearn_q_table_v04 = transform(tl_algo = q_learning,\n",
    "                                                                                     episode_num = EPISODE_NUM,  \n",
    "                                                                                     repeat_times = REPEAT_TIMES,\n",
    "                                                                                     target_task = \"Taxi-v04\",\n",
    "                                                                                     source_task = None\n",
    "                                                                                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#ops_v04_03\n",
    "ops_episodes_length_v04_03, ops_penalties_v04_03,ops_rewards_v04_03, ops_q_table_v04_03 = transform(tl_algo= OPS_TL,\n",
    "                                                                                       episode_num = EPISODE_NUM,\n",
    "                                                                                       repeat_times = REPEAT_TIMES,\n",
    "                                                                                       target_task = \"Taxi-v03\",\n",
    "                                                                                       source_task = \"Taxi-v04\",\n",
    "                                                                                       policy_library = [[q_func] for q_func in qlearn_q_table_v04])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "#ops_v01_03\n",
    "ops_episodes_length_v01_03, ops_penalties_v01_03,ops_rewards_v01_03, ops_q_table_v01_03 = transform(tl_algo= OPS_TL,\n",
    "                                                                                       episode_num = EPISODE_NUM,\n",
    "                                                                                       repeat_times = REPEAT_TIMES,\n",
    "                                                                                       target_task = \"Taxi-v03\",\n",
    "                                                                                       source_task = \"Taxi-v01\",\n",
    "                                                                                       policy_library = [[q_func] for q_func in qlearn_q_table_v01])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(500), smooth(qlearn_rewards_v5[:500], 20), label = \"qlearn_v5\")\n",
    "#plt.plot(range(EPISODE_NUM), smooth(qlearn_rewards_v5, 100), label = \"qlearn_v5\")\n",
    "\n",
    "#plt.plot(range(EPISODE_NUM), smooth(qlearn_rewards_v1_3,100), label = \"qlearn_v1_3\")\n",
    "#plt.plot(range(EPISODE_NUM), smooth(qlearn_rewards_v4_3,100), label = \"qlearn_v4_3\")\n",
    "\n",
    "#plt.plot(range(EPISODE_NUM), smooth(pr_rewards_v4_3, 100), label = \"pr_v4_3\")\n",
    "#plt.plot(range(EPISODE_NUM), smooth(pr_rewards_v1_3, 100), label = \"pr_v1_3\")\n",
    "\n",
    "plt.plot(range(500), smooth(ops_rewards_v4_5[:500], 20), label = \"ops_v4_5\")\n",
    "plt.plot(range(500), smooth(ops_rewards_v1_5[:500], 20), label = \"ops_v1_5\")\n",
    "\n",
    "plt.title(\"episode-reward graph\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"rewards\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in qlearn_rewards_v3:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Taxi-v3\")\n",
    "r,_, _ = policy_evaluate(env, np.zeros([env.observation_space.n,env.action_space.n]), 1)\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(range(EPISODE_NUM//10 ),smooth(pr_rewards_v4_3[-EPISODE_NUM//10: ],100), label = 'pr_v4_3')\n",
    "plt.plot(range(EPISODE_NUM//10 ),smooth(pr_rewards_v4_3[-EPISODE_NUM//10: ],100), label = 'prql_v4_3')\n",
    "plt.plot(range(EPISODE_NUM//10), smooth(qlearn_rewards_v3[-EPISODE_NUM//10:], 100), label = \"qlearn_v3\")\n",
    "plt.plot(range(EPISODE_NUM//10), smooth(qlearn_rewards_v4[-EPISODE_NUM//10:], 100), label = \"qlearn_v4\")\n",
    "plt.plot(range(EPISODE_NUM//10), smooth(qlearn_rewards_v1_3[-EPISODE_NUM//10:],100), label = \"qlearn_v1_3\")\n",
    "\n",
    "plt.title(\"smoothed episode-reward graph: last {} episodes\".format(EPISODE_NUM//10))\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"reward\")\n",
    "plt.legend()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ops_episodes_length_v4_3, ops_penalties_v4_3,ops_rewards_v4_3, ops_q_table_v4_3 = transform(tl_algo= OPS_TL,\n",
    "                                                                                       episode_num = EPISODE_NUM,\n",
    "                                                                                       repeat_times = 10,\n",
    "                                                                                       target_task = \"Taxi-v3\",\n",
    "                                                                                       source_task = \"Taxi-v4\",\n",
    "                                                                                       policy_library = [[q_func] for q_func in qlearn_q_table_v4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "train_episodes = EPISODE_NUM\n",
    "\n",
    "plt.plot(range(train_episodes),smooth(qlearn_rewards_v3,100), 'k', label = 'qlearn_v3')\n",
    "plt.plot(range(train_episodes),smooth(qlearn_rewards_v1_3,100), label = 'qlearn_v1_3')\n",
    "plt.plot(range(train_episodes),smooth(qlearn_rewards_v4_3,100), label = 'qlearn_v4_3')\n",
    "\n",
    "\n",
    "plt.plot(range(train_episodes),smooth(pr_rewards_v1_3,100), label = 'pr_v1_3')\n",
    "plt.plot(range(train_episodes),smooth(pr_rewards_v4_3,100), label = 'pr_v4_3')\n",
    "\n",
    "\n",
    "plt.plot(range(train_episodes),smooth(ops_rewards_v1_3,100), label = 'ops_v1_3')\n",
    "plt.plot(range(train_episodes),smooth(ops_rewards_v4_3,100), label = 'ops_v4_3')\n",
    "\n",
    "plt.title(\"episode-reward graph\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"reward\")\n",
    "plt.legend()\n",
    "plt.show\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(train_episodes//10 ),smooth(qlearn_rewards_v3[-train_episodes//10: ],100), 'k',label = 'qlearn_v3')\n",
    "plt.plot(range(train_episodes//10 ),smooth(qlearn_rewards_v1_3[-train_episodes//10: ],100), label = 'qlearn_v1_3')\n",
    "plt.plot(range(train_episodes//10 ),smooth(qlearn_rewards_v4_3[-train_episodes//10: ],100), label = 'qlearn_v1_3')\n",
    "\n",
    "#plt.plot(range(train_episodes//10 ),smooth(sarsa_rewards_v3[-train_episodes//10: ],100), label = 'sarsa_v3')\n",
    "#plt.plot(range(train_episodes//10 ),smooth(sarsa_rewards_v1_3[-train_episodes//10: ],100), label = 'sarsa_v1_3')\n",
    "#plt.plot(range(train_episodes//10 ),smooth(sarsa_rewards_v4_3[-train_episodes//10: ],100), label = 'sarsa_v1_3')\n",
    "\n",
    "plt.plot(range(train_episodes//10 ),smooth(pr_rewards_v1_3[-train_episodes//10: ],100), label = 'pr_v1_3')\n",
    "plt.plot(range(train_episodes//10 ),smooth(pr_rewards_v4_3[-train_episodes//10: ],100), label = 'pr_v4_3')\n",
    "\n",
    "plt.plot(range(train_episodes//10 ),smooth(pr1_rewards_v1_3[-train_episodes//10: ],100), label = 'pr1_v1_3')\n",
    "plt.plot(range(train_episodes//10 ),smooth(pr1_rewards_v4_3[-train_episodes//10: ],100), label = 'pr1_v4_3')\n",
    "\n",
    "\n",
    "plt.title(\"smoothed episode-reward graph: last 1000 episodes\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"reward\")\n",
    "plt.legend()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(range(train_episodes//100),qlearn_rewards_v3[:train_episodes//100], label = 'qlearn_v3')\n",
    "plt.plot(range(train_episodes//100),qlearn_rewards_v1_3[:train_episodes//100], label = 'qlearn_v1_3')\n",
    "plt.plot(range(train_episodes//100),qlearn_rewards_v4_3[:train_episodes//100], label = 'qlearn_v4_3')\n",
    "\n",
    "plt.plot(range(train_episodes//100),sarsa_rewards_v3[:train_episodes//100], label = 'sarsa_v3')\n",
    "plt.plot(range(train_episodes//100),sarsa_rewards_v1_3[:train_episodes//100], label = 'sarsa_v1_3')\n",
    "plt.plot(range(train_episodes//100),sarsa_rewards_v4_3[:train_episodes//100], label = 'sarsa_v4_3')\n",
    "\n",
    "plt.title(\"episode-reward graph: first 1000 episodes\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"reward\")\n",
    "plt.legend()\n",
    "plt.show\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(train_episodes//100),smooth(qlearn_rewards_v3[:train_episodes//100],10), label = 'qlearn_v3')\n",
    "plt.plot(range(train_episodes//100),smooth(qlearn_rewards_v1_3[:train_episodes//100],10), label = 'qlearn_v1_3')\n",
    "plt.plot(range(train_episodes//100),smooth(qlearn_rewards_v4_3[:train_episodes//100],10), label = 'qlearn_v4_3')\n",
    "\n",
    "plt.plot(range(train_episodes//100),smooth(sarsa_rewards_v3[:train_episodes//100],10), label = 'sarsa_v3')\n",
    "plt.plot(range(train_episodes//100),smooth(sarsa_rewards_v1_3[:train_episodes//100],10), label = 'sarsa_v1_3')\n",
    "plt.plot(range(train_episodes//100),smooth(sarsa_rewards_v4_3[:train_episodes//100],10), label = 'sarsa_v4_3')\n",
    "\n",
    "plt.title(\"smoothed episode-reward graph: first 1000 episodes\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"reward\")\n",
    "plt.legend()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(train_episodes//100),smooth(qlearn_rewards_v3[:train_episodes//100],10), label = 'qlearn_v3')\n",
    "plt.plot(range(train_episodes//100),smooth(qlearn_rewards_v1_3[:train_episodes//100],10), label = 'qlearn_v1_3')\n",
    "plt.plot(range(train_episodes//100),smooth(qlearn_rewards_v4_3[:train_episodes//100],10), label = 'qlearn_v4_3')\n",
    "\n",
    "\n",
    "plt.title(\"q-learning smoothed episode-reward graph: first 1000 episodes\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"reward\")\n",
    "plt.legend()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(train_episodes//100),qlearn_episodes_length_v3[:train_episodes//100], label = 'qlearn_v3')\n",
    "plt.plot(range(train_episodes//100),qlearn_episodes_length_v1_3[:train_episodes//100], label = 'qlearn_v1_3')\n",
    "plt.plot(range(train_episodes//100),qlearn_episodes_length_v4_3[:train_episodes//100], label = 'qlearn_v4_3')\n",
    "\n",
    "plt.plot(range(train_episodes//100),sarsa_episodes_length_v3[:train_episodes//100], label = 'sarsa_v3')\n",
    "plt.plot(range(train_episodes//100),sarsa_episodes_length_v1_3[:train_episodes//100], label = 'sarsa_v1_3')\n",
    "plt.plot(range(train_episodes//100),sarsa_episodes_length_v4_3[:train_episodes//100], label = 'sarsa_v4_3')\n",
    "\n",
    "plt.title(\"episode-length graph: first 1000 episodes\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"length\")\n",
    "plt.legend()\n",
    "plt.show\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(train_episodes//100),smooth(qlearn_episodes_length_v3[:train_episodes//100],10), label = 'qlearn_v3')\n",
    "plt.plot(range(train_episodes//100),smooth(qlearn_episodes_length_v1_3[:train_episodes//100],10), label = 'qlearn_v1_3')\n",
    "plt.plot(range(train_episodes//100),smooth(qlearn_episodes_length_v4_3[:train_episodes//100],10), label = 'qlearn_v4_3')\n",
    "\n",
    "plt.plot(range(train_episodes//100),smooth(sarsa_episodes_length_v3[:train_episodes//100],10), label = 'sarsa_v3')\n",
    "plt.plot(range(train_episodes//100),smooth(sarsa_episodes_length_v1_3[:train_episodes//100],10), label = 'sarsa_v1_3')\n",
    "plt.plot(range(train_episodes//100),smooth(sarsa_episodes_length_v4_3[:train_episodes//100],10), label = 'sarsa_v4_3')\n",
    "\n",
    "plt.title(\"smoothed episode-length graph: first 1000 episodes\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"length\")\n",
    "plt.legend()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(train_episodes//100),smooth(qlearn_episodes_length_v3[:train_episodes//100],10), label = 'qlearn_v3')\n",
    "plt.plot(range(train_episodes//100),smooth(qlearn_episodes_length_v1_3[:train_episodes//100],10), label = 'qlearn_v1_3')\n",
    "plt.plot(range(train_episodes//100),smooth(qlearn_episodes_length_v4_3[:train_episodes//100],10), label = 'qlearn_v4_3')\n",
    "\n",
    "plt.title(\"q-learning smoothed episode-length graph: first 1000 episodes\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"length\")\n",
    "plt.legend()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(train_episodes//100),smooth(sarsa_episodes_length_v3[:train_episodes//100],10), label = 'sarsa_v3')\n",
    "plt.plot(range(train_episodes//100),smooth(sarsa_episodes_length_v1_3[:train_episodes//100],10), label = 'sarsa_v1_3')\n",
    "plt.plot(range(train_episodes//100),smooth(sarsa_episodes_length_v4_3[:train_episodes//100],10), label = 'sarsa_v4_3')\n",
    "\n",
    "plt.title(\"sarsa smoothed episode-length graph: first 1000 episodes\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"length\")\n",
    "plt.legend()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(train_episodes),qlearn_penalties_v3, label = 'qlearn_v3')\n",
    "plt.plot(range(train_episodes),qlearn_penalties_v1_3, label = 'qlearn_v1_3')\n",
    "plt.plot(range(train_episodes),qlearn_penalties_v4_3, label = 'qlearn_v4_3')\n",
    "\n",
    "plt.plot(range(train_episodes),sarsa_penalties_v3, label = 'sarsa_v3')\n",
    "plt.plot(range(train_episodes),sarsa_penalties_v1_3, label = 'sarsa_v1_3')\n",
    "plt.plot(range(train_episodes),sarsa_penalties_v4_3, label = 'sarsa_v4_3')\n",
    "\n",
    "plt.title(\"episode-penalties graph\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"penalties\")\n",
    "plt.legend()\n",
    "plt.show\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(train_episodes//100),qlearn_penalties_v3[:train_episodes//100], label = 'qlearn_v3')\n",
    "plt.plot(range(train_episodes//100),qlearn_penalties_v1_3[:train_episodes//100], label = 'qlearn_v1_3')\n",
    "plt.plot(range(train_episodes//100),qlearn_penalties_v4_3[:train_episodes//100], label = 'qlearn_v4_3')\n",
    "\n",
    "plt.plot(range(train_episodes//100),sarsa_penalties_v3[:train_episodes//100], label = 'sarsa_v3')\n",
    "plt.plot(range(train_episodes//100),sarsa_penalties_v1_3[:train_episodes//100], label = 'sarsa_v1_3')\n",
    "plt.plot(range(train_episodes//100),sarsa_penalties_v4_3[:train_episodes//100], label = 'sarsa_v4_3')\n",
    "\n",
    "plt.title(\"episode-penalties graph: first 1000 episodes\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"penalties\")\n",
    "plt.legend()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(train_episodes//100),smooth(qlearn_penalties_v3[:train_episodes//100],10), label = 'qlearn_v3')\n",
    "plt.plot(range(train_episodes//100),smooth(qlearn_penalties_v1_3[:train_episodes//100],10), label = 'qlearn_v1_3')\n",
    "plt.plot(range(train_episodes//100),smooth(qlearn_penalties_v4_3[:train_episodes//100],10), label = 'qlearn_v4_3')\n",
    "\n",
    "plt.plot(range(train_episodes//100),smooth(sarsa_penalties_v3[:train_episodes//100],10), label = 'sarsa_v3')\n",
    "plt.plot(range(train_episodes//100),smooth(sarsa_penalties_v1_3[:train_episodes//100],10), label = 'sarsa_v1_3')\n",
    "plt.plot(range(train_episodes//100),smooth(sarsa_penalties_v4_3[:train_episodes//100],10), label = 'sarsa_v4_3')\n",
    "\n",
    "plt.title(\"smoothed episode-penalties graph: first 1000 episodes\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"penalties \")\n",
    "plt.legend()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(train_episodes//100),smooth(qlearn_penalties_v3[:train_episodes//100],10), label = 'qlearn_v3')\n",
    "plt.plot(range(train_episodes//100),smooth(qlearn_penalties_v1_3[:train_episodes//100],10), label = 'qlearn_v1_3')\n",
    "plt.plot(range(train_episodes//100),smooth(qlearn_penalties_v4_3[:train_episodes//100],10), label = 'qlearn_v4_3')\n",
    "\n",
    "\n",
    "plt.title(\"q-learning smoothed episode-penalties graph: first 1000 episodes\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"penalties \")\n",
    "plt.legend()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(train_episodes//100),smooth(sarsa_penalties_v3[:train_episodes//100],10), label = 'sarsa_v3')\n",
    "plt.plot(range(train_episodes//100),smooth(sarsa_penalties_v1_3[:train_episodes//100],10), label = 'sarsa_v1_3')\n",
    "plt.plot(range(train_episodes//100),smooth(sarsa_penalties_v4_3[:train_episodes//100],10), label = 'sarsa_v4_3')\n",
    "\n",
    "plt.title(\"sarsa smoothed episode-penalties graph: first 1000 episodes\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"penalties \")\n",
    "plt.legend()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Exp1: similarity diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 799800\n",
      "Training finished\n",
      "CPU times: user 8min, sys: 1min 42s, total: 9min 42s\n",
      "Wall time: 8min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#qlearn_v004_003\n",
    "qlearn_episodes_length_v004_003, qlearn_penalties_v004_003,qlearn_rewards_v004_003, _, qlearn_q_table_v004_003 = transform(tl_algo= q_learning,\n",
    "                                                                                       episode_num = EPISODE_NUM,\n",
    "                                                                                       repeat_times = REPEAT_TIMES,\n",
    "                                                                                       target_task = \"Taxi-v003\",\n",
    "                                                                                       \n",
    "                                                                                       policy_library =  qlearn_q_table_v004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 799800\n",
      "Training finished\n",
      "CPU times: user 21min 52s, sys: 5min 52s, total: 27min 45s\n",
      "Wall time: 24min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#qlearn_v001_003\n",
    "qlearn_episodes_length_v001_003, qlearn_penalties_v001_003,qlearn_rewards_v001_003, _, qlearn_q_table_v001_003 = transform(tl_algo= q_learning,\n",
    "                                                                                       episode_num = EPISODE_NUM,\n",
    "                                                                                       repeat_times = REPEAT_TIMES,\n",
    "                                                                                       target_task = \"Taxi-v003\",\n",
    "                                                                                       \n",
    "                                                                                       policy_library =  qlearn_q_table_v001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 799800\n",
      "Training finished \n",
      "\n",
      "CPU times: user 23min 8s, sys: 8min 17s, total: 31min 26s\n",
      "Wall time: 27min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#sarsa_v001_003\n",
    "sarsa_episodes_length_v001_003, sarsa_penalties_v001_003,sarsa_rewards_v001_003, _, sarsa_q_table_v001_003 = transform(tl_algo= sarsa,\n",
    "                                                                                       episode_num = EPISODE_NUM,\n",
    "                                                                                       repeat_times = REPEAT_TIMES,\n",
    "                                                                                       target_task = \"Taxi-v003\",\n",
    "                                                                                       \n",
    "                                                                                       policy_library =  sarsa_q_table_v001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 799800\n",
      "Training finished \n",
      "\n",
      "CPU times: user 7min 31s, sys: 1min 47s, total: 9min 18s\n",
      "Wall time: 8min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#sarsa_v004_003\n",
    "sarsa_episodes_length_v004_003, sarsa_penalties_v004_003,sarsa_rewards_v004_003, _, sarsa_q_table_v004_003 = transform(tl_algo= sarsa,\n",
    "                                                                                       episode_num = EPISODE_NUM,\n",
    "                                                                                       repeat_times = REPEAT_TIMES,\n",
    "                                                                                       target_task = \"Taxi-v003\",\n",
    "                                                                                       \n",
    "                                                                                       policy_library =  sarsa_q_table_v004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 799800\n",
      "Training finished\n",
      "CPU times: user 5h 39min 33s, sys: 13min 53s, total: 5h 53min 26s\n",
      "Wall time: 5h 49min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#rl_algs = ['qlearn','sarsa']\n",
    "rl_algs = ['sarsa']\n",
    "tl_algs = [ 'prql','ops','caps']\n",
    "levels =['v001', 'v003','v004']\n",
    "for rl_alg in rl_algs:    \n",
    "    for level in levels:\n",
    "        # source task training from scratch\n",
    "        _,_,locals()[rl_alg + '_rewards_'+level ],_,locals()[rl_alg + '_q_table_'+level ] = transform(tl_algo= eval(rl_alg),\n",
    "                                                                            episode_num = EPISODE_NUM,\n",
    "                                                                            repeat_times = REPEAT_TIMES,\n",
    "                                                                            target_task = \"Taxi-v003\",\n",
    "                                                                                  )\n",
    "         # transfer training\n",
    "        if level != 'v003':\n",
    "            for tl_alg in tl_algs:\n",
    "                if tl_alg == 'prql':\n",
    "                    _,_,locals()[tl_alg + '_' + rl_alg +'_rewards_'+level +'_003'],_,_ = transform(tl_algo= eval(tl_alg),\n",
    "                                                                                episode_num = EPISODE_NUM,\n",
    "                                                                                repeat_times = REPEAT_TIMES,\n",
    "                                                                                target_task = \"Taxi-v003\",\n",
    "                                                                                policy_library =  eval(rl_alg + '_q_table_' + level))\n",
    "                else:\n",
    "                    _,_,locals()[tl_alg + '_' + rl_alg + '_rewards_'+level +'_003'],_,_ = transform(tl_algo= eval(tl_alg),\n",
    "                                                                                episode_num = EPISODE_NUM,\n",
    "                                                                                repeat_times = REPEAT_TIMES,\n",
    "                                                                                target_task = \"Taxi-v003\",\n",
    "                                                                                policy_library =  [[qfunc] for qfunc in eval(rl_alg + '_q_table_' + level)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl_algs = ['prql','ops','caps']\n",
    "data = [None] * len(algs)\n",
    "all_data = {}\n",
    "for i,tl_alg in enumerate(tl_algs):\n",
    "    data[i] = cache(tl_alg =tl_alg, rl_alg ='sarsa')\n",
    "    all_data.update(data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Close_transfer_caps_sarsa': array([[ 0.,  0.,  6., ..., 20., 20., 20.],\n",
       "        [ 0.,  0.,  4., ..., 20., 20., 20.],\n",
       "        [ 0.,  0.,  0., ..., 20., 20., 20.],\n",
       "        ...,\n",
       "        [ 2.,  2.,  6., ..., 20., 20., 20.],\n",
       "        [ 4.,  0.,  2., ..., 20., 20., 20.],\n",
       "        [ 0.,  8.,  8., ..., 20., 20., 20.]]),\n",
       " 'Close_transfer_ops_sarsa': array([[ 0.,  2.,  4., ..., 20., 20., 20.],\n",
       "        [ 0.,  0.,  0., ..., 20., 20., 20.],\n",
       "        [ 0.,  0.,  4., ..., 20., 20., 20.],\n",
       "        ...,\n",
       "        [ 0.,  0.,  0., ..., 20., 20., 20.],\n",
       "        [ 0.,  0.,  2., ..., 20., 20., 20.],\n",
       "        [ 0.,  4.,  0., ..., 20., 20., 20.]]),\n",
       " 'Close_transfer_prql_sarsa': array([[ 0.,  0.,  0., ..., 20., 20., 20.],\n",
       "        [ 0.,  2.,  0., ..., 20., 20., 20.],\n",
       "        [ 4.,  2.,  2., ..., 20., 20., 20.],\n",
       "        ...,\n",
       "        [ 0.,  0.,  0., ..., 20., 20., 20.],\n",
       "        [ 0.,  0.,  0., ..., 20., 20., 20.],\n",
       "        [ 0.,  2.,  6., ..., 20., 20., 20.]]),\n",
       " 'Far_transfer_caps_sarsa': array([[ 2.,  2.,  4., ..., 20., 20., 20.],\n",
       "        [ 2.,  0.,  2., ..., 20., 20., 20.],\n",
       "        [ 6.,  2.,  4., ..., 20., 20., 20.],\n",
       "        ...,\n",
       "        [ 4.,  0.,  2., ..., 20., 20., 20.],\n",
       "        [ 0.,  2.,  0., ..., 20., 20., 20.],\n",
       "        [ 0.,  0.,  4., ..., 20., 20., 20.]]),\n",
       " 'Far_transfer_ops_sarsa': array([[ 2.,  2.,  0., ..., 20., 20., 20.],\n",
       "        [ 2.,  0.,  0., ..., 20., 20., 20.],\n",
       "        [ 0.,  0.,  0., ..., 20., 20., 20.],\n",
       "        ...,\n",
       "        [ 0.,  0.,  8., ..., 20., 20., 20.],\n",
       "        [ 2.,  4.,  4., ..., 20., 20., 20.],\n",
       "        [ 0.,  8.,  2., ..., 20., 20., 20.]]),\n",
       " 'Far_transfer_prql_sarsa': array([[ 0.,  4.,  0., ..., 20., 20., 20.],\n",
       "        [ 0.,  0.,  4., ..., 20., 20., 20.],\n",
       "        [ 2.,  4.,  0., ..., 20., 20., 20.],\n",
       "        ...,\n",
       "        [ 0.,  2.,  0., ..., 20., 20., 20.],\n",
       "        [ 0.,  0.,  0., ..., 20., 20., 20.],\n",
       "        [ 0.,  0.,  6., ..., 20., 20., 20.]]),\n",
       " 'No_transfer_sarsa': array([[ 0.,  2.,  0., ..., 20., 20., 20.],\n",
       "        [ 2.,  0.,  0., ..., 20., 20., 20.],\n",
       "        [ 0.,  2.,  0., ..., 20., 20., 20.],\n",
       "        ...,\n",
       "        [ 2.,  0.,  0., ..., 20., 20., 20.],\n",
       "        [ 0.,  0.,  2., ..., 20., 20., 20.],\n",
       "        [ 0.,  0.,  2., ..., 20., 20., 20.]])}"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PRQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 799800\n",
      "Training finished \n",
      "\n",
      "CPU times: user 7min 4s, sys: 1min 9s, total: 8min 13s\n",
      "Wall time: 7min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#pr_v001_003\n",
    "pr_episodes_length_v001_003, pr_penalties_v001_003,pr_rewards_v001_003, _, pr_q_table_v001_003 = transform(tl_algo= prql,\n",
    "                                                                                       episode_num = EPISODE_NUM,\n",
    "                                                                                       repeat_times = REPEAT_TIMES,\n",
    "                                                                                       target_task = \"Taxi-v003\",\n",
    "                                                                                       \n",
    "                                                                                       policy_library =  qlearn_q_table_v001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 799800\n",
      "Training finished \n",
      "\n",
      "CPU times: user 7min 14s, sys: 1min 28s, total: 8min 42s\n",
      "Wall time: 7min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#pr_v004_003\n",
    "pr_episodes_length_v004_003, pr_penalties_v004_003,pr_rewards_v004_003, _, pr_q_table_v004_003 = transform(tl_algo= prql,\n",
    "                                                                                       episode_num = EPISODE_NUM,\n",
    "                                                                                       repeat_times = REPEAT_TIMES,\n",
    "                                                                                       target_task = \"Taxi-v003\",\n",
    "                                                                                       \n",
    "                                                                                       policy_library =  qlearn_q_table_v004)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OPS-TL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4000\n",
      "Training finished\n",
      "CPU times: user 31min 22s, sys: 1min, total: 32min 23s\n",
      "Wall time: 31min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#ops_v001_003\n",
    "ops_episodes_length_v001_003, ops_penalties_v001_003,ops_rewards_v001_003, _, ops_q_table_v001_003 = transform(tl_algo= OPS_TL,\n",
    "                                                                                       episode_num = EPISODE_NUM,\n",
    "                                                                                   repeat_times = REPEAT_TIMES,\n",
    "                                                                                       target_task = \"Taxi-v003\",\n",
    "                                                                                       \n",
    "                                                                                       policy_library =  [[qfunc] for qfunc in qlearn_q_table_v001])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4000\n",
      "Training finished\n",
      "CPU times: user 47min 26s, sys: 39.8 s, total: 48min 6s\n",
      "Wall time: 47min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#ops_v004_003\n",
    "ops_episodes_length_v004_003, ops_penalties_v004_003,ops_rewards_v004_003, _, ops_q_table_v004_003 = transform(tl_algo= OPS_TL,\n",
    "                                                                                       episode_num = EPISODE_NUM,\n",
    "                                                                                       repeat_times = REPEAT_TIMES,\n",
    "                                                                                       target_task = \"Taxi-v003\",\n",
    "                                                                                       \n",
    "                                                                                       policy_library =  [[qfunc] for qfunc in qlearn_q_table_v004])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### caps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 799800\n",
      "Training finished\n",
      "CPU times: user 31min 14s, sys: 1min 18s, total: 32min 33s\n",
      "Wall time: 31min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#caps_v001_003\n",
    "caps_episodes_length_v001_003, caps_penalties_v001_003,caps_rewards_v001_003, _, caps_q_table_v001_003 = transform(tl_algo= caps,\n",
    "                                                                                       episode_num = EPISODE_NUM,\n",
    "                                                                                       repeat_times = REPEAT_TIMES,\n",
    "                                                                                       target_task = \"Taxi-v003\",\n",
    "                                                                                       \n",
    "                                                                                       policy_library =  [[qfunc] for qfunc in qlearn_q_table_v001])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 799800\n",
      "Training finished\n",
      "CPU times: user 49min 35s, sys: 44.3 s, total: 50min 20s\n",
      "Wall time: 49min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#caps_v004_003\n",
    "caps_episodes_length_v004_003, caps_penalties_v004_003,caps_rewards_v004_003, _, caps_q_table_v004_003 = transform(tl_algo= caps,\n",
    "                                                                                       episode_num = EPISODE_NUM,\n",
    "                                                                                       repeat_times = REPEAT_TIMES,\n",
    "                                                                                       target_task = \"Taxi-v003\",\n",
    "                                                                                       \n",
    "                                                                                       policy_library =  [[qfunc] for qfunc in qlearn_q_table_v004])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### qlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Transfer rate</th>\n",
       "      <th>Jumpstart</th>\n",
       "      <th>Asympotic performace</th>\n",
       "      <th>Time to threshold</th>\n",
       "      <th>Accumulated rewards</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>10.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.010</td>\n",
       "      <td>10.200</td>\n",
       "      <td>0.000</td>\n",
       "      <td>24.800</td>\n",
       "      <td>1310.200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.539</td>\n",
       "      <td>3.327</td>\n",
       "      <td>0.000</td>\n",
       "      <td>228.083</td>\n",
       "      <td>1841.795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.842</td>\n",
       "      <td>6.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-303.000</td>\n",
       "      <td>-988.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.231</td>\n",
       "      <td>8.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-90.000</td>\n",
       "      <td>118.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.084</td>\n",
       "      <td>10.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-32.500</td>\n",
       "      <td>927.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.336</td>\n",
       "      <td>12.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>155.000</td>\n",
       "      <td>2323.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.811</td>\n",
       "      <td>16.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>365.000</td>\n",
       "      <td>5114.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Transfer rate  Jumpstart  Asympotic performace  Time to threshold  \\\n",
       "count         10.000     10.000                10.000             10.000   \n",
       "mean           0.010     10.200                 0.000             24.800   \n",
       "std            0.539      3.327                 0.000            228.083   \n",
       "min           -0.842      6.000                 0.000           -303.000   \n",
       "25%           -0.231      8.000                 0.000            -90.000   \n",
       "50%           -0.084     10.000                 0.000            -32.500   \n",
       "75%            0.336     12.000                 0.000            155.000   \n",
       "max            0.811     16.000                 0.000            365.000   \n",
       "\n",
       "       Accumulated rewards  \n",
       "count               10.000  \n",
       "mean              1310.200  \n",
       "std               1841.795  \n",
       "min               -988.000  \n",
       "25%                118.000  \n",
       "50%                927.000  \n",
       "75%               2323.500  \n",
       "max               5114.000  "
      ]
     },
     "execution_count": 589,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# qlearn-Close transfer \n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "res = get_results(qlearn_rewards_v003,qlearn_rewards_v004_003)\n",
    "res = pd.DataFrame(res)\n",
    "res.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Transfer rate</th>\n",
       "      <th>Jumpstart</th>\n",
       "      <th>Asympotic performace</th>\n",
       "      <th>Time to threshold</th>\n",
       "      <th>Accumulated rewards</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>10.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-inf</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-13.800</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-68990.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>nan</td>\n",
       "      <td>1.414</td>\n",
       "      <td>3.190</td>\n",
       "      <td>nan</td>\n",
       "      <td>2596.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-inf</td>\n",
       "      <td>-4.000</td>\n",
       "      <td>-18.000</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-72612.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-inf</td>\n",
       "      <td>-2.000</td>\n",
       "      <td>-16.000</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-71320.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-inf</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-14.000</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-69026.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>-inf</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-10.500</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-66954.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>-7.928</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-10.000</td>\n",
       "      <td>-3504.000</td>\n",
       "      <td>-64660.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Transfer rate  Jumpstart  Asympotic performace  Time to threshold  \\\n",
       "count         10.000     10.000                10.000             10.000   \n",
       "mean            -inf     -1.000               -13.800               -inf   \n",
       "std              nan      1.414                 3.190                nan   \n",
       "min             -inf     -4.000               -18.000               -inf   \n",
       "25%             -inf     -2.000               -16.000               -inf   \n",
       "50%             -inf      0.000               -14.000               -inf   \n",
       "75%             -inf      0.000               -10.500               -inf   \n",
       "max           -7.928      0.000               -10.000          -3504.000   \n",
       "\n",
       "       Accumulated rewards  \n",
       "count               10.000  \n",
       "mean            -68990.800  \n",
       "std               2596.006  \n",
       "min             -72612.000  \n",
       "25%             -71320.500  \n",
       "50%             -69026.000  \n",
       "75%             -66954.000  \n",
       "max             -64660.000  "
      ]
     },
     "execution_count": 590,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# qlearn-Far transfer\n",
    "res = get_results(qlearn_rewards_v003,qlearn_rewards_v001_003)\n",
    "res = pd.DataFrame(res)\n",
    "res.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PRQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Transfer rate</th>\n",
       "      <th>Jumpstart</th>\n",
       "      <th>Asympotic performace</th>\n",
       "      <th>Time to threshold</th>\n",
       "      <th>Accumulated rewards</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>10.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.910</td>\n",
       "      <td>-0.400</td>\n",
       "      <td>0.000</td>\n",
       "      <td>385.800</td>\n",
       "      <td>5854.600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.016</td>\n",
       "      <td>1.578</td>\n",
       "      <td>0.000</td>\n",
       "      <td>59.243</td>\n",
       "      <td>829.821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.886</td>\n",
       "      <td>-4.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>314.000</td>\n",
       "      <td>4970.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.898</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>338.000</td>\n",
       "      <td>5212.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.912</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>390.000</td>\n",
       "      <td>5705.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.917</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>412.500</td>\n",
       "      <td>6268.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.934</td>\n",
       "      <td>2.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>491.000</td>\n",
       "      <td>7528.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Transfer rate  Jumpstart  Asympotic performace  Time to threshold  \\\n",
       "count         10.000     10.000                10.000             10.000   \n",
       "mean           0.910     -0.400                 0.000            385.800   \n",
       "std            0.016      1.578                 0.000             59.243   \n",
       "min            0.886     -4.000                 0.000            314.000   \n",
       "25%            0.898      0.000                 0.000            338.000   \n",
       "50%            0.912      0.000                 0.000            390.000   \n",
       "75%            0.917      0.000                 0.000            412.500   \n",
       "max            0.934      2.000                 0.000            491.000   \n",
       "\n",
       "       Accumulated rewards  \n",
       "count               10.000  \n",
       "mean              5854.600  \n",
       "std                829.821  \n",
       "min               4970.000  \n",
       "25%               5212.500  \n",
       "50%               5705.000  \n",
       "75%               6268.500  \n",
       "max               7528.000  "
      ]
     },
     "execution_count": 591,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pr-Close transfer\n",
    "res = get_results(qlearn_rewards_v003,pr_rewards_v004_003)\n",
    "res = pd.DataFrame(res)\n",
    "res.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Transfer rate</th>\n",
       "      <th>Jumpstart</th>\n",
       "      <th>Asympotic performace</th>\n",
       "      <th>Time to threshold</th>\n",
       "      <th>Accumulated rewards</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>10.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.397</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>0.000</td>\n",
       "      <td>171.000</td>\n",
       "      <td>2371.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.105</td>\n",
       "      <td>1.476</td>\n",
       "      <td>0.000</td>\n",
       "      <td>66.483</td>\n",
       "      <td>1034.359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.226</td>\n",
       "      <td>-4.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>92.000</td>\n",
       "      <td>1198.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.359</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>140.000</td>\n",
       "      <td>1890.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.388</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>155.000</td>\n",
       "      <td>2155.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.399</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>182.750</td>\n",
       "      <td>2335.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.637</td>\n",
       "      <td>2.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>335.000</td>\n",
       "      <td>4742.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Transfer rate  Jumpstart  Asympotic performace  Time to threshold  \\\n",
       "count         10.000     10.000                10.000             10.000   \n",
       "mean           0.397     -0.200                 0.000            171.000   \n",
       "std            0.105      1.476                 0.000             66.483   \n",
       "min            0.226     -4.000                 0.000             92.000   \n",
       "25%            0.359      0.000                 0.000            140.000   \n",
       "50%            0.388      0.000                 0.000            155.000   \n",
       "75%            0.399      0.000                 0.000            182.750   \n",
       "max            0.637      2.000                 0.000            335.000   \n",
       "\n",
       "       Accumulated rewards  \n",
       "count               10.000  \n",
       "mean              2371.000  \n",
       "std               1034.359  \n",
       "min               1198.000  \n",
       "25%               1890.000  \n",
       "50%               2155.000  \n",
       "75%               2335.500  \n",
       "max               4742.000  "
      ]
     },
     "execution_count": 592,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pr-Far transfer\n",
    "res = get_results(qlearn_rewards_v003,pr_rewards_v001_003)\n",
    "res = pd.DataFrame(res)\n",
    "res.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### OPS-TL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Transfer rate</th>\n",
       "      <th>Jumpstart</th>\n",
       "      <th>Asympotic performace</th>\n",
       "      <th>Time to threshold</th>\n",
       "      <th>Accumulated rewards</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>10.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.883</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.000</td>\n",
       "      <td>375.000</td>\n",
       "      <td>5682.600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.032</td>\n",
       "      <td>2.119</td>\n",
       "      <td>0.000</td>\n",
       "      <td>63.821</td>\n",
       "      <td>904.087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.820</td>\n",
       "      <td>-4.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>310.000</td>\n",
       "      <td>4440.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.866</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>312.750</td>\n",
       "      <td>5097.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.886</td>\n",
       "      <td>2.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>376.000</td>\n",
       "      <td>5539.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.902</td>\n",
       "      <td>2.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>405.500</td>\n",
       "      <td>6116.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.937</td>\n",
       "      <td>2.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>493.000</td>\n",
       "      <td>7492.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Transfer rate  Jumpstart  Asympotic performace  Time to threshold  \\\n",
       "count         10.000     10.000                10.000             10.000   \n",
       "mean           0.883      0.600                 0.000            375.000   \n",
       "std            0.032      2.119                 0.000             63.821   \n",
       "min            0.820     -4.000                 0.000            310.000   \n",
       "25%            0.866      0.000                 0.000            312.750   \n",
       "50%            0.886      2.000                 0.000            376.000   \n",
       "75%            0.902      2.000                 0.000            405.500   \n",
       "max            0.937      2.000                 0.000            493.000   \n",
       "\n",
       "       Accumulated rewards  \n",
       "count               10.000  \n",
       "mean              5682.600  \n",
       "std                904.087  \n",
       "min               4440.000  \n",
       "25%               5097.500  \n",
       "50%               5539.000  \n",
       "75%               6116.500  \n",
       "max               7492.000  "
      ]
     },
     "execution_count": 593,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ops-Close transfer\n",
    "res = get_results(qlearn_rewards_v003,ops_rewards_v004_003)\n",
    "res = pd.DataFrame(res)\n",
    "res.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Transfer rate</th>\n",
       "      <th>Jumpstart</th>\n",
       "      <th>Asympotic performace</th>\n",
       "      <th>Time to threshold</th>\n",
       "      <th>Accumulated rewards</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>10.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.469</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>0.000</td>\n",
       "      <td>200.800</td>\n",
       "      <td>2892.600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.142</td>\n",
       "      <td>1.647</td>\n",
       "      <td>0.000</td>\n",
       "      <td>69.723</td>\n",
       "      <td>976.823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.272</td>\n",
       "      <td>-4.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>98.000</td>\n",
       "      <td>1606.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.384</td>\n",
       "      <td>-1.500</td>\n",
       "      <td>0.000</td>\n",
       "      <td>153.000</td>\n",
       "      <td>2026.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.460</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>222.500</td>\n",
       "      <td>3138.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.577</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>247.750</td>\n",
       "      <td>3570.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.681</td>\n",
       "      <td>2.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>301.000</td>\n",
       "      <td>4226.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Transfer rate  Jumpstart  Asympotic performace  Time to threshold  \\\n",
       "count         10.000     10.000                10.000             10.000   \n",
       "mean           0.469     -0.600                 0.000            200.800   \n",
       "std            0.142      1.647                 0.000             69.723   \n",
       "min            0.272     -4.000                 0.000             98.000   \n",
       "25%            0.384     -1.500                 0.000            153.000   \n",
       "50%            0.460      0.000                 0.000            222.500   \n",
       "75%            0.577      0.000                 0.000            247.750   \n",
       "max            0.681      2.000                 0.000            301.000   \n",
       "\n",
       "       Accumulated rewards  \n",
       "count               10.000  \n",
       "mean              2892.600  \n",
       "std                976.823  \n",
       "min               1606.000  \n",
       "25%               2026.000  \n",
       "50%               3138.000  \n",
       "75%               3570.500  \n",
       "max               4226.000  "
      ]
     },
     "execution_count": 594,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ops-Far transfer\n",
    "res = get_results(qlearn_rewards_v003,ops_rewards_v001_003)\n",
    "res = pd.DataFrame(res)\n",
    "res.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### caps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Transfer rate</th>\n",
       "      <th>Jumpstart</th>\n",
       "      <th>Asympotic performace</th>\n",
       "      <th>Time to threshold</th>\n",
       "      <th>Accumulated rewards</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>10.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.904</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>383.400</td>\n",
       "      <td>5813.600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.021</td>\n",
       "      <td>1.886</td>\n",
       "      <td>0.000</td>\n",
       "      <td>61.446</td>\n",
       "      <td>845.584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.877</td>\n",
       "      <td>-4.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>308.000</td>\n",
       "      <td>4908.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.889</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>335.500</td>\n",
       "      <td>5190.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.901</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>381.000</td>\n",
       "      <td>5567.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.915</td>\n",
       "      <td>1.500</td>\n",
       "      <td>0.000</td>\n",
       "      <td>411.250</td>\n",
       "      <td>6331.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.943</td>\n",
       "      <td>2.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>489.000</td>\n",
       "      <td>7450.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Transfer rate  Jumpstart  Asympotic performace  Time to threshold  \\\n",
       "count         10.000     10.000                10.000             10.000   \n",
       "mean           0.904      0.000                 0.000            383.400   \n",
       "std            0.021      1.886                 0.000             61.446   \n",
       "min            0.877     -4.000                 0.000            308.000   \n",
       "25%            0.889      0.000                 0.000            335.500   \n",
       "50%            0.901      0.000                 0.000            381.000   \n",
       "75%            0.915      1.500                 0.000            411.250   \n",
       "max            0.943      2.000                 0.000            489.000   \n",
       "\n",
       "       Accumulated rewards  \n",
       "count               10.000  \n",
       "mean              5813.600  \n",
       "std                845.584  \n",
       "min               4908.000  \n",
       "25%               5190.500  \n",
       "50%               5567.000  \n",
       "75%               6331.000  \n",
       "max               7450.000  "
      ]
     },
     "execution_count": 595,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# caps-Close transfer\n",
    "res = get_results(qlearn_rewards_v003,caps_rewards_v004_003)\n",
    "res = pd.DataFrame(res)\n",
    "res.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Transfer rate</th>\n",
       "      <th>Jumpstart</th>\n",
       "      <th>Asympotic performace</th>\n",
       "      <th>Time to threshold</th>\n",
       "      <th>Accumulated rewards</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>10.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.436</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.000</td>\n",
       "      <td>190.900</td>\n",
       "      <td>2839.400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.205</td>\n",
       "      <td>2.503</td>\n",
       "      <td>0.000</td>\n",
       "      <td>106.820</td>\n",
       "      <td>1560.574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.154</td>\n",
       "      <td>-4.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>59.000</td>\n",
       "      <td>322.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.279</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>102.500</td>\n",
       "      <td>1780.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.439</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>181.500</td>\n",
       "      <td>2611.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.632</td>\n",
       "      <td>2.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>287.750</td>\n",
       "      <td>4118.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.693</td>\n",
       "      <td>4.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>343.000</td>\n",
       "      <td>5212.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Transfer rate  Jumpstart  Asympotic performace  Time to threshold  \\\n",
       "count         10.000     10.000                10.000             10.000   \n",
       "mean           0.436      0.600                 0.000            190.900   \n",
       "std            0.205      2.503                 0.000            106.820   \n",
       "min            0.154     -4.000                 0.000             59.000   \n",
       "25%            0.279      0.000                 0.000            102.500   \n",
       "50%            0.439      0.000                 0.000            181.500   \n",
       "75%            0.632      2.000                 0.000            287.750   \n",
       "max            0.693      4.000                 0.000            343.000   \n",
       "\n",
       "       Accumulated rewards  \n",
       "count               10.000  \n",
       "mean              2839.400  \n",
       "std               1560.574  \n",
       "min                322.000  \n",
       "25%               1780.000  \n",
       "50%               2611.000  \n",
       "75%               4118.000  \n",
       "max               5212.000  "
      ]
     },
     "execution_count": 597,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# caps-Far transfer\n",
    "res = get_results(qlearn_rewards_v003,caps_rewards_v001_003)\n",
    "res = pd.DataFrame(res)\n",
    "res.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### OC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Transfer Setting|**Transfer Rate**|**Jumpstart(100 epsidoe)**|**Asymptotic Performance**|**Time to Threshold**|**Accumulated Rewards**|\n",
    "|---|---|---|---|---|---|\n",
    "|***Close transfer***|p =0.000|p =0.000|p =0.000|p =0.000|p =0.000|\n",
    "|Q-table|0.010$\\pm$0.539|8.400$\\pm$3.978|0.000$\\pm$0.000|24.800$\\pm$228.083|1310.200$\\pm$1841.795|\n",
    "|PRQL|**0.910**$\\pm$0.016|**17.600**$\\pm$1.578|0.000$\\pm$0.000|**385.800**$\\pm$59.243|**5854.600**$\\pm$829.821|\n",
    "|OPS-TL|0.883$\\pm$0.032|17.400$\\pm$1.647|0.000$\\pm$0.000|375.000$\\pm$63.821|5682.600$\\pm$904.087|\n",
    "|caps|0.904$\\pm$0.021|**17.600**$\\pm$1.578|0.000$\\pm$0.000|383.400$\\pm$61.446|5813.600$\\pm$845.584|\n",
    "|***Far transfer***|\n",
    "|Q-table|-4.08$\\pm$0.234|-2.400$\\pm$1.578|-13.800$\\pm$3.190|-inf$\\pm$nan|-68990.800$\\pm$2596.006|\n",
    "|PRQL|0.397$\\pm$0.105|1.000$\\pm$2.160|0.000$\\pm$0.000|171.000$\\pm$66.483|2371.000$\\pm$1034.359|\n",
    "|OPS-TL|**0.469**$\\pm$0.142|4.600$\\pm$5.502|0.000$\\pm$0.000|**200.800**$\\pm$69.723|**2892.600**$\\pm$976.823|\n",
    "|caps|0.436$\\pm$0.205|**2.400**$\\pm$3.627|0.000$\\pm$0.000|190.900$\\pm$106.820|2839.400$\\pm$1560.574|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qlearn\n",
      "Two-sample t-statistic D = 324.466, p-value = 0.0000\n",
      "pr\n",
      "Two-sample t-statistic D = 14.203, p-value = 0.0000\n",
      "ops\n",
      "Two-sample t-statistic D = 12.281, p-value = 0.0000\n",
      "caps\n",
      "Two-sample t-statistic D = 13.631, p-value = 0.0000\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "import scipy.optimize as opt\n",
    "algs = ['qlearn','pr','ops','caps']\n",
    "level=['Close_transfer_', 'Far_transfer_']\n",
    "   \n",
    "for alg in algs:\n",
    "    print(alg)\n",
    "    stat_val, p_val = stats.ttest_ind(np.mean(all_data[level[0]+alg],axis=0), np.mean(all_data[level[1]+alg],axis=0), equal_var=False)\n",
    "    #看看两个分布在均值上有没有显著差异\n",
    "    #注意，这里我们生成的第二组数据样本大小、方差和第一组均不相等，在运用t检验时需要使用Welch's t-test\n",
    "    #即指定ttest_ind中的equal_var=False。\n",
    "    print ('Two-sample t-statistic D = %6.3f, p-value = %6.4f' % (stat_val, p_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 799800\n",
      "Training finished\n",
      "CPU times: user 14min 56s, sys: 3min 6s, total: 18min 2s\n",
      "Wall time: 16min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "option_policies, option_terminations, policy_over_options, nrewards,critic = [ [None]*4 for i in range(5) ] \n",
    "env_list = [ \"Taxi-v001\", \"Taxi-v003\", \"Taxi-v004\"]\n",
    "\n",
    "for i in range(len(env_list)):\n",
    "    _, _ ,nrewards[i] ,option_policies[i], option_terminations[i], policy_over_options[i],critic[i] = oc(env = gym.make(env_list[i]),episode_num = 4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 799800\n",
      "Training finished\n",
      "CPU times: user 3h 23min 1s, sys: 45min 12s, total: 4h 8min 13s\n",
      "Wall time: 3h 42min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "oc_rewards_v001_003, oc_rewards_v004_003 = np.zeros([10,4000]), np.zeros([10,4000])\n",
    "for i in range(10):\n",
    "    _, _ ,oc_rewards_v001_003[i] ,_, _, _,_ = oc(env = gym.make( \"Taxi-v003\"),episode_num = 4000,option_policies_lib=option_policies[0],option_terminations_lib =option_terminations[0] )\n",
    "    _, _ ,oc_rewards_v004_003[i] ,_, _, _,_ = oc(env = gym.make(\"Taxi-v003\"),episode_num = 4000,option_policies_lib=option_policies[2],option_terminations_lib =option_terminations[2] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200,)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nrewards[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Close_transfer_\n",
      "qlearn\n",
      "|-0.028$\\pm$0.532|10.200$\\pm$3.824|0.000$\\pm$0.000|-3.700$\\pm$196.410|1141.000$\\pm$1593.943|\n",
      "pr\n",
      "|0.904$\\pm$0.018|-0.400$\\pm$1.265|0.000$\\pm$0.000|357.300$\\pm$50.945|5685.400$\\pm$842.672|\n",
      "ops\n",
      "|0.877$\\pm$0.027|0.600$\\pm$1.647|0.000$\\pm$0.000|346.500$\\pm$48.665|5513.400$\\pm$832.137|\n",
      "caps\n",
      "|0.898$\\pm$0.018|0.000$\\pm$1.333|0.000$\\pm$0.000|354.900$\\pm$48.521|5644.400$\\pm$804.308|\n",
      "Far_transfer_\n",
      "qlearn\n",
      "|-inf$\\pm$nan|-1.000$\\pm$1.414|-13.800$\\pm$3.190|-inf$\\pm$nan|-69160.000$\\pm$2149.471|\n",
      "pr\n",
      "|0.352$\\pm$0.147|-0.200$\\pm$1.476|0.000$\\pm$0.000|142.500$\\pm$63.568|2201.800$\\pm$869.542|\n",
      "ops\n",
      "|0.428$\\pm$0.162|-0.600$\\pm$1.350|0.000$\\pm$0.000|172.300$\\pm$79.195|2723.400$\\pm$1115.657|\n",
      "caps\n",
      "|0.404$\\pm$0.198|0.600$\\pm$2.675|0.000$\\pm$0.000|162.400$\\pm$81.606|2670.200$\\pm$1324.553|\n"
     ]
    }
   ],
   "source": [
    "algs = ['qlearn','pr','ops','caps']\n",
    "for level in ['Close_transfer_', 'Far_transfer_']:\n",
    "    print(level)\n",
    "    for alg in algs:\n",
    "        print(alg)\n",
    "        res = get_results(qlearn_rewards_v003,all_data[level+alg])\n",
    "        res = pd.DataFrame(res)\n",
    "        for i in range(len(res.std())):\n",
    "            print(\"|{:.3f}$\\pm${:.3f}\".format(round(res.mean()[i],3),round(res.std()[i],3)),end='')\n",
    "        print(\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Separated figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "plt.rcParams['savefig.dpi'] = 600\n",
    "algs = [\"qlearn\", \"pr\", \"ops\", \"caps\"]\n",
    "data =[None] *len(algs)\n",
    "for i,alg in enumerate(algs):\n",
    "    data[i] = cache(ex = \"similarity_diff\", alg = alg)\n",
    "    plot_result(data[i],4000,smooth_radius=20)\n",
    "    plt.savefig(fname = './combined_figure_similarity_diff_'+ alg ,dpi = 600)\n",
    "    plot_result(data[i],4000,combine_figures =False,average_group=False,figsize=(9,12),smooth_radius=20) \n",
    "    plt.savefig(fname = './separated_figure_similarity_diff_'+ alg ,dpi = 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### combined figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cache(ex = \"similarity_diff\", tl_alg = \"qlearn\", rl_alg = '', all_data = None):\n",
    "    \n",
    "    if os.path.exists(\"./\"+ time.strftime(\"%Y%m%d\", time.localtime())) is not True:\n",
    "        os.mkdir(\"./\"+ time.strftime(\"%Y%m%d\", time.localtime()) )\n",
    "    if all_data is None:\n",
    "        data = {\"No_transfer_\" + tl_alg: eval(rl_alg +\"_rewards_v003\"),\n",
    "                \"Far_transfer_\"+tl_alg: eval(tl_alg+'_'+ rl_alg +\"_rewards_v001_003\"),\n",
    "                \"Close_transfer_\"+tl_alg: eval(tl_alg+'_'+ rl_alg +\"_rewards_v004_003\")\n",
    "               }\n",
    "        path = \"./\" + time.strftime(\"%Y%m%d\", time.localtime()) +'/' + ex + '_' + tl_alg + '_'  + rl_alg \n",
    "        saveData(data,path)\n",
    "    else:\n",
    "        data = all_data\n",
    "        path = \"./\"+ time.strftime(\"%Y%m%d\", time.localtime()) +'/'  + ex + '_' +  'alldata_'  + rl_alg \n",
    "        saveData(data,path)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Close_transfer_caps': array([[ 0.,  0.,  6., ..., 20., 20., 20.],\n",
       "        [ 0.,  0.,  4., ..., 20., 20., 20.],\n",
       "        [ 0.,  0.,  0., ..., 20., 20., 20.],\n",
       "        ...,\n",
       "        [ 2.,  2.,  6., ..., 20., 20., 20.],\n",
       "        [ 4.,  0.,  2., ..., 20., 20., 20.],\n",
       "        [ 0.,  8.,  8., ..., 20., 20., 20.]]),\n",
       " 'Close_transfer_ops': array([[ 0.,  2.,  4., ..., 20., 20., 20.],\n",
       "        [ 0.,  0.,  0., ..., 20., 20., 20.],\n",
       "        [ 0.,  0.,  4., ..., 20., 20., 20.],\n",
       "        ...,\n",
       "        [ 0.,  0.,  0., ..., 20., 20., 20.],\n",
       "        [ 0.,  0.,  2., ..., 20., 20., 20.],\n",
       "        [ 0.,  4.,  0., ..., 20., 20., 20.]]),\n",
       " 'Close_transfer_prql': array([[ 0.,  0.,  0., ..., 20., 20., 20.],\n",
       "        [ 0.,  2.,  0., ..., 20., 20., 20.],\n",
       "        [ 4.,  2.,  2., ..., 20., 20., 20.],\n",
       "        ...,\n",
       "        [ 0.,  0.,  0., ..., 20., 20., 20.],\n",
       "        [ 0.,  0.,  0., ..., 20., 20., 20.],\n",
       "        [ 0.,  2.,  6., ..., 20., 20., 20.]]),\n",
       " 'Far_transfer_caps': array([[ 2.,  2.,  4., ..., 20., 20., 20.],\n",
       "        [ 2.,  0.,  2., ..., 20., 20., 20.],\n",
       "        [ 6.,  2.,  4., ..., 20., 20., 20.],\n",
       "        ...,\n",
       "        [ 4.,  0.,  2., ..., 20., 20., 20.],\n",
       "        [ 0.,  2.,  0., ..., 20., 20., 20.],\n",
       "        [ 0.,  0.,  4., ..., 20., 20., 20.]]),\n",
       " 'Far_transfer_ops': array([[ 2.,  2.,  0., ..., 20., 20., 20.],\n",
       "        [ 2.,  0.,  0., ..., 20., 20., 20.],\n",
       "        [ 0.,  0.,  0., ..., 20., 20., 20.],\n",
       "        ...,\n",
       "        [ 0.,  0.,  8., ..., 20., 20., 20.],\n",
       "        [ 2.,  4.,  4., ..., 20., 20., 20.],\n",
       "        [ 0.,  8.,  2., ..., 20., 20., 20.]]),\n",
       " 'Far_transfer_prql': array([[ 0.,  4.,  0., ..., 20., 20., 20.],\n",
       "        [ 0.,  0.,  4., ..., 20., 20., 20.],\n",
       "        [ 2.,  4.,  0., ..., 20., 20., 20.],\n",
       "        ...,\n",
       "        [ 0.,  2.,  0., ..., 20., 20., 20.],\n",
       "        [ 0.,  0.,  0., ..., 20., 20., 20.],\n",
       "        [ 0.,  0.,  6., ..., 20., 20., 20.]]),\n",
       " 'No_transfer_caps': array([[ 0.,  2.,  0., ..., 20., 20., 20.],\n",
       "        [ 2.,  0.,  0., ..., 20., 20., 20.],\n",
       "        [ 0.,  2.,  0., ..., 20., 20., 20.],\n",
       "        ...,\n",
       "        [ 2.,  0.,  0., ..., 20., 20., 20.],\n",
       "        [ 0.,  0.,  2., ..., 20., 20., 20.],\n",
       "        [ 0.,  0.,  2., ..., 20., 20., 20.]]),\n",
       " 'No_transfer_ops': array([[ 0.,  2.,  0., ..., 20., 20., 20.],\n",
       "        [ 2.,  0.,  0., ..., 20., 20., 20.],\n",
       "        [ 0.,  2.,  0., ..., 20., 20., 20.],\n",
       "        ...,\n",
       "        [ 2.,  0.,  0., ..., 20., 20., 20.],\n",
       "        [ 0.,  0.,  2., ..., 20., 20., 20.],\n",
       "        [ 0.,  0.,  2., ..., 20., 20., 20.]]),\n",
       " 'No_transfer_prql': array([[ 0.,  2.,  0., ..., 20., 20., 20.],\n",
       "        [ 2.,  0.,  0., ..., 20., 20., 20.],\n",
       "        [ 0.,  2.,  0., ..., 20., 20., 20.],\n",
       "        ...,\n",
       "        [ 2.,  0.,  0., ..., 20., 20., 20.],\n",
       "        [ 0.,  0.,  2., ..., 20., 20., 20.],\n",
       "        [ 0.,  0.,  2., ..., 20., 20., 20.]])}"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache(all_data = all_data,rl_alg ='sarsa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Close_transfer_caps': array([[ 0.,  0.,  4., ..., 20., 20., 20.],\n",
       "        [ 0.,  0.,  2., ..., 20., 20., 20.],\n",
       "        [ 0.,  0.,  2., ..., 20., 20., 20.],\n",
       "        ...,\n",
       "        [ 0.,  0.,  0., ..., 20., 20., 20.],\n",
       "        [ 2.,  0.,  0., ..., 20., 20., 20.],\n",
       "        [ 2.,  2.,  0., ..., 20., 20., 20.]]),\n",
       " 'Close_transfer_ops': array([[ 2.,  0.,  0., ..., 20., 20., 20.],\n",
       "        [ 0.,  0.,  0., ..., 20., 20., 20.],\n",
       "        [ 2.,  0.,  4., ..., 20., 20., 20.],\n",
       "        ...,\n",
       "        [ 2.,  0.,  0., ..., 20., 20., 20.],\n",
       "        [ 2.,  4.,  0., ..., 20., 20., 20.],\n",
       "        [ 4.,  2.,  0., ..., 20., 20., 20.]]),\n",
       " 'Close_transfer_pr': array([[ 0.,  4.,  0., ..., 20., 20., 20.],\n",
       "        [ 0.,  0.,  2., ..., 20., 20., 20.],\n",
       "        [ 0.,  0.,  0., ..., 20., 20., 20.],\n",
       "        ...,\n",
       "        [ 0.,  0.,  2., ..., 20., 20., 20.],\n",
       "        [ 0.,  2.,  2., ..., 20., 20., 20.],\n",
       "        [ 2.,  0.,  2., ..., 20., 20., 20.]]),\n",
       " 'Close_transfer_qlearn': array([[12., 18.,  8., ..., 20., 20., 20.],\n",
       "        [12., 10., 10., ..., 20., 20., 20.],\n",
       "        [10.,  8., 14., ..., 20., 20., 20.],\n",
       "        ...,\n",
       "        [14.,  4., 10., ..., 20., 20., 20.],\n",
       "        [16., 18., 10., ..., 20., 20., 20.],\n",
       "        [ 8., 10.,  6., ..., 20., 20., 20.]]),\n",
       " 'Far_transfer_caps': array([[ 2.,  0.,  0., ..., 20., 20., 20.],\n",
       "        [ 0.,  0.,  0., ..., 20., 20., 20.],\n",
       "        [ 0.,  2.,  0., ..., 20., 20., 20.],\n",
       "        ...,\n",
       "        [ 0.,  2.,  2., ..., 20., 20., 20.],\n",
       "        [ 2.,  0.,  0., ..., 20., 20., 20.],\n",
       "        [ 0.,  0.,  0., ..., 20., 20., 20.]]),\n",
       " 'Far_transfer_ops': array([[ 0.,  2.,  2., ..., 20., 20., 20.],\n",
       "        [ 0.,  0.,  0., ..., 20., 20., 20.],\n",
       "        [ 0.,  2.,  0., ..., 20., 20., 20.],\n",
       "        ...,\n",
       "        [ 0.,  2.,  0., ..., 20., 20., 20.],\n",
       "        [ 0.,  2.,  2., ..., 20., 20., 20.],\n",
       "        [ 0.,  2.,  2., ..., 20., 20., 20.]]),\n",
       " 'Far_transfer_pr': array([[ 0.,  2.,  2., ..., 20., 20., 20.],\n",
       "        [ 0.,  2.,  2., ..., 20., 20., 20.],\n",
       "        [ 0.,  2.,  0., ..., 20., 20., 20.],\n",
       "        ...,\n",
       "        [ 2.,  2.,  0., ..., 20., 20., 20.],\n",
       "        [ 0.,  0.,  2., ..., 20., 20., 20.],\n",
       "        [ 2.,  2.,  0., ..., 20., 20., 20.]]),\n",
       " 'Far_transfer_qlearn': array([[ 0.,  0.,  0., ...,  4.,  4.,  4.],\n",
       "        [ 0.,  0.,  0., ...,  6.,  6.,  2.],\n",
       "        [ 0.,  0.,  0., ...,  8.,  0.,  6.],\n",
       "        ...,\n",
       "        [ 0.,  0.,  0., ...,  4.,  2., 10.],\n",
       "        [ 0.,  0.,  0., ...,  4., 10.,  4.],\n",
       "        [ 0.,  0.,  0., ...,  2.,  6., 10.]]),\n",
       " 'No_transfer_qlearn': array([[ 0.,  0.,  0., ..., 20., 20., 20.],\n",
       "        [ 4.,  0.,  0., ..., 20., 20., 20.],\n",
       "        [ 0.,  2.,  0., ..., 20., 20., 20.],\n",
       "        ...,\n",
       "        [ 0.,  0.,  0., ..., 20., 20., 20.],\n",
       "        [ 0.,  2.,  0., ..., 20., 20., 20.],\n",
       "        [ 2.,  4.,  0., ..., 20., 20., 20.]])}"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algs = ['qlearn','pr','ops','caps']\n",
    "data = [None] * len(algs)\n",
    "all_data = {}\n",
    "for i,alg in enumerate(algs):\n",
    "    data[i] = loadData('./' + \"similarity_diff\" + '_' + alg + '_'  + '20210429' )\n",
    "    all_data.update(data[i])\n",
    "all_data    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "algs = ['prql','ops','caps']\n",
    "data = [None] * len(algs)\n",
    "all_data = {}\n",
    "for i,alg in enumerate(algs):\n",
    "    data[i] = cache(tl_alg =alg,rl_alg ='sarsa')\n",
    "    all_data.update(data[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['qlearn', 'pr', 'ops', 'caps']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11b0ee2ba42544f4a8861e8b79fc5bc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['qlearn', 'pr', 'ops', 'caps']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9923a90c8a8440049cf293f6c91b37d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plot_result(all_data,4000,title = \"3 similarity + 4 algs : 4000 episode-rewareds graph\")\n",
    "#plt.savefig('./3similarity+4algs:4000episode',dpi =  600)\n",
    "plot_result(all_data,1000,title = \"3 similarity + 4 algs : 1000 episode-rewareds graph\", show_std= False ,smooth_radius = 20)\n",
    "#plt.savefig('./3similarity+4algs:1000episode',dpi =  600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Exp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 155800\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-abc5b5252d29>\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(target_rl_algo, episode_num, repeat_times, target_task, source_task, source_rl_algo, policy_library)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mepisodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpenalties\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mknowledge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_rl_algo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-c83c33ebef2c>\u001b[0m in \u001b[0;36mq_learning\u001b[0;34m(env, train_episodes, init_q_table)\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m200\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0;31m#evaluate policy for learning curve after each episode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                 \u001b[0mepisode_rewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_penalties\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_copy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_table\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEVALUATION_TIMES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0mall_episodes_length\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-4d8b12d466fa>\u001b[0m in \u001b[0;36mpolicy_evaluate\u001b[0;34m(env, policy, times)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAX_EPISODE_LEN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg_max\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;31m#update data for learning curve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gym/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gym/gym/envs/toy_text/discrete.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mtransitions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcategorical_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransitions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtransitions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gym/gym/envs/toy_text/discrete.py\u001b[0m in \u001b[0;36mcategorical_sample\u001b[0;34m(prob_n, np_random)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprob_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mcsprob_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcsprob_n\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mnp_random\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# training Taxi-v3 from scratch\n",
    "qlearn_episodes_length_v3, qlearn_penalties_v3, qlearn_rewards_v3, _, qlearn_q_table_v3 = transform(tl_algo = q_learning,\n",
    "                                                                                     episode_num = EPISODE_NUM, \n",
    "                                                                                     repeat_times = REPEAT_TIMES,\n",
    "                                                                                     target_task = \"Taxi-v3\",\n",
    "                                                                                     source_task = None\n",
    "                                                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 799800\n",
      "Training finished\n",
      "CPU times: user 8min 48s, sys: 1min 21s, total: 10min 9s\n",
      "Wall time: 9min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# training Taxi-v1 from scratch\n",
    "qlearn_episodes_length_v1, qlearn_penalties_v1, qlearn_rewards_v1,_, qlearn_q_table_v1 = transform(tl_algo = q_learning,\n",
    "                                                                                     episode_num = EPISODE_NUM, \n",
    "                                                                                     repeat_times = REPEAT_TIMES,\n",
    "                                                                                     target_task = \"Taxi-v1\",\n",
    "                                                                                     source_task = None\n",
    "                                                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 799800\n",
      "Training finished\n",
      "CPU times: user 9min 6s, sys: 1min 13s, total: 10min 19s\n",
      "Wall time: 9min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# training Taxi-v4 from scratch\n",
    "qlearn_episodes_length_v4, qlearn_penalties_v4, qlearn_rewards_v4,_, qlearn_q_table_v4 = transform(tl_algo = q_learning,\n",
    "                                                                                     episode_num = EPISODE_NUM, \n",
    "                                                                                     repeat_times = REPEAT_TIMES,\n",
    "                                                                                     target_task = \"Taxi-v4\",\n",
    "                                                                                     source_task = None\n",
    "                                                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 799800\n",
      "Training finished\n",
      "CPU times: user 9min 5s, sys: 1min 23s, total: 10min 28s\n",
      "Wall time: 9min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# training Taxi-v03 from scratch\n",
    "qlearn_episodes_length_v03, qlearn_penalties_v03, qlearn_rewards_v03, _, qlearn_q_table_v03 = transform(tl_algo = q_learning,\n",
    "                                                                                     episode_num = EPISODE_NUM, \n",
    "                                                                                     repeat_times = REPEAT_TIMES,\n",
    "                                                                                     target_task = \"Taxi-v03\",\n",
    "                                                                                     source_task = None\n",
    "                                                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 799800\n",
      "Training finished\n",
      "CPU times: user 8min 48s, sys: 1min 21s, total: 10min 9s\n",
      "Wall time: 9min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# training Taxi-v01 from scratch\n",
    "qlearn_episodes_length_v01, qlearn_penalties_v01, qlearn_rewards_v01,_, qlearn_q_table_v01 = transform(tl_algo = q_learning,\n",
    "                                                                                     episode_num = EPISODE_NUM, \n",
    "                                                                                     repeat_times = REPEAT_TIMES,\n",
    "                                                                                     target_task = \"Taxi-v01\",\n",
    "                                                                                     source_task = None\n",
    "                                                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 799800\n",
      "Training finished\n",
      "CPU times: user 9min 6s, sys: 1min 13s, total: 10min 19s\n",
      "Wall time: 9min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# training Taxi-v04 from scratch\n",
    "qlearn_episodes_length_v04, qlearn_penalties_v04, qlearn_rewards_v04,_, qlearn_q_table_v04 = transform(tl_algo = q_learning,\n",
    "                                                                                     episode_num = EPISODE_NUM, \n",
    "                                                                                     repeat_times = REPEAT_TIMES,\n",
    "                                                                                     target_task = \"Taxi-v04\",\n",
    "                                                                                     source_task = None\n",
    "                                                                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Exp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 154400\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-9fc5cc68f80b>\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(target_rl_algo, episode_num, repeat_times, target_task, source_task, source_rl_algo, policy_library)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mpast_policies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_library\u001b[0m \u001b[0;31m# policy_library = [ [[policy11][policy21]...] [[policy12][policy22]...]...]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtarget_rl_algo\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mCAPS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m                 \u001b[0mepisodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpenalties\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfrequency\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mknowledge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_rl_algo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_episodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast_policies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mepisodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpenalties\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mknowledge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_rl_algo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_episodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast_policies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-0307c0ff7df1>\u001b[0m in \u001b[0;36mCAPS\u001b[0;34m(env, train_episodes, rng, past_policies)\u001b[0m\n\u001b[1;32m     53\u001b[0m             episode_penalties, episode_rewards, episode_length,i = policy_reuse(timesteps, env, q_table, state, i,\n\u001b[1;32m     54\u001b[0m                                                                               \u001b[0mall_episodes_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mall_penalties\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mall_rewards\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m                                                                               past_policy = past_policies[j])\n\u001b[0m\u001b[1;32m     56\u001b[0m             \u001b[0mpolicy_expect\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpolicy_expect\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpolicy_times\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mepisode_rewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mpolicy_times\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mpolicy_times\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-5d9eb6ae084e>\u001b[0m in \u001b[0;36mpolicy_reuse\u001b[0;34m(timesteps, env, q_table, initial_state, i, all_episodes_length, all_penalties, all_rewards, past_policy, fi, mu, alpha, gamma)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# update q-value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mq_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mq_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mq_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mamax\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spinningup/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mamax\u001b[0;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2704\u001b[0m     \"\"\"\n\u001b[1;32m   2705\u001b[0m     return _wrapreduction(a, np.maximum, 'max', axis, None, out,\n\u001b[0;32m-> 2706\u001b[0;31m                           keepdims=keepdims, initial=initial, where=where)\n\u001b[0m\u001b[1;32m   2707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spinningup/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#caps_v014_003\n",
    "caps_episodes_length_v014_003, caps_penalties_v014_003,caps_rewards_v014_003,caps_fre_v014_003, caps_q_table_v014_003 = transform(tl_algo= caps,\n",
    "                                                                                       episode_num = EPISODE_NUM,\n",
    "                                                                                       repeat_times = REPEAT_TIMES,\n",
    "                                                                                       target_task = \"Taxi-v003\",\n",
    "                                                                                      \n",
    "                                                                                       policy_library = [[q_func1,q_func2,q_func3 ] for q_func1,q_func2,q_func3 in zip(qlearn_q_table_v000, qlearn_q_table_v001, qlearn_q_table_v004)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 's_table' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-9fc5cc68f80b>\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(target_rl_algo, episode_num, repeat_times, target_task, source_task, source_rl_algo, policy_library)\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0mepisodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpenalties\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfrequency\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mknowledge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_rl_algo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_episodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast_policies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m                 \u001b[0mepisodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpenalties\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mknowledge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_rl_algo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_episodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast_policies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;31m#Situation 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0msource_task\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-720e1c574ecd>\u001b[0m in \u001b[0;36mprql\u001b[0;34m(env, train_episodes, rng, past_policy)\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m200\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                 \u001b[0;31m#evaluate policy for learning curve after each episode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m                 \u001b[0mepisode_rewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_penalties\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_copy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_table\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEVALUATION_TIMES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mall_episodes_length\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 's_table' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#pr_v014_003\n",
    "pr_episodes_length_v014_003, pr_penalties_v014_003,pr_rewards_v014_003, _, pr_q_table_v014_003 = transform(tl_algo= prql,\n",
    "                                                                                       episode_num = EPISODE_NUM,\n",
    "                                                                                       repeat_times = REPEAT_TIMES,\n",
    "                                                                                       target_task = \"Taxi-v003\",\n",
    "                                                                                                                                                                            policy_library =  qlearn_q_table_v004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 112000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-9fc5cc68f80b>\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(target_rl_algo, episode_num, repeat_times, target_task, source_task, source_rl_algo, policy_library)\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0mepisodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpenalties\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfrequency\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mknowledge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_rl_algo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_episodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast_policies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m                 \u001b[0mepisodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpenalties\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mknowledge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_rl_algo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_episodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast_policies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;31m#Situation 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0msource_task\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-50-0c97c6ab7cc8>\u001b[0m in \u001b[0;36mOPS_TL\u001b[0;34m(env, train_episodes, rng, past_policies)\u001b[0m\n\u001b[1;32m     50\u001b[0m             episode_penalties, episode_rewards, episode_length,i = policy_reuse(timesteps, env, q_table, state, i,\n\u001b[1;32m     51\u001b[0m                                                                               \u001b[0mall_episodes_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mall_penalties\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mall_rewards\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                                                                               past_policy = past_policies[j])\n\u001b[0m\u001b[1;32m     53\u001b[0m             \u001b[0mpolicy_expect\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpolicy_expect\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpolicy_times\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mepisode_rewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mpolicy_times\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mpolicy_times\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-5d9eb6ae084e>\u001b[0m in \u001b[0;36mpolicy_reuse\u001b[0;34m(timesteps, env, q_table, initial_state, i, all_episodes_length, all_penalties, all_rewards, past_policy, fi, mu, alpha, gamma)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mepisode_penalties\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_rewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0m_env_copy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtimesteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mfi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spinningup/lib/python3.6/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    178\u001b[0m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_reconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;31m# If is its own copy, don't memoize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spinningup/lib/python3.6/copy.py\u001b[0m in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdeep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__setstate__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spinningup/lib/python3.6/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spinningup/lib/python3.6/copy.py\u001b[0m in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0mmemo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spinningup/lib/python3.6/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    178\u001b[0m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_reconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;31m# If is its own copy, don't memoize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spinningup/lib/python3.6/copy.py\u001b[0m in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdeep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__setstate__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spinningup/lib/python3.6/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spinningup/lib/python3.6/copy.py\u001b[0m in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0mmemo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spinningup/lib/python3.6/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spinningup/lib/python3.6/copy.py\u001b[0m in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0mmemo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spinningup/lib/python3.6/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spinningup/lib/python3.6/copy.py\u001b[0m in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0mmemo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spinningup/lib/python3.6/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spinningup/lib/python3.6/copy.py\u001b[0m in \u001b[0;36m_deepcopy_list\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0mmemo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0mappend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m         \u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#ops_v014_003\n",
    "tl_episodes_length_v014_003, tl_penalties_v014_003,tl_rewards_v014_003,tl_fre_v014_003, tl_q_table_v014_003 = transform(tl_algo= OPS_TL,\n",
    "                                                                                       episode_num = EPISODE_NUM,\n",
    "                                                                                       repeat_times = REPEAT_TIMES,\n",
    "                                                                                       target_task = \"Taxi-v003\",\n",
    "                                                                                      \n",
    "                                                                                       policy_library = [[q_func1,q_func2,q_func3 ] for q_func1,q_func2,q_func3 in zip(qlearn_q_table_v000, qlearn_q_table_v001, qlearn_q_table_v004)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "taxiEnvs = [gym.make(\"Taxi-v000\"),gym.make(\"Taxi-v001\"),None,gym.make(\"Taxi-v003\"),gym.make(\"Taxi-v004\")]\n",
    "for i in [0,1,3,4]:\n",
    "    _, _ ,nrewards[i] ,option_policies[i], option_terminations[i], policy_over_options[i],critic[i]= oc( taxiEnvs[i], 4000, noptions=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "source_lib = [0,1,4]\n",
    "_,_, trans_rewards_012, _, _, _,_= oc( four_room_envs[3], 4000, [option_policies[i][0] for i in source_lib], noptions=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(run_times, q_table = None ,task = \"Taxi-v003\", mode = 'learning agent', show_frame = True):\n",
    "    \"\"\"Evaluate agent's performance after policy updates\"\"\"\n",
    "    env = gym.make(task)\n",
    "    if q_table is None:\n",
    "        list_length = 1\n",
    "    else:\n",
    "        list_length = len(q_table)\n",
    "    all_epochs, all_penalties, all_rewards, all_success_rate =  np.zeros(list_length), np.zeros(list_length),np.zeros(list_length),np.zeros(list_length)\n",
    "    all_epochs_std, all_penalties_std, all_rewards_std = np.zeros(list_length), np.zeros(list_length),np.zeros(list_length)\n",
    "    \n",
    "    \n",
    "        \n",
    "    for i in range( list_length ):\n",
    "        table_epochs, table_penalties, table_rewards = np.zeros(run_times), np.zeros(run_times),np.zeros(run_times)\n",
    "        if q_table is not None:\n",
    "            table = q_table[i]\n",
    "        success = 0\n",
    "        for j in range(run_times):\n",
    "            state = env.reset()\n",
    "            epochs, penalties, reward, sum_reward = 0, 0, 0, 0\n",
    "\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "                if mode == 'learning agent':\n",
    "                    action = arg_max(table[state])\n",
    "                elif mode == 'random agent':\n",
    "                    action = np.random.randint(0,env.action_space.n)\n",
    "                state, reward, done, info = env.step(action)\n",
    "\n",
    "                if reward == -10:\n",
    "                    penalties += 1\n",
    "                elif reward == 20:\n",
    "                    success += 1\n",
    "\n",
    "                epochs += 1\n",
    "                sum_reward += reward\n",
    "                if show_frame:\n",
    "                    show_frames(env,j, epochs,sum_reward )\n",
    "            table_penalties[j] = penalties\n",
    "            table_epochs[j] = epochs\n",
    "            table_rewards[j] = sum_reward\n",
    "\n",
    "        all_success_rate[i] = success/run_times    \n",
    "        all_epochs[i], all_penalties[i], all_rewards[i]  = np.mean(table_epochs), np.mean(table_penalties), np.mean(table_rewards)\n",
    "        all_epochs_std[i], all_penalties_std[i], all_rewards_std[i] = np.std(table_epochs), np.std(table_penalties), np.std(table_rewards)\n",
    "      \n",
    "    print(f\"Results over {run_times} evaluating episodes:\")\n",
    "    print(f\"Success rate : {np.mean(all_success_rate)}\")\n",
    "    print(f\"Average  episode length : {np.mean(all_epochs)} ± {np.mean(all_epochs_std)}\")\n",
    "    print(f\"Average penalties per episode: {np.mean(all_penalties)} ± {np.mean(all_penalties_std)}\")\n",
    "    print(f\"Average rewards per episode: {np.mean(all_rewards)} ± {np.mean(all_rewards_std)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def op_evaluate(run_times,option_policies, option_terminations, policy_over_options, q_table = None ,task = \"Taxi-v003\", mode = 'learning agent'):\n",
    "    \"\"\"Evaluate agent's performance after policy updates\"\"\"\n",
    "    env = gym.make(task)\n",
    "    if q_table is None:\n",
    "        list_length = 1\n",
    "    else:\n",
    "        list_length = len(q_table)\n",
    "    all_epochs, all_penalties, all_rewards, all_success_rate =  np.zeros(list_length), np.zeros(list_length),np.zeros(list_length),np.zeros(list_length)\n",
    "    all_epochs_std, all_penalties_std, all_rewards_std = np.zeros(list_length), np.zeros(list_length),np.zeros(list_length)\n",
    "    \n",
    "    \n",
    "        \n",
    "    for i in range( list_length ):\n",
    "        table_epochs, table_penalties, table_rewards = np.zeros(run_times), np.zeros(run_times),np.zeros(run_times)\n",
    "        if q_table is not None:\n",
    "            table = q_table[i]\n",
    "        success = 0\n",
    "        for j in range(run_times):\n",
    "            state = env.reset()\n",
    "            epochs, penalties, reward, sum_reward = 0, 0, 0, 0\n",
    "\n",
    "            done = False\n",
    "            option = policy_over_options.evaluate(state)\n",
    "            while not done:\n",
    "                if mode == 'learning agent':\n",
    "                    if option_terminations[option].sample(state):\n",
    "                        option = policy_over_options.evaluate(state)\n",
    "\n",
    "                    action = option_policies[option].evaluate(state)\n",
    "                    \n",
    "                    \n",
    "                elif mode == 'random agent':\n",
    "                    action = np.random.randint(0,env.action_space.n)\n",
    "                state, reward, done, info = env.step(action)\n",
    "\n",
    "                if reward == -10:\n",
    "                    penalties += 1\n",
    "                elif reward == 20:\n",
    "                    success += 1\n",
    "\n",
    "                epochs += 1\n",
    "                sum_reward += reward\n",
    "                show_frames(env,j,epochs,sum_reward )\n",
    "            table_penalties[j] = penalties\n",
    "            table_epochs[j] = epochs\n",
    "            table_rewards[j] = sum_reward\n",
    "\n",
    "        all_success_rate[i] = success/run_times    \n",
    "        all_epochs[i], all_penalties[i], all_rewards[i]  = np.mean(table_epochs), np.mean(table_penalties), np.mean(table_rewards)\n",
    "        all_epochs_std[i], all_penalties_std[i], all_rewards_std[i] = np.std(table_epochs), np.std(table_penalties), np.std(table_rewards)\n",
    "      \n",
    "    print(f\"Results after {run_times} runs:\")\n",
    "    print(f\"Success rate : {np.mean(all_success_rate)}\")\n",
    "    print(f\"Average  episode length : {np.mean(all_epochs)} ± {np.mean(all_epochs_std)}\")\n",
    "    print(f\"Average penalties per episode: {np.mean(all_penalties)} ± {np.mean(all_penalties_std)}\")\n",
    "    print(f\"Average rewards per episode: {np.mean(all_rewards)} ± {np.mean(all_rewards_std)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 79800\n",
      "Training finished\n",
      "CPU times: user 12.3 s, sys: 675 ms, total: 13 s\n",
      "Wall time: 12.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#ops_v0_003\n",
    "ops_episodes_length_v0_003, ops_penalties_v0_003,ops_rewards_v0_003,ops_fre_v0_003, ops_q_table_v0_003 = transform(tl_algo= caps,\n",
    "                                                                                       episode_num = EPISODE_NUM//10,\n",
    "                                                                                       repeat_times = REPEAT_TIMES,\n",
    "                                                                                       target_task = \"Taxi-v003\",\n",
    "                                                                                      \n",
    "                                                                                       policy_library = [[q_func1] for q_func1 in qlearn_q_table_v000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 79800\n",
      "Training finished\n",
      "CPU times: user 13 s, sys: 527 ms, total: 13.5 s\n",
      "Wall time: 13.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#ops_v1_003\n",
    "ops_episodes_length_v1_003, ops_penalties_v1_003,ops_rewards_v1_003,ops_fre_v1_003, ops_q_table_v1_003 = transform(tl_algo= caps,\n",
    "                                                                                       episode_num = EPISODE_NUM//10,\n",
    "                                                                                       repeat_times = REPEAT_TIMES,\n",
    "                                                                                       target_task = \"Taxi-v003\",\n",
    "                                                                                      \n",
    "                                                                                       policy_library = [[q_func1] for q_func1 in qlearn_q_table_v001])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 399\n",
      "Training finished \n",
      "\n",
      "CPU times: user 2.21 s, sys: 106 µs, total: 2.21 s\n",
      "Wall time: 2.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pr_episodes_length_v1_003, pr_penalties_v1_003,pr_rewards_v1_003, _, pr_q_table_v1_003 = transform(tl_algo= prql,\n",
    "                                                                                       episode_num = EPISODE_NUM//10,\n",
    "                                                                                       repeat_times = REPEAT_TIMES,\n",
    "                                                                                       target_task = \"Taxi-v003\",\n",
    "                                                                                       \n",
    "                                                                                       policy_library =  qlearn_q_table_v001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 79800\n",
      "Training finished\n",
      "CPU times: user 3.7 s, sys: 455 ms, total: 4.15 s\n",
      "Wall time: 3.81 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "qlearn_episodes_length_v3, qlearn_penalties_v3, qlearn_rewards_v3, _, qlearn_q_table_v3 = transform(tl_algo = q_learning,\n",
    "                                                                                     episode_num = EPISODE_NUM//10, \n",
    "                                                                                     repeat_times = 1,\n",
    "                                                                                     target_task = \"Taxi-v003\",\n",
    "                                                                                     source_task = None\n",
    "                                                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56d5b099cde549529106f6714c0b7732",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show>"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "RATIO = 10\n",
    "SMOOTH_RADIUS = 20\n",
    "plt.plot(range(EPISODE_NUM//RATIO),smooth(qlearn_rewards_v3[:EPISODE_NUM//RATIO],SMOOTH_RADIUS), label = 'qlearn_v3')\n",
    "\n",
    "plt.plot(range(EPISODE_NUM//RATIO),smooth(pr_rewards_v1_003[:EPISODE_NUM//RATIO],SMOOTH_RADIUS), label = 'pr_v1_3')\n",
    "plt.plot(range(EPISODE_NUM//RATIO),smooth(ops_rewards_v1_003[:EPISODE_NUM//RATIO],SMOOTH_RADIUS), label = 'ops_v1_3')\n",
    "\n",
    "plt.title(\"episode-penalties graph\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"penalties\")\n",
    "plt.legend()\n",
    "plt.show\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results over 1000 evaluating episodes:\n",
      "Success rate : 0.779\n",
      "Average  episode length : 84.576 ± 75.92020958875179\n",
      "Average penalties per episode: 0.0 ± 0.0\n",
      "Average rewards per episode: 15.58 ± 8.298409486160585\n"
     ]
    }
   ],
   "source": [
    "evaluate(1000,qlearn_q_table_v3,show_frame = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "episode 3  step 6  rewards=20\n",
      "Results over 3 evaluating episodes:\n",
      "Success rate : 1.0\n",
      "Average  episode length : 17.0 ± 9.41629792788369\n",
      "Average penalties per episode: 0.0 ± 0.0\n",
      "Average rewards per episode: 20.0 ± 0.0\n"
     ]
    }
   ],
   "source": [
    "evaluate(3,ops_q_table_v1_003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results over 1000 evaluating episodes:\n",
      "Success rate : 0.988\n",
      "Average  episode length : 18.847 ± 25.006750908504685\n",
      "Average penalties per episode: 0.0 ± 0.0\n",
      "Average rewards per episode: 19.76 ± 2.1777052142105915\n"
     ]
    }
   ],
   "source": [
    "evaluate(3,pr_q_table_v1_003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results over 1000 evaluating episodes:\n",
      "Success rate : 0.706\n",
      "Average  episode length : 85.098 ± 82.33279052722554\n",
      "Average penalties per episode: 0.0 ± 0.0\n",
      "Average rewards per episode: 14.12 ± 9.111838453352869\n"
     ]
    }
   ],
   "source": [
    "evaluate(1000,pr_q_table_v1_003,show_frame = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(1000, mode = 'random agent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# training Taxi-v3 reusing policy from Taxi-v1\n",
    "pr1_rewards_v1_3 = np.zeros(10000)\n",
    "for i in range(10):\n",
    "    pr1_episodes_length_v1_3, pr1_penalties_v1_3,rewards, pr1_q_table_v1_3 = prql(gym.make(\"Taxi-v3\"), qlearn_q_table_v1[i], 10000)\n",
    "    pr1_rewards_v1_3 += rewards\n",
    "pr1_rewards_v1_3/=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# training Taxi-v3 reusing policy from Taxi-v1\n",
    "pr1_rewards_v4_3 = np.zeros(10000)\n",
    "for i in range(10):\n",
    "    pr1_episodes_length_v4_3, pr1_penalties_v4_3,rewards, pr1_q_table_v4_3 = prql(gym.make(\"Taxi-v3\"), qlearn_q_table_v4[i], 10000)\n",
    "    pr1_rewards_v4_3 += rewards\n",
    "pr1_rewards_v4_3/=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "env = gym.make(\"Taxi-v4\")\n",
    "# training Taxi-v4 from scratch\n",
    "lprofiler = LineProfiler(q_learning)\n",
    "lprofiler.run('qlearn_episodes_length_v41, qlearn_penalties_v41, qlearn_rewards_v41, qlearn_q_table_v41 = q_learning(env,np.zeros([env.observation_space.n,env.action_space.n]),EPISODE_NUM)')\n",
    "lprofiler.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spinningup",
   "language": "python",
   "name": "spinningup"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
