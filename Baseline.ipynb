{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import copy\n",
    "import time\n",
    "from line_profiler import LineProfiler\n",
    "from option_critic.utils import *\n",
    "from matplotlib import pyplot as plt\n",
    "from option_critic.fourrooms_copy import FourRooms\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m|\u001b[43m \u001b[0m: |B: |\n",
      "+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nHere\\'s our restructured problem statement (from Gym docs):\\n\\n\"There are 4 locations (labeled by different letters), and our job is to pick up the passenger \\nat one location and drop him off at another. We receive +20 points for a successful drop-off and \\nlose 1 point for every time-step it takes. There is also a 10 point penalty for illegal pick-up \\nand drop-off actions.\"\\n\\n- The filled square represents the taxi, which is yellow without a passenger and green with a passenger.\\n- The pipe (\"|\") represents a wall which the taxi cannot cross.\\n- R, G, Y, B are the possible pickup and destination locations. The blue letter represents the current passenger\\n  pick-up location, and the purple letter is the current destination.\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#[env.id for env in gym.envs.registry.all() if env.id.startswith('Taxi')]\n",
    "import Taxi_v0, Taxi_v1, Taxi_v4\n",
    "import Taxi_v00, Taxi_v01, Taxi_v03, Taxi_v04\n",
    "import Taxi_v000,Taxi_v001,Taxi_v003,Taxi_v004\n",
    "env = gym.make(\"Taxi-v3\")\n",
    "env.render()\n",
    "\"\"\"\n",
    "Here's our restructured problem statement (from Gym docs):\n",
    "\n",
    "\"There are 4 locations (labeled by different letters), and our job is to pick up the passenger \n",
    "at one location and drop him off at another. We receive +20 points for a successful drop-off and \n",
    "lose 1 point for every time-step it takes. There is also a 10 point penalty for illegal pick-up \n",
    "and drop-off actions.\"\n",
    "\n",
    "- The filled square represents the taxi, which is yellow without a passenger and green with a passenger.\n",
    "- The pipe (\"|\") represents a wall which the taxi cannot cross.\n",
    "- R, G, Y, B are the possible pickup and destination locations. The blue letter represents the current passenger\n",
    "  pick-up location, and the purple letter is the current destination.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "%autoreload 2\n",
    "#goals = [(8,8), (8,20), (20, 8), (20,20)]\n",
    "goals = [(27,27), (18,24), (24, 18), (20,20)]\n",
    "four_room_envs =[None] * 4\n",
    "for i in range(len(goals)):\n",
    "    four_room_envs[i] = FourRooms()\n",
    "    four_room_envs[i].reset()\n",
    "    four_room_envs[i].goal =  four_room_envs[i].tostate[goals[i]]\n",
    "    clear_output(True)\n",
    "    plt.subplot(2,2,i+1)\n",
    "    plt.imshow(four_room_envs[i].render(show_goal=True), cmap='Blues')\n",
    "    plt.axis('off')\n",
    "    plt.title('level ' + str(i))\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "| :\u001b[43m \u001b[0m| : : |\n",
      "| :R| :\u001b[34;1mG\u001b[0m: |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "| |\u001b[35mY\u001b[0m: | :B|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env=gym.make(\"Taxi-v001\")\n",
    "env.seed(0)\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "| :\u001b[43m \u001b[0m| : : |\n",
      "| :R| :\u001b[34;1mG\u001b[0m: |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "| |\u001b[35mY\u001b[0m: | :B|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gym.make(\"Taxi-v003\").render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "| : | : : |\n",
      "| :\u001b[35mR\u001b[0m| :G: |\n",
      "| : : : :\u001b[43m \u001b[0m|\n",
      "| | : | : |\n",
      "| |\u001b[34;1mY\u001b[0m: | :B|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gym.make(\"Taxi-v1\").render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : :\u001b[43m \u001b[0m|\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gym.make(\"Taxi-v3\").render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "| : | : : |\n",
      "| :\u001b[34;1mR\u001b[0m| :G: |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "| |\u001b[35mY\u001b[0m: | :\u001b[43mB\u001b[0m|\n",
      "+---------+\n",
      "\n",
      "Action space = Discrete(6)\n",
      "State space = Discrete(500)\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "env.render()\n",
    "print(\"Action space = {}\".format(env.action_space))\n",
    "print(\"State space = {}\".format(env.observation_space))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(1.0, 428, 0, False)],\n",
       " 1: [(1.0, 228, 0, False)],\n",
       " 2: [(1.0, 348, 0, False)],\n",
       " 3: [(1.0, 328, 0, False)],\n",
       " 4: [(1.0, 328, 0, False)],\n",
       " 5: [(1.0, 328, 0, False)]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.P[328] # {action: [(probability, nextstate, reward, done)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rng' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-47c79b39ac27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrng\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rng' is not defined"
     ]
    }
   ],
   "source": [
    "rng.choice(10)\n",
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arg_max(state_action):\n",
    "    max_index_list = []\n",
    "    max_value = state_action[0]\n",
    "    for index, value in enumerate(state_action):\n",
    "        if value > max_value:\n",
    "            max_index_list.clear()\n",
    "            max_value = value\n",
    "            max_index_list.append(index)\n",
    "        elif value == max_value:\n",
    "            max_index_list.append(index)\n",
    "    return random.choice(max_index_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_zero_state(q_table):\n",
    "    zero_state = 0\n",
    "    for arr_2d in q_table:\n",
    "         zero_state += np.sum(np.sum(arr_2d,axis = 1) == 0)\n",
    "    return zero_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "def saveData(data, path):\n",
    "    output = open(path, 'wb')\n",
    "    # Pickle dictionary using protocol 0.\n",
    "    pickle.dump(data, output)\n",
    "    output.close()\n",
    "    \n",
    "def loadData(path):\n",
    "    pkl_file = open(path, 'rb')\n",
    "    segContent = pickle.load(pkl_file)\n",
    "    pkl_file.close()\n",
    "    return segContent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODE_NUM = 4000\n",
    "MAX_EPISODE_LEN = 200\n",
    "REPEAT_TIMES = 10  # train agent REAPEAT_TIMES to get averaged learning curves\n",
    "EVALUATION_TIMES = 10 # evaluate target policy EVALUATION_TIMES after x updates in off-policy RL algorithms\n",
    "CLIST = ['b','c', 'g', 'k','m', 'r', 'y', 'w'] #colors to plot learning curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "def show_frames(env,j ,i, episode_rewards ):\n",
    "    clear_output(wait=True)\n",
    "    env.render()\n",
    "    print(f\"episode {j+1}  step {i}  rewards={episode_rewards}\")\n",
    "    sleep(0.1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(y, radius):\n",
    "    '''\n",
    "    smooth data y by averaging the values in each window [max{index-radius,0}, min{(index+radius), len(y)-1}] \n",
    "    \n",
    "    '''\n",
    "    if (len(y) < 2 * radius + 1):\n",
    "        return np.mean(y) * np.ones_like(y)\n",
    "    else:\n",
    "        convkernel = np.ones(2 * radius + 1)\n",
    "        out = np.convolve(y, convkernel, mode = 'same') / np.convolve(np.ones_like(y), convkernel, mode = 'same')\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluate(env, policy, times = 10):\n",
    "    # store data, make average and return\n",
    "    all_length, all_penalties, all_rewards = np.zeros(times), np.zeros(times), np.zeros(times)\n",
    "\n",
    "    for i in range(times):\n",
    "\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        episode_penalties, episode_rewards, episode_length = 0, 0, 0\n",
    "        \n",
    "        for j in range(MAX_EPISODE_LEN):\n",
    "            action = arg_max(policy[state])\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            state = next_state\n",
    "            #update data for learning curve\n",
    "            if reward == -10:\n",
    "                episode_penalties +=1\n",
    "\n",
    "            episode_rewards += reward\n",
    "            episode_length += 1\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        all_rewards[i] = episode_rewards\n",
    "        all_penalties[i] = episode_penalties\n",
    "        all_length[i] = episode_length\n",
    "    return np.mean(all_rewards), np.mean(all_penalties), np.mean(all_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.randint(0,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, train_episodes, init_q_table=None):\n",
    "    \"\"\"\n",
    "    Training the agent\n",
    "    Q(state,action)←(1−α)Q(state,action)+α(reward+γmaxaQ(next state,all actions))\n",
    "    \"\"\"  \n",
    "    \n",
    "    # Hyper parameters\n",
    "    alpha = 0.05  #(alpha) is the learning rate (0<α≤1) \n",
    "    gamma = 0.95  # (gamma) is the discount factor (0≤γ≤1) \n",
    "    epsilon = 0.1 # the rate to act randomly in the epsilon-greedy alg for expolration in Q-learning\n",
    "\n",
    "    timesteps =  train_episodes * MAX_EPISODE_LEN\n",
    "\n",
    "    # To plot learning curve\n",
    "    all_episodes_length = []\n",
    "    all_penalties = []\n",
    "    all_rewards = []\n",
    "    \n",
    "    if init_q_table is None:\n",
    "        q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "    else:\n",
    "        q_table = copy.deepcopy(init_q_table)\n",
    "    \n",
    "    env_copy = copy.deepcopy(env) # for policy_evaluate\n",
    "    i = 0\n",
    "    while i < timesteps:\n",
    "        env.seed(random.randint(0,timesteps))\n",
    "        state = env.reset()\n",
    "\n",
    "        episode_penalties, episode_rewards, episode_length = 0, 0, 0\n",
    "        done = False\n",
    "\n",
    "        while not done and i < timesteps:\n",
    "            # epsilon greedy alg balancing exporation and exploitation\n",
    "            if random.uniform(0,1)< epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = arg_max(q_table[state])\n",
    "\n",
    "            # step to next state\n",
    "            next_state, reward, done, info = env.step(action)       \n",
    "            \n",
    "            # update q-value\n",
    "            q_value = (1-alpha) * q_table[state, action] + alpha * (reward + gamma * np.max(q_table[next_state]))\n",
    "            q_table[state, action] = q_value\n",
    "\n",
    "            state = next_state\n",
    "            if i%200 == 0:\n",
    "                #evaluate policy for learning curve after each episode\n",
    "                episode_rewards, episode_penalties, episode_length = policy_evaluate(env_copy, q_table, EVALUATION_TIMES)\n",
    "                \n",
    "                all_episodes_length.append(episode_length)\n",
    "                all_penalties.append(episode_penalties)\n",
    "                all_rewards.append(episode_rewards)\n",
    "                \n",
    "                #show training progress\n",
    "                clear_output(wait=True)\n",
    "                print(f\"timesteps: {i}\")\n",
    "            i += 1\n",
    "         \n",
    "    print(f\"Training finished\")\n",
    "    \n",
    "    # return ndarray\n",
    "    all_episodes_length = np.array(all_episodes_length)\n",
    "    all_penalties = np.array(all_penalties)\n",
    "    all_rewards = np.array(all_rewards)\n",
    "    \n",
    "    return all_episodes_length, all_penalties, all_rewards, q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SARSA(env,  train_episodes,init_s_table = None):\n",
    "    \"\"\"Training the agent\n",
    "    Q(state,action)←(1−α)Q(state,action)+α(reward+γQ(next state,next action))\n",
    "\n",
    "    \"\"\"  \n",
    "    # Hyper parameters\n",
    "    alpha = 0.05  #(alpha) is the learning rate (0<α≤1) \n",
    "    gamma = 0.95  # (gamma) is the discount factor (0≤γ≤1) \n",
    "    epsilon = 0.1 # the rate to act randomly in the epsilon-greedy alg for expolration in Q-learning\n",
    "\n",
    "    train_episodes =  train_episodes \n",
    "    timesteps =  train_episodes * MAX_EPISODE_LEN\n",
    "    # To plot learning curve\n",
    "    all_episodes_length = []\n",
    "    all_penalties = []\n",
    "    all_rewards = []\n",
    "    \n",
    "    \n",
    "    if init_s_table is None:\n",
    "        s_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "    else:\n",
    "        s_table = copy.deepcopy(init_s_table)\n",
    "    env_copy = copy.deepcopy(env)\n",
    "    \n",
    "    i = 0\n",
    "    while i < timesteps:\n",
    "        env.seed(random.randint(0,timesteps))\n",
    "        state = env.reset()\n",
    "\n",
    "        episode_penalties, episode_rewards, episode_length = 0, 0, 0\n",
    "        done = False\n",
    "\n",
    "        # epsilon greedy alg balancing exporation and exploitation\n",
    "        if random.uniform(0,1)< epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = arg_max(s_table[state])\n",
    "\n",
    "        while not done and i < timesteps:\n",
    "\n",
    "            # step to next state\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "\n",
    "            # choose next action\n",
    "            if random.uniform(0,1)< epsilon:\n",
    "                next_action = env.action_space.sample()\n",
    "            else:\n",
    "                next_action = arg_max(s_table[next_state])\n",
    "\n",
    "            # update q-value\n",
    "            q_value = (1-alpha) * s_table[state, action] + alpha * (reward + gamma * s_table[next_state,next_action])\n",
    "            s_table[state, action] = q_value\n",
    "\n",
    "            state = next_state\n",
    "            action = next_action\n",
    "\n",
    "            if i%200 == 0:\n",
    "                #evaluate policy for learning curve after each episode\n",
    "                episode_rewards, episode_penalties, episode_length = policy_evaluate(env_copy, s_table, EVALUATION_TIMES)\n",
    "                \n",
    "                all_episodes_length.append(episode_length)\n",
    "                all_penalties.append(episode_penalties)\n",
    "                all_rewards.append(episode_rewards)\n",
    "                \n",
    "                #show training progress\n",
    "                clear_output(wait=True)\n",
    "                print(f\"timesteps: {i}\")\n",
    "            i += 1\n",
    "    print(\"Training finished \\n\")\n",
    "    \n",
    "    # return ndarray\n",
    "    all_episodes_length = np.array(all_episodes_length)\n",
    "    all_penalties = np.array(all_penalties)\n",
    "    all_rewards = np.array(all_rewards)\n",
    "    \n",
    "    return all_episodes_length, all_penalties, all_rewards, s_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 PRQL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prql(env, train_episodes, past_policy):\n",
    "        \n",
    "    # Hyper parameters\n",
    "    alpha = 0.05  #(alpha) is the learning rate (0<α≤1) \n",
    "    gamma = 0.95  # (gamma) is the discount factor (0≤γ≤1) \n",
    "    epsilon = 0.1 # the rate to act randomly in the epsilon-greedy alg for expolration in Q-learning\n",
    "    fi = 1 # the intial probability to reuse past policy in policy-reuse algo, decays after each step in one episode\n",
    "    mu = 0.95 # the decaying rate of fi\n",
    "    \n",
    "    # To plot learning curve\n",
    "    all_episodes_length = []\n",
    "    all_penalties = []\n",
    "    all_rewards = []\n",
    "\n",
    "    timesteps =  train_episodes * MAX_EPISODE_LEN\n",
    "    q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    " \n",
    "    past_table = copy.deepcopy(past_policy)\n",
    "    env_copy = copy.deepcopy(env)\n",
    "    \n",
    "    i = 0\n",
    "    while i < timesteps:\n",
    "        env.seed(random.randint(0,timesteps))\n",
    "        state = env.reset()\n",
    "\n",
    "        episode_penalties, episode_rewards, episode_length = 0, 0, 0\n",
    "        done = False\n",
    "        f = fi\n",
    "        while not done and i < timesteps:\n",
    "            # epsilon greedy alg balancing exporation and exploitation\n",
    "            if random.uniform(0,1) < f:\n",
    "                action = arg_max(past_table[state])\n",
    "                #print(f\"f = {f} in episode {i} \\n\")\n",
    "            elif random.uniform(0,1) < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "                #print(\"random action \\n\")\n",
    "                \n",
    "            else:\n",
    "                action = arg_max(q_table[state])\n",
    "                #print(\"on the policy now \\n\")\n",
    "\n",
    "            # step to next state\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "\n",
    "            # update q-value\n",
    "            q_value = (1-alpha) * q_table[state, action] + alpha * (reward + gamma * np.max(q_table[next_state]))\n",
    "            q_table[state, action] = q_value\n",
    "\n",
    "            state = next_state\n",
    "            f = f*mu\n",
    "            \n",
    "            if i%200 == 0:\n",
    "                #evaluate policy for learning curve after each episode\n",
    "                episode_rewards, episode_penalties, episode_length = policy_evaluate(env_copy, q_table, EVALUATION_TIMES)\n",
    "                \n",
    "                all_episodes_length.append(episode_length)\n",
    "                all_penalties.append(episode_penalties)\n",
    "                all_rewards.append(episode_rewards)\n",
    "                \n",
    "                #show training progress\n",
    "                clear_output(wait=True)\n",
    "                print(f\"timesteps: {i}\")\n",
    "            i += 1\n",
    "    print(\"Training finished \\n\")\n",
    "    \n",
    "    # return ndarray\n",
    "    all_episodes_length = np.array(all_episodes_length)\n",
    "    all_penalties = np.array(all_penalties)\n",
    "    all_rewards = np.array(all_rewards)\n",
    "    \n",
    "    return all_episodes_length, all_penalties, all_rewards, q_table    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 OPS-TL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OPS_TL(env,train_episodes,past_policies):\n",
    "    \"\"\"\n",
    "    Training the agent using algorithm in paper \"An Optimal Online Method of Selecting \n",
    "    Source Policies for Reinforcement Learning\"\n",
    "    \"\"\"\n",
    "    # Hyper parameters\n",
    "    alpha = 0.05 #(alpha) is the learning rate (0<α≤1) \n",
    "    gamma = 0.95 # (gamma) is the discount factor (0≤γ≤1)\n",
    "    epsilon = 0.1 # the rate to act randomly in the epsilon-greedy alg for expolration in Q-learning\n",
    "    fi = 0.95 # the intial probability to reuse past policy in policy-reuse algo, decays after each step in one episode\n",
    "    c = 0.0049 # the factor in UCB-tuned deciding the rate of exploration, a lager c will lead to a higher exploration rate\n",
    "    \n",
    "    # To plot learning curve\n",
    "    all_episodes_length = []\n",
    "    all_penalties = []\n",
    "    all_rewards = []\n",
    "   \n",
    "    #win = {'reuse': [], 'new': []}\n",
    "    timesteps = train_episodes * MAX_EPISODE_LEN\n",
    "   \n",
    "    q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "    env_copy = copy.deepcopy(env)\n",
    "    \n",
    "    # pt = 1-k/(k+1500)\n",
    "    \n",
    "    #initialize the expected reward of every bandit/past_policy\n",
    "    policy_expect = np.zeros([len(past_policies)])\n",
    "    policy_times = np.zeros([len(past_policies)])\n",
    "    for j in range(len(past_policies)):\n",
    "        t = 0\n",
    "        env.reset()\n",
    "        _,policy_expect[j],_,_= policy_reuse(timesteps, env,q_table, env.reset(), t,[],[],[],past_policies[j])\n",
    "        policy_times[j] += 1\n",
    "        \n",
    "    # training strat  \n",
    "    i = 0\n",
    "    while i < timesteps:\n",
    "        env.seed(random.randint(0,timesteps))\n",
    "        state = env.reset()\n",
    "        \n",
    "        episode_penalties, episode_rewards, episode_length = 0, 0, 0\n",
    "        done = False\n",
    "        \n",
    "        k = i // MAX_EPISODE_LEN  + 1\n",
    "        pt = 1-k/(k+1500)\n",
    "        \n",
    "        # use UCB1 to solve MAB(multi armed bandit problem\n",
    "        if random.uniform(0,1) < pt:\n",
    "            j = arg_max(policy_expect + np.sqrt(c * np.log(np.sum(policy_times)) / policy_times )) # UCB1-tuned\n",
    "            episode_penalties, episode_rewards, episode_length,i = policy_reuse(timesteps, env, q_table, state, i,\n",
    "                                                                              all_episodes_length,all_penalties,all_rewards,\n",
    "                                                                              past_policy = past_policies[j])\n",
    "            policy_expect[j] = (policy_expect[j]*policy_times[j] + episode_rewards)/( policy_times[j] +1)\n",
    "            policy_times[j] += 1\n",
    "#            if episode_length < 200:\n",
    "#                win['reuse'].append(k)\n",
    "        # follow episilon-greedy strategy    \n",
    "        else:\n",
    "            episode_penalties, episode_rewards, episode_length,i = epsilon_greedy(timesteps, env, q_table, state, i , \n",
    "                                                                                all_episodes_length,all_penalties,all_rewards,\n",
    "                                                                                epsilon, alpha, gamma)\n",
    "#            if episode_length < 200:\n",
    "#                win['new'].append(k)        \n",
    "\n",
    "\n",
    "        #show training progress\n",
    "        if (k) % 100 == 0:\n",
    "            clear_output(wait=True)\n",
    "            print(f\"Episode {k}\")\n",
    "    print(f\"Training finished\")\n",
    "    \n",
    "    # return ndarray\n",
    "    all_episodes_length = np.array(all_episodes_length)\n",
    "    all_penalties = np.array(all_penalties)\n",
    "    all_rewards = np.array(all_rewards)\n",
    "    \n",
    "    return all_episodes_length, all_penalties, all_rewards, q_table    \n",
    "            \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(timesteps, env, q_table, initial_state, i,\n",
    "                   all_episodes_length,all_penalties,all_rewards,epsilon = 0.1, alpha = 0.05,  gamma = 0.95 ):\n",
    "    done = False\n",
    "    state = initial_state\n",
    "    episode_penalties, episode_rewards, episode_length = 0, 0, 0\n",
    "  \n",
    "    _env_copy = copy.deepcopy(env)\n",
    "    while not done and i < timesteps:\n",
    "        if random.uniform(0,1) < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = arg_max(q_table[state])\n",
    "            \n",
    "        # step to next state\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # update q-value\n",
    "        q_value = (1-alpha) * q_table[state, action] + alpha * (reward + gamma * np.max(q_table[next_state]))\n",
    "        q_table[state, action] = q_value\n",
    "\n",
    "        state = next_state\n",
    "      \n",
    "        #update data for learning curve\n",
    "        if reward == -10:\n",
    "            episode_penalties +=1\n",
    "\n",
    "        episode_rewards += reward\n",
    "        episode_length += 1\n",
    "        \n",
    "        if i%200 == 0:\n",
    "            #evaluate policy for learning curve after each episode\n",
    "            _episode_rewards, _episode_penalties, _episode_length = policy_evaluate(_env_copy, q_table, EVALUATION_TIMES)\n",
    "\n",
    "            all_episodes_length.append(_episode_length)\n",
    "            all_penalties.append(_episode_penalties)\n",
    "            all_rewards.append(_episode_rewards)\n",
    "\n",
    "            #show training progress\n",
    "            clear_output(wait=True)\n",
    "            print(f\"timesteps: {i}\")\n",
    "        i += 1\n",
    "\n",
    "    return episode_penalties, episode_rewards, episode_length,i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_reuse(timesteps,env, q_table, initial_state, i, \n",
    "                 all_episodes_length,all_penalties,all_rewards, past_policy,  fi = 0.95, mu=0.95, alpha = 0.05,  gamma = 0.95 ):\n",
    "    done = False\n",
    "    state = initial_state\n",
    "    episode_penalties, episode_rewards, episode_length = 0, 0, 0\n",
    "    \n",
    "    _env_copy = copy.deepcopy(env)\n",
    "    while not done and i < timesteps:\n",
    "        if random.uniform(0,1) < fi:\n",
    "            action = arg_max(past_policy[state])\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "            \n",
    "        # step to next state\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # update q-value\n",
    "        q_value = (1-alpha) * q_table[state, action] + alpha * (reward + gamma * np.max(q_table[next_state]))\n",
    "        q_table[state, action] = q_value\n",
    "\n",
    "        state = next_state\n",
    "        fi = fi*mu\n",
    "        #update data for learning curve\n",
    "        if reward == -10:\n",
    "            episode_penalties +=1\n",
    "\n",
    "        episode_rewards += reward\n",
    "        episode_length += 1\n",
    "        \n",
    "        if i%200 == 0:\n",
    "            #evaluate policy for learning curve after each episode\n",
    "            _episode_rewards, _episode_penalties, _episode_length = policy_evaluate(_env_copy, q_table, EVALUATION_TIMES)\n",
    "\n",
    "            all_episodes_length.append(_episode_length)\n",
    "            all_penalties.append(_episode_penalties)\n",
    "            all_rewards.append(_episode_rewards)\n",
    "\n",
    "            #show training progress\n",
    "            clear_output(wait=True)\n",
    "            print(f\"timesteps: {i}\")\n",
    "        i += 1\n",
    "        \n",
    "    return episode_penalties, episode_rewards, episode_length,i\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 CAPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CAPS(env,train_episodes,past_policies):\n",
    "    \"\"\"\n",
    "    Training the agent using algorithm in paper \"Context-aware policy reuse\"\n",
    "    \"\"\"\n",
    "    # Hyper parameters\n",
    "    alpha = 0.05 #(alpha) is the learning rate (0<α≤1) \n",
    "    gamma = 0.95 # (gamma) is the discount factor (0≤γ≤1)\n",
    "    epsilon = 0.1 # the rate to act randomly in the epsilon-greedy alg for expolration in Q-learning\n",
    "    fi = 0.95 # the intial probability to reuse past policy in policy-reuse algo, decays after each step in one episode\n",
    "    c = 0.0049 # the factor in UCB-tuned deciding the rate of exploration, a lager c will lead to a higher exploration rate\n",
    "    \n",
    "    # To plot learning curve\n",
    "    all_episodes_length = []\n",
    "    all_penalties = []\n",
    "    all_rewards = []\n",
    "    all_frequency = np.zeros([len(past_policies)+1, train_episodes])\n",
    "#   win = {'reuse': [], 'new': []}\n",
    "    timesteps = train_episodes * MAX_EPISODE_LEN\n",
    "   \n",
    "    q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "    env_copy = copy.deepcopy(env)\n",
    "    \n",
    "    # pt = 1-k/(k+1500)\n",
    "    \n",
    "    #initialize the expected reward of every bandit/past_policy\n",
    "    frequency = np.zeros([len(past_policies)+1, train_episodes])\n",
    "    policy_expect = np.zeros([len(past_policies)])\n",
    "    policy_times = np.zeros([len(past_policies)])\n",
    "    for j in range(len(past_policies)):\n",
    "        t = 0\n",
    "        env.reset()\n",
    "        for _ in range(5):\n",
    "            _,temp,_,_= policy_reuse(timesteps, env,q_table, env.reset(), t,[],[],[],past_policies[j])\n",
    "            policy_expect[j] += temp\n",
    "        policy_expect[j] /= 10\n",
    "        policy_times[j] += 1\n",
    "        \n",
    "    # training strat  \n",
    "    i = 0\n",
    "    while i < timesteps:\n",
    "        env.seed(random.randint(0,timesteps))\n",
    "        state = env.reset()\n",
    "        \n",
    "        episode_penalties, episode_rewards, episode_length = 0, 0, 0\n",
    "        done = False\n",
    "        \n",
    "        k = i // MAX_EPISODE_LEN  + 1\n",
    "        pt = 1-k/(k+1500)\n",
    "        \n",
    "        # use UCB1 to solve MAB(multi armed bandit problem\n",
    "        if random.uniform(0,1) < pt:\n",
    "            j = arg_max(policy_expect/20 + np.sqrt(c * np.log(np.sum(policy_times)) / policy_times )) # UCB1-tuned\n",
    "            episode_penalties, episode_rewards, episode_length,i = policy_reuse(timesteps, env, q_table, state, i,\n",
    "                                                                              all_episodes_length,all_penalties,all_rewards,\n",
    "                                                                              past_policy = past_policies[j])\n",
    "            policy_expect[j] = (policy_expect[j]*policy_times[j] + episode_rewards)/( policy_times[j] +1)\n",
    "            policy_times[j] += 1\n",
    "            frequency[j][k-1] += 1\n",
    "#            if episode_length < 200:\n",
    "#                win['reuse'].append(k)\n",
    "        # follow episilon-greedy strategy    \n",
    "        else:\n",
    "            episode_penalties, episode_rewards, episode_length,i = epsilon_greedy(timesteps, env, q_table, state, i , \n",
    "                                                                                all_episodes_length,all_penalties,all_rewards,\n",
    "                                                                                epsilon, alpha, gamma)\n",
    "#            if episode_length < 200:\n",
    "#                win['new'].append(k)        \n",
    "            frequency[len(past_policies)][k-1] += 1\n",
    "\n",
    "\n",
    "    print(f\"Training finished\")\n",
    "    \n",
    "    # return ndarray\n",
    "    all_episodes_length = np.array(all_episodes_length)\n",
    "    all_penalties = np.array(all_penalties)\n",
    "    all_rewards = np.array(all_rewards)\n",
    "    for line in range(train_episodes):\n",
    "        for row in range(len(past_policies)+1):\n",
    "            all_frequency[row][line]=np.sum(frequency[row][:line+1], keepdims= True) /np.sum(np.sum(frequency[:,:line+1]))\n",
    "    return all_episodes_length, all_penalties, all_rewards, all_frequency, q_table    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 OC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def option_critic(env ,episode_num ,  option_policies_lib = None, \n",
    "                  option_terminations_lib = None, policy_over_options = None, critic = None, noptions = 4 ):\n",
    "    \n",
    "    # Discount\n",
    "    discount = 0.99\n",
    "    \n",
    "\n",
    "    # Learning rates - termination, intra-option, critic\n",
    "    lr_term = 0.25\n",
    "    lr_intra = 0.25\n",
    "    lr_critic = 0.5\n",
    "\n",
    "    # Epsilon for epsilon-greedy for policy over options\n",
    "    epsilon = 1e-1\n",
    "\n",
    "    # Temperature for softmax\n",
    "    temperature = 0.01\n",
    "    \n",
    "    # To plot learning curve\n",
    "    all_episodes_length = []\n",
    "    all_penalties = []\n",
    "    all_rewards = []\n",
    "    \n",
    "\n",
    "    \n",
    "    #trainning timesteps\n",
    "    timesteps =  episode_num * MAX_EPISODE_LEN\n",
    "\n",
    "    nstates = env.observation_space.n\n",
    "    nactions = env.action_space.n\n",
    "    \n",
    "    # for option_evaluate\n",
    "    env.seed(random.randint(0,timesteps))\n",
    "    state = env.reset()\n",
    "    env_copy = copy.deepcopy(env) \n",
    "\n",
    "    # Following three belong to the Actor\n",
    "\n",
    "    # 1. The intra-option policies - linear softmax functions\n",
    "    if option_policies_lib is None:\n",
    "        option_policies = [SoftmaxPolicy(rng, lr_intra, nstates, nactions, epsilon, temperature) for _ in range(noptions)]\n",
    "      \n",
    "       \n",
    "    else:\n",
    "        option_policies = [None] * noptions\n",
    "        for i in range(noptions):\n",
    "            option_policies[i] = copy.deepcopy(option_policies_lib[i])\n",
    "\n",
    "\n",
    "    # 2. The termination function - linear sigmoid function\n",
    "    if option_terminations_lib is None:\n",
    "        option_terminations = [SigmoidTermination(rng, lr_term, nstates) for _ in range(noptions)]\n",
    "        \n",
    "    else:\n",
    "        option_terminations =  [None] * noptions\n",
    "        for i in range(noptions):\n",
    "            option_terminations[i] = copy.deepcopy(option_terminations_lib[i])\n",
    "\n",
    "\n",
    "\n",
    "    # 3. The epsilon-greedy policy over options\n",
    "    if policy_over_options is None:\n",
    "        policy_over_options = EpsGreedyPolicy(rng, nstates, noptions, epsilon)\n",
    "\n",
    "        \n",
    "    else:\n",
    "        policy_over_options = copy.deepcopy(policy_over_options)\n",
    "\n",
    "\n",
    "    # Critic\n",
    "    if critic is None:\n",
    "        critic = Critic(lr_critic, discount, policy_over_options.Q_Omega_table, nstates, noptions, nactions)\n",
    "    else:\n",
    "        critic = copy.deepcopy(critic)\n",
    "\n",
    "   \n",
    "    \n",
    "    i=0\n",
    "    while i < timesteps:\n",
    "\n",
    "        # Change goal location after 1000 episodes \n",
    "        # Comment it for not doing transfer experiments\n",
    "#        if episode == 1000:\n",
    "#            env.goal = rng.choice(possible_next_goals)\n",
    "#            print('New goal: ', env.goal)\n",
    "\n",
    "        state = env.reset()\n",
    "\n",
    "        option = policy_over_options.sample(state)\n",
    "        action = option_policies[option].sample(state)\n",
    "\n",
    "        critic.cache(state, option, action)\n",
    "        \n",
    "        episode_penalties, episode_rewards, episode_length = 0, 0, 0\n",
    "        done = False\n",
    "        \n",
    "        \n",
    "        while not done and i < timesteps:\n",
    "            state, reward, done, _ = env.step(action)\n",
    "\n",
    "            # Termination might occur upon entering new state\n",
    "            if option_terminations[option].sample(state):\n",
    "                option = policy_over_options.sample(state)\n",
    "\n",
    "\n",
    "            action = option_policies[option].sample(state)\n",
    "\n",
    "            # Critic update\n",
    "            critic.update_Qs(state, option, action, reward, done, option_terminations)\n",
    "\n",
    "            # Intra-option policy update with baseline\n",
    "            Q_U = critic.Q_U(state, option, action)\n",
    "            Q_U = Q_U - critic.Q_Omega(state, option)\n",
    "            option_policies[option].update(state, action, Q_U)\n",
    "\n",
    "            # Termination condition update\n",
    "            option_terminations[option].update(state, critic.A_Omega(state, option))\n",
    "            \n",
    "          \n",
    "            if i%200 == 0:\n",
    "                #evaluate policy for learning curve after each episode\n",
    "                episode_rewards, episode_penalties, episode_length = option_evaluate(env_copy, option_policies, option_terminations, policy_over_options, EVALUATION_TIMES)\n",
    "                \n",
    "                all_episodes_length.append(episode_length)\n",
    "                all_penalties.append(episode_penalties)\n",
    "                all_rewards.append(episode_rewards)\n",
    "\n",
    "                #show training progress\n",
    "                clear_output(wait=True)\n",
    "                print(f\"timesteps: {i}\")\n",
    "            i += 1\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"Training finished\")\n",
    "    \n",
    "    # return ndarray\n",
    "    all_episodes_length = np.array(all_episodes_length)\n",
    "    all_penalties = np.array(all_penalties)\n",
    "    all_rewards = np.array(all_rewards)\n",
    "    \n",
    "    return all_episodes_length, all_penalties, all_rewards, option_policies, option_terminations, policy_over_options,critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def option_evaluate(env,  option_policies, option_terminations, policy_over_options , times):\n",
    "    # store data, make average and return\n",
    "    all_length, all_penalties, all_rewards = np.zeros(times), np.zeros(times), np.zeros(times)\n",
    "\n",
    "    for i in range(times):\n",
    "\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        episode_penalties, episode_rewards, episode_length = 0, 0, 0\n",
    "        \n",
    "        option = policy_over_options.evaluate(state)\n",
    "        action = option_policies[option].evaluate(state)\n",
    "       \n",
    "        for j in range(MAX_EPISODE_LEN):\n",
    "            # Termination might occur upon entering new state\n",
    "            if option_terminations[option].sample(state):\n",
    "                option = policy_over_options.evaluate(state)\n",
    "\n",
    "\n",
    "            action = option_policies[option].evaluate(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            state = next_state\n",
    "            #update data for learning curve\n",
    "            if reward == -10:\n",
    "                episode_penalties +=1\n",
    "\n",
    "            episode_rewards += reward\n",
    "            episode_length += 1\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        all_rewards[i] = episode_rewards\n",
    "        all_penalties[i] = episode_penalties\n",
    "        all_length[i] = episode_length\n",
    "    return np.mean(all_rewards), np.mean(all_penalties), np.mean(all_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  tranform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros([1,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform( target_rl_algo, episode_num, repeat_times, target_task, \n",
    "              source_task= None, source_rl_algo = None, policy_library = None):\n",
    "    \n",
    "    \"\"\"\n",
    "    Situation 1:\n",
    "    no source task available, train agent on target task from scratch \n",
    "    \n",
    "    Situation 2:\n",
    "    source task is available but source policy unavailable, train agent on source task to get source policy, \n",
    "    then train agent on target task using knowledge from source policy\n",
    "    \n",
    "    Situation 3:\n",
    "    source policies are availale, reuse source policies to train agent on target task\n",
    "    \"\"\"\n",
    "    train_episodes = episode_num    \n",
    "    # data collected during trainning\n",
    "    all_episodes_length = np.zeros([repeat_times,train_episodes])\n",
    "    all_penalties = np.zeros([repeat_times,train_episodes])\n",
    "    all_rewards = np.zeros([repeat_times,train_episodes])\n",
    "    if target_rl_algo is CAPS and policy_library is not None:\n",
    "        all_frequency = np.zeros([len(policy_library[0])+1, train_episodes])\n",
    "    else:\n",
    "        all_frequency = None\n",
    "    \n",
    "    all_trans_knowledge = []\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in range(repeat_times):\n",
    "       \n",
    "        rng = np.random.seed(i)\n",
    "        \n",
    "        #Situation 3    \n",
    "        if policy_library is not None: \n",
    "            \n",
    "            if type(target_task) is str:\n",
    "                env = gym.make(target_task)\n",
    "            else:\n",
    "                env = target_task\n",
    "            past_policies = policy_library # policy_library = [ [[policy11][policy21]...] [[policy12][policy22]...]...]\n",
    "            if target_rl_algo is CAPS:\n",
    "                episodes,penalties,rewards,frequency, *knowledge = target_rl_algo(env, train_episodes, past_policies[i]) \n",
    "            else:\n",
    "                episodes,penalties,rewards, *knowledge = target_rl_algo(env, train_episodes,  past_policies[i]) \n",
    "        #Situation 2   \n",
    "        elif source_task is not None:\n",
    "            if source_rl_algo is None:\n",
    "                source_rl_algo = target_rl_algo\n",
    "            if type(source_task) is str:\n",
    "                env = gym.make(source_task)\n",
    "            else:\n",
    "                env = source_task\n",
    "            _, _,_ ,*knowledge = source_rl_algo(env,  train_episodes)\n",
    "            \n",
    "            if type(target_task) is str:\n",
    "                env = gym.make(target_task)\n",
    "            else:\n",
    "                env = target_task\n",
    "            episodes,penalties,rewards,*knowledge = target_rl_algo(env, train_episodes , *knowledge)\n",
    "         #Situation 1\n",
    "        else: \n",
    "            if type(target_task) is str:\n",
    "                env = gym.make(target_task)\n",
    "            else:\n",
    "                env = target_task\n",
    "                \n",
    "           \n",
    "            episodes,penalties,rewards,*knowledge = target_rl_algo(env, train_episodes)\n",
    "            \n",
    "\n",
    "        all_episodes_length[i] = episodes\n",
    "        all_penalties[i] = penalties\n",
    "        all_rewards[i] = rewards\n",
    "        if target_rl_algo is CAPS and policy_library is not None:\n",
    "            all_frequency += frequency\n",
    "        if len(knowledge) > 1:\n",
    "            all_trans_knowledge.append(knowledge)\n",
    "        else:\n",
    "            all_trans_knowledge.append(*knowledge)\n",
    "\n",
    "    \n",
    "    if target_rl_algo is CAPS and policy_library is not None:\n",
    "        all_frequency /= repeat_times\n",
    "    \n",
    "    return all_episodes_length, all_penalties, all_rewards, all_frequency, all_trans_knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0.])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a= np.zeros([2,3])\n",
    "a[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'FourRooms' object has no attribute 'seed'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-9fc5cc68f80b>\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(target_rl_algo, episode_num, repeat_times, target_task, source_task, source_rl_algo, policy_library)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mepisodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpenalties\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mknowledge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_rl_algo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_episodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrng\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-8f3fe6f95cee>\u001b[0m in \u001b[0;36mq_learning\u001b[0;34m(env, train_episodes, rng, init_q_table)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtimesteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrng\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimesteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'FourRooms' object has no attribute 'seed'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "qlearn_episodes_length_fourroom, qlearn_penalties_fourroom, qlearn_rewards_fourroom, qlearn_q_table_fourroom = [[None] * len(four_room_envs) for _ in range(4)]\n",
    "for i in range(len(four_room_envs)):\n",
    "    qlearn_episodes_length_fourroom[i], qlearn_penalties_fourroom[i], qlearn_rewards_fourroom[i], _, qlearn_q_table_fourroom[i] = transform(target_rl_algo = q_learning,\n",
    "                                                                                     episode_num = EPISODE_NUM, \n",
    "                                                                                   repeat_times = REPEAT_TIMES,\n",
    "                                                                                     target_task = four_room_envs[i],\n",
    "                                                                                     source_task = None\n",
    "                                                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 799800\n",
      "Training finished\n",
      "CPU times: user 12min 50s, sys: 1min 28s, total: 14min 19s\n",
      "Wall time: 15min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# training Taxi-v003 from scratch\n",
    "qlearn_episodes_length_v003, qlearn_penalties_v003, qlearn_rewards_v003, _, qlearn_q_table_v003 = transform(target_rl_algo = q_learning,\n",
    "                                                                                     episode_num = EPISODE_NUM, \n",
    "                                                                                     repeat_times = REPEAT_TIMES,\n",
    "                                                                                     target_task = \"Taxi-v003\",\n",
    "                                                                                     source_task = None\n",
    "                                                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 799800\n",
      "Training finished\n",
      "CPU times: user 12min 15s, sys: 1min 8s, total: 13min 24s\n",
      "Wall time: 14min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# training Taxi-v000 from scratch\n",
    "qlearn_episodes_length_v000, qlearn_penalties_v000, qlearn_rewards_v000, _, qlearn_q_table_v000 = transform(target_rl_algo = q_learning,\n",
    "                                                                                     episode_num = EPISODE_NUM, \n",
    "                                                                                     repeat_times = REPEAT_TIMES,\n",
    "                                                                                     target_task = \"Taxi-v000\",\n",
    "                                                                                     source_task = None\n",
    "                                                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 799800\n",
      "Training finished\n",
      "CPU times: user 12min 4s, sys: 1min 14s, total: 13min 19s\n",
      "Wall time: 14min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# training Taxi-v001 from scratch\n",
    "qlearn_episodes_length_v001, qlearn_penalties_v001, qlearn_rewards_v001,_, qlearn_q_table_v001 = transform(target_rl_algo = q_learning,\n",
    "                                                                                     episode_num = EPISODE_NUM, \n",
    "                                                                                     repeat_times = REPEAT_TIMES,\n",
    "                                                                                     target_task = \"Taxi-v001\",\n",
    "                                                                                     source_task = None\n",
    "                                                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 799800\n",
      "Training finished\n",
      "CPU times: user 12min 55s, sys: 1min 27s, total: 14min 22s\n",
      "Wall time: 15min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# training Taxi-v004 from scratch\n",
    "qlearn_episodes_length_v004, qlearn_penalties_v004, qlearn_rewards_v004,_, qlearn_q_table_v004 = transform(target_rl_algo = q_learning,\n",
    "                                                                                     episode_num = EPISODE_NUM, \n",
    "                                                                                     repeat_times = REPEAT_TIMES,\n",
    "                                                                                     target_task = \"Taxi-v004\",\n",
    "                                                                                     source_task = None\n",
    "                                                                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 799800\n",
      "Training finished \n",
      "\n",
      "CPU times: user 10min 37s, sys: 1min 44s, total: 12min 21s\n",
      "Wall time: 12min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# training Taxi-v003 from scratch\n",
    "sarsa_episodes_length_v003, sarsa_penalties_v003, sarsa_rewards_v003, _, sarsa_q_table_v003 = transform(target_rl_algo = SARSA,\n",
    "                                                                                     episode_num = EPISODE_NUM, \n",
    "                                                                                     repeat_times = REPEAT_TIMES,\n",
    "                                                                                     target_task = \"Taxi-v003\",\n",
    "                                                                                     source_task = None\n",
    "                                                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 799800\n",
      "Training finished \n",
      "\n",
      "CPU times: user 7min 21s, sys: 2min 3s, total: 9min 24s\n",
      "Wall time: 8min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# training Taxi-v001 from scratch\n",
    "sarsa_episodes_length_v001, sarsa_penalties_v001, sarsa_rewards_v001,_, sarsa_q_table_v001 = transform(target_rl_algo = SARSA,\n",
    "                                                                                     episode_num = EPISODE_NUM, \n",
    "                                                                                     repeat_times = REPEAT_TIMES,\n",
    "                                                                                     target_task = \"Taxi-v001\",\n",
    "                                                                                     source_task = None\n",
    "                                                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 799800\n",
      "Training finished \n",
      "\n",
      "CPU times: user 7min 58s, sys: 2min 29s, total: 10min 27s\n",
      "Wall time: 8min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# training Taxi-v004 from scratch\n",
    "sarsa_episodes_length_v004, sarsa_penalties_v004, sarsa_rewards_v004,_, sarsa_q_table_v004 = transform(target_rl_algo = SARSA,\n",
    "                                                                                     episode_num = EPISODE_NUM, \n",
    "                                                                                     repeat_times = REPEAT_TIMES,\n",
    "                                                                                     target_task = \"Taxi-v004\",\n",
    "                                                                                     source_task = None\n",
    "                                                                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 PRQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'qlearn_q_table_fourroom' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'qlearn_q_table_fourroom' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pr_episodes_length_fourroom, pr_penalties_fourroom, pr_rewards_fourroom, pr_q_table_fourroom = [[None] * (len(four_room_envs)-1) for _ in range(4)]\n",
    "\n",
    "#pr_fourroom_v012_3\n",
    "for i in range(len(four_room_envs)-1):\n",
    "    pr_episodes_length_fourroom[i], pr_penalties_fourroom[i], pr_rewards_fourroom[i], _, pr_q_table_fourroom[i] = transform(target_rl_algo = prql,\n",
    "                                                                                     episode_num = EPISODE_NUM, \n",
    "                                                                                   repeat_times = REPEAT_TIMES,\n",
    "                                                                                     target_task = four_room_envs[3],\n",
    "                                                                                     policy_library = qlearn_q_table_fourroom[i]\n",
    "                                                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "randint() missing 1 required positional argument: 'b'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-abc5b5252d29>\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(target_rl_algo, episode_num, repeat_times, target_task, source_task, source_rl_algo, policy_library)\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0mepisodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpenalties\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfrequency\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mknowledge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_rl_algo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_episodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast_policies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m                 \u001b[0mepisodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpenalties\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mknowledge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_rl_algo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_episodes\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mpast_policies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;31m#Situation 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0msource_task\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-f3c5f7f96693>\u001b[0m in \u001b[0;36mprql\u001b[0;34m(env, train_episodes, past_policy)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtimesteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimesteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: randint() missing 1 required positional argument: 'b'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#pr_v014_003\n",
    "pr_episodes_length_v014_003, pr_penalties_v014_003,pr_rewards_v014_003, _, pr_q_table_v014_003 = transform(target_rl_algo= prql,\n",
    "                                                                                       episode_num = EPISODE_NUM,\n",
    "                                                                                       repeat_times = REPEAT_TIMES,\n",
    "                                                                                       target_task = \"Taxi-v003\",\n",
    "                                                                                       \n",
    "                                                                                       policy_library =  qlearn_q_table_v004)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3 OPS-TL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'qlearn_q_table_fourroom' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'qlearn_q_table_fourroom' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "ops_episodes_length_fourroom, ops_penalties_fourroom, ops_rewards_fourroom, ops_q_table_fourroom = [[None] * (len(four_room_envs)-1) for _ in range(4)]\n",
    "\n",
    "#ops_fourroom_v012_3\n",
    "\n",
    "ops_episodes_length_fourroom, ops_penalties_fourroom, ops_rewards_fourroom, _, ops_q_table_fourroom = transform(target_rl_algo = OPS_TL,\n",
    "                                                                                     episode_num = EPISODE_NUM, \n",
    "                                                                                   repeat_times = REPEAT_TIMES,\n",
    "                                                                                     target_task = four_room_envs[3],\n",
    "                                                                                     policy_library =  [[q_func1,q_func2,q_func3 ] for q_func1,q_func2,q_func3 in zip(qlearn_q_table_fourroom[0],qlearn_q_table_fourroom[1],qlearn_q_table_fourroom[2])]\n",
    "                                                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'qlearn_q_table_v000' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'qlearn_q_table_v000' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#ops_v014_003\n",
    "ops_episodes_length_v014_003, ops_penalties_v014_003,ops_rewards_v014_003,ops_fre_v014_003, ops_q_table_v014_003 = transform(target_rl_algo= OPS_TL,\n",
    "                                                                                       episode_num = EPISODE_NUM,\n",
    "                                                                                       repeat_times = REPEAT_TIMES,\n",
    "                                                                                       target_task = \"Taxi-v003\",\n",
    "                                                                                      \n",
    "                                                                                       policy_library = [[q_func1,q_func2,q_func3 ] for q_func1,q_func2,q_func3 in zip(qlearn_q_table_v000, qlearn_q_table_v001, qlearn_q_table_v004)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 CAPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'qlearn_q_table_fourroom' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'qlearn_q_table_fourroom' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "#caps_fourroom_v012_3\n",
    "\n",
    "caps_episodes_length_fourroom, caps_penalties_fourroom, caps_rewards_fourroom, _, caps_q_table_fourroom = transform(target_rl_algo = CAPS,\n",
    "                                                                                     episode_num = EPISODE_NUM, \n",
    "                                                                                   repeat_times = REPEAT_TIMES,\n",
    "                                                                                     target_task = four_room_envs[3],\n",
    "                                                                                     policy_library =  [[q_func1,q_func2,q_func3 ] for q_func1,q_func2,q_func3 in zip(qlearn_q_table_fourroom[0],qlearn_q_table_fourroom[1],qlearn_q_table_fourroom[2])]\n",
    "                                                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'qlearn_q_table_v000' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'qlearn_q_table_v000' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#caps_v014_003\n",
    "caps_episodes_length_v014_003, caps_penalties_v014_003,caps_rewards_v014_003,caps_fre_v014_003, caps_q_table_v014_003 = transform(target_rl_algo= CAPS,\n",
    "                                                                                       episode_num = EPISODE_NUM,\n",
    "                                                                                       repeat_times = REPEAT_TIMES,\n",
    "                                                                                       target_task = \"Taxi-v003\",\n",
    "                                                                                      \n",
    "                                                                                       policy_library = [[q_func1,q_func2,q_func3 ] for q_func1,q_func2,q_func3 in zip(qlearn_q_table_v000, qlearn_q_table_v001, qlearn_q_table_v004)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 OC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Line magic function `%autoreload` not found.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%autoreload 2\n",
    "option_policies, option_terminations, policy_over_options, critic, nrewards = [ [None]*4 for i in range(5) ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 799800\n",
      "Training finished\n",
      "CPU times: user 13min 44s, sys: 2min 30s, total: 16min 15s\n",
      "Wall time: 14min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for i in [0,1,2,3]:\n",
    "    _, _ ,nrewards[i] ,option_policies[i], option_terminations[i], policy_over_options[i],critic[i]= option_critic( four_room_envs[i], 4000, noptions=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-253-07e25a4c40f4>\u001b[0m in \u001b[0;36moption_critic\u001b[0;34m(env, episode_num, option_policies_lib, option_terminations_lib, policy_over_options, critic, noptions)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0moption_policies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnoptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0moption_policies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moption_policies_lib\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trans_rewards,trans_terminations, trans_policies = [ [None]*4 for _ in range(3)]\n",
    " \n",
    "for i in [0,1,2]:\n",
    "    _,_, trans_rewards[i], trans_policies[i], trans_terminations[i], _,_= option_critic( four_room_envs[3], 4000, option_policies[i], option_terminations[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 799800\n",
      "Training finished\n"
     ]
    }
   ],
   "source": [
    "source_lib = [1,2]\n",
    "_,_, trans_rewards_012, _, _, _,_= option_critic( four_room_envs[3], 4000, [option_policies[i][0] for i in source_lib], noptions=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc810a14b62f456bb382c2c8a2b95a7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "import seaborn as sns\n",
    "RATIO = 1\n",
    "SMOOTH_RADIUS = 50\n",
    "sns.set(style = \"darkgrid\")\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(qlearn_rewards_fourroom[2], SMOOTH_RADIUS), label = \"NoneTransfer\")\n",
    "#plt.plot(range(EPISODE_NUM), smooth(qlearn_rewards_v5, 100), label = \"qlearn_v5\")\n",
    "\n",
    "#plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v000_003,SMOOTH_RADIUS), label = \"CAPS_v0_003\",color='b')\n",
    "#plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v001_003,SMOOTH_RADIUS), label = \"CAPS_v1_003\",color='m')\n",
    "#plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v004_003,SMOOTH_RADIUS), label = \"CAPS_v4_003\",color='y')\n",
    "i = 0\n",
    "#plt.plot(range(EPISODE_NUM//RATIO), smooth(pr_rewards_fourroom[i],SMOOTH_RADIUS), label = \"pr_v014_003\",color='c')\n",
    "#plt.plot(range(EPISODE_NUM//RATIO), smooth(tl_rewards_fourroom,SMOOTH_RADIUS), label = \"ops_v014_003\",color='g')\n",
    "#plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_fourroom,SMOOTH_RADIUS), label = \"caps_v014_003\",color='r')\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(trans_rewards_012,SMOOTH_RADIUS), label = \"oc_v01_3\")\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(nrewards[3], SMOOTH_RADIUS), label = \"oc_v3\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.title(\"0-4000 episode-reward graph\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"rewards\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc98862b686745548d8e84d04a8f09b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'pr_rewards_fourroom' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-242-e296c1cbaef0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v004_003,SMOOTH_RADIUS), label = \"ops_norm_v4_003\",color='y')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPISODE_NUM\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mRATIO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmooth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpr_rewards_fourroom\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mEPISODE_NUM\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mRATIO\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mSMOOTH_RADIUS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"pr_v014_003\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPISODE_NUM\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mRATIO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmooth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtl_rewards_fourroom\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mEPISODE_NUM\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mRATIO\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mSMOOTH_RADIUS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ops_v014_003\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'g'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPISODE_NUM\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mRATIO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmooth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops_rewards_fourroom\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mEPISODE_NUM\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mRATIO\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mSMOOTH_RADIUS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"caps_v014_003\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pr_rewards_fourroom' is not defined"
     ]
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "RATIO = 4\n",
    "SMOOTH_RADIUS = 100\n",
    "sns.set(style = \"darkgrid\")\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(qlearn_rewards_fourroom[3][:EPISODE_NUM//RATIO], SMOOTH_RADIUS), label = \"qlearn_v003\", color='k')\n",
    "#plt.plot(range(EPISODE_NUM), smooth(qlearn_rewards_v5, 100), label = \"qlearn_v5\")\n",
    "\n",
    "#plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v000_003,SMOOTH_RADIUS), label = \"CAPS_v0_003\",color='b')\n",
    "#plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v001_003,SMOOTH_RADIUS), label = \"CAPS_v1_003\",color='m')\n",
    "#plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v004_003,SMOOTH_RADIUS), label = \"CAPS_v4_003\",color='y')\n",
    "\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(pr_rewards_fourroom[0][:EPISODE_NUM//RATIO],SMOOTH_RADIUS), label = \"pr_v014_003\",color='c')\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(tl_rewards_fourroom[:EPISODE_NUM//RATIO],SMOOTH_RADIUS), label = \"ops_v014_003\",color='g')\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_fourroom[:EPISODE_NUM//RATIO],SMOOTH_RADIUS), label = \"caps_v014_003\",color='r')\n",
    "#plt.plot(range(EPISODE_NUM//RATIO), smooth(trans_rewards[3][:EPISODE_NUM//RATIO],SMOOTH_RADIUS), label = \"oc_v014_003\",color='m')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.title(\"0-1000 episode-reward graph\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"rewards\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pr_rewards_v014_003' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-372-8aae36421dd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v004_003,SMOOTH_RADIUS), label = \"ops_norm_v4_003\",color='y')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPISODE_NUM\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mRATIO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmooth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpr_rewards_v014_003\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mSMOOTH_RADIUS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"pr_v014_003\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPISODE_NUM\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mRATIO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmooth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtl_rewards_v014_003\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mSMOOTH_RADIUS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ops_v014_003\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'g'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPISODE_NUM\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mRATIO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmooth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops_rewards_v014_003\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mSMOOTH_RADIUS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"caps_v014_003\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pr_rewards_v014_003' is not defined"
     ]
    }
   ],
   "source": [
    "#%matplotlib widget\n",
    "RATIO = 1\n",
    "SMOOTH_RADIUS = 50\n",
    "sns.set(style = \"darkgrid\")\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(qlearn_rewards_v003, SMOOTH_RADIUS), label = \"qlearn_v003\", color='k')\n",
    "#plt.plot(range(EPISODE_NUM), smooth(qlearn_rewards_v5, 100), label = \"qlearn_v5\")\n",
    "\n",
    "#plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v000_003,SMOOTH_RADIUS), label = \"CAPS_v0_003\",color='b')\n",
    "#plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v001_003,SMOOTH_RADIUS), label = \"CAPS_v1_003\",color='m')\n",
    "#plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v004_003,SMOOTH_RADIUS), label = \"CAPS_v4_003\",color='y')\n",
    "\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(pr_rewards_v014_003,SMOOTH_RADIUS), label = \"pr_v014_003\",color='c')\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(tl_rewards_v014_003,SMOOTH_RADIUS), label = \"ops_v014_003\",color='g')\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v014_003,SMOOTH_RADIUS), label = \"caps_v014_003\",color='r')\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(trans_rewards[3],SMOOTH_RADIUS), label = \"oc_v014_003\",color='m')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.title(\"0-4000 episode-reward graph\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"rewards\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RATIO = 4\n",
    "SMOOTH_RADIUS = 50\n",
    "sns.set(style = \"darkgrid\")\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(qlearn_rewards_v003[:EPISODE_NUM//RATIO], SMOOTH_RADIUS), label = \"qlearn_v003\", color='k')\n",
    "#plt.plot(range(EPISODE_NUM), smooth(qlearn_rewards_v5, 100), label = \"qlearn_v5\")\n",
    "\n",
    "#plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v000_003,SMOOTH_RADIUS), label = \"CAPS_v0_003\",color='b')\n",
    "#plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v001_003,SMOOTH_RADIUS), label = \"CAPS_v1_003\",color='m')\n",
    "#plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v004_003,SMOOTH_RADIUS), label = \"CAPS_v4_003\",color='y')\n",
    "\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(pr_rewards_v014_003[:EPISODE_NUM//RATIO],SMOOTH_RADIUS), label = \"pr_v014_003\",color='c')\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(tl_rewards_v014_003[:EPISODE_NUM//RATIO],SMOOTH_RADIUS), label = \"ops_v014_003\",color='g')\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v014_003[:EPISODE_NUM//RATIO],SMOOTH_RADIUS), label = \"caps_v014_003\",color='r')\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(trans_rewards[3][:EPISODE_NUM//RATIO],SMOOTH_RADIUS), label = \"oc_v014_003\",color='m')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.title(\"0-1000 episode-reward graph\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"rewards\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "RATIO = 1\n",
    "SMOOTH_RADIUS = 100\n",
    "sns.set(style = \"darkgrid\")\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_fre_v01_003[0],SMOOTH_RADIUS), label = 'ploicy_v001')\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_fre_v01_003[1],SMOOTH_RADIUS), label = 'ploicy_v004')\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_fre_v01_003[2],SMOOTH_RADIUS), label = 'target policy')\n",
    "plt.legend()\n",
    "plt.xlabel('episodes')\n",
    "plt.ylabel('frequency')\n",
    "plt.title(\"frequency of policy selection\")\n",
    "plt.show()\n",
    "#plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_fre_v04_003,SMOOTH_RADIUS), label = \"CAPS_v04_003\",color='g')\n",
    "#plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_fre_v14_003,SMOOTH_RADIUS), label = \"CAPS_v14_003\",color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RATIO = 10\n",
    "SMOOTH_RADIUS = 20\n",
    "\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(qlearn_rewards_v003[:EPISODE_NUM//RATIO], SMOOTH_RADIUS), label = \"qlearn_v003\", color='k')\n",
    "#plt.plot(range(EPISODE_NUM), smooth(qlearn_rewards_v5, 100), label = \"qlearn_v5\")\n",
    "\n",
    "#plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v000_003[:EPISODE_NUM//RATIO],SMOOTH_RADIUS), label = \"CAPS_v0_003\",color='b')\n",
    "#plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v001_003[:EPISODE_NUM//RATIO],SMOOTH_RADIUS), label = \"CAPS_v1_003\",color='m')\n",
    "#plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v004_003[:EPISODE_NUM//RATIO],SMOOTH_RADIUS), label = \"CAPS_v4_003\",color='y')\n",
    "\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v01_003[:EPISODE_NUM//RATIO],SMOOTH_RADIUS), label = \"caps_v01_003\",color='c')\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v04_003[:EPISODE_NUM//RATIO],SMOOTH_RADIUS), label = \"caps_v04_003\",color='g')\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v14_003[:EPISODE_NUM//RATIO],SMOOTH_RADIUS), label = \"caps_v14_003\",color='r')\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v014_003[:EPISODE_NUM//RATIO], SMOOTH_RADIUS), label = \"caps_v014_003\", color='navy')\n",
    "\n",
    "plt.title(\"first 400 episode-reward graph\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"rewards\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#ops_v1_003\n",
    "ops_episodes_length_v1_003, ops_penalties_v1_003,ops_rewards_v1_003, ops_q_table_v1_003 = transform(target_rl_algo= OPS_TL,\n",
    "                                                                                       episode_num = EPISODE_NUM,\n",
    "                                                                                       repeat_times = REPEAT_TIMES,\n",
    "                                                                                       target_task = \"Taxi-v003\",\n",
    "                                                                                       source_task = \"Taxi-v1\",\n",
    "                                                                                       policy_library = [[q_func] for q_func in qlearn_q_table_v1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#ops_v4_003\n",
    "ops_episodes_length_v4_003, ops_penalties_v4_003,ops_rewards_v4_003, ops_q_table_v4_003 = transform(target_rl_algo= OPS_TL,\n",
    "                                                                                       episode_num = EPISODE_NUM,\n",
    "                                                                                       repeat_times = REPEAT_TIMES,\n",
    "                                                                                       target_task = \"Taxi-v003\",\n",
    "                                                                                       source_task = \"Taxi-v4\",\n",
    "                                                                                       policy_library = [[q_func] for q_func in qlearn_q_table_v4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#ops_v14_003\n",
    "ops_episodes_length_v14_003, ops_penalties_v14_003,ops_rewards_v14_003, ops_q_table_v14_003 = transform(target_rl_algo= OPS_TL,\n",
    "                                                                                       episode_num = EPISODE_NUM,\n",
    "                                                                                       repeat_times = REPEAT_TIMES,\n",
    "                                                                                       target_task = \"Taxi-v003\",\n",
    "                                                                                       source_task = \"Taxi-v4\",\n",
    "                                                                                       policy_library = [[q_func1,q_func2 ] for q_func1,q_func2 in zip(qlearn_q_table_v1, qlearn_q_table_v4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#CAPS_v14_003\n",
    "CAPS_episodes_length_v14_003, ops__norm_penalties_v14_003,CAPS_rewards_v14_003, CAPS_q_table_v14_003 = transform(target_rl_algo= CAPS,\n",
    "                                                                                       episode_num = EPISODE_NUM,\n",
    "                                                                                       repeat_times = 5,\n",
    "                                                                                       target_task = \"Taxi-v003\",\n",
    "                                                                                       source_task = \"Taxi-v4\",\n",
    "                                                                                       policy_library = [[q_func1,q_func2 ] for q_func1,q_func2 in zip(qlearn_q_table_v1, qlearn_q_table_v4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "#CAPS_v14_003\n",
    "pl=[[q_func1,q_func2 ] for q_func1,q_func2 in zip(qlearn_q_table_v1, qlearn_q_table_v4)]\n",
    "ctuple = (0, 0.0049, 0.1, 0.2, 0.4,0.8, 1, 2, 4, 8, 16)\n",
    "ops_cnorm_episodes_length_v14_003,ops__cnorm_penalties_v14_003,ops_cnorm_rewards_v14_003,ops_cnorm_q_table_v14_003 = [0] * len(ctuple), [0]*len(ctuple),[0]*len(ctuple), [0]*len(ctuple)\n",
    "for i in range(len(ctuple)):\n",
    "    OPS_patial = functools.partial( CAPS, c = ctuple[i])\n",
    "    ops_cnorm_episodes_length_v14_003[i], ops__cnorm_penalties_v14_003[i],ops_cnorm_rewards_v14_003[i], ops_cnorm_q_table_v14_003[i] = transform(target_rl_algo= OPS_patial,\n",
    "               episode_num = EPISODE_NUM,\n",
    "               repeat_times = REPEAT_TIMES,\n",
    "               target_task = \"Taxi-v003\",\n",
    "               policy_library = pl)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style = \"darkgrid\")\n",
    "plt.plot(range(EPISODE_NUM), smooth(qlearn_rewards_v003, 100), label = \"qlearn_v003\", color='k')\n",
    "for i in range(len(ctuple)):\n",
    "    plt.plot(range(EPISODE_NUM), smooth(ops_cnorm_rewards_v14_003[i], 100), label = \"c = \"+str(ctuple[i]))\n",
    "    \n",
    "plt.title(\"episode-reward graph\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"rewards\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RATIO = 20\n",
    "SMOOTH_RADIUS = 200\n",
    "\n",
    "\n",
    "for i in range(len(ctuple)):\n",
    "    plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_cnorm_rewards_v14_003[i][:EPISODE_NUM//RATIO], SMOOTH_RADIUS), label = \"c = \"+str(ctuple[i]))\n",
    "    \n",
    "plt.title(\"reward-epsidoe graph using CAPS different c\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"rewards\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ctuple)):\n",
    "    print(f\"c = {ctuple[i]}, reward = {smooth(ops_cnorm_rewards_v14_003[i][:EPISODE_NUM//20], 200)[0]}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.rcParams['font.sans-serif'] = ['SimHei']\n",
    "matplotlib.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "reward = [smooth( ops_cnorm_rewards_v14_003[i][:EPISODE_NUM//20], 200)[0] for i in range(len(ctuple)) ]\n",
    "\"\"\"\n",
    "绘制水平条形图方法barh\n",
    "参数一：y轴\n",
    "参数二：x轴\n",
    "\"\"\"\n",
    "plt.barh(range(len(ctuple)), reward, height=0.7, color='steelblue', alpha=0.8)      # 从下往上画\n",
    "plt.yticks(range(len(ctuple)), [f\"c={str(ctuple[i])}\" for i in range(len(ctuple))])\n",
    "plt.xlim(15,20)\n",
    "plt.xlabel(\"average reward \")\n",
    "plt.title(\"average reward over first 200 episode under different c\")\n",
    "for x, y in enumerate(reward):\n",
    "    plt.text(y + 0.2, x - 0.1, '%s' % y)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "sns.set(style = \"darkgrid\")\n",
    "\n",
    "\n",
    "plt.plot(range(EPISODE_NUM), smooth(qlearn_rewards_v003, 100), label = \"qlearn_v003\", color='k')\n",
    "#plt.plot(range(EPISODE_NUM), smooth(qlearn_rewards_v5, 100), label = \"qlearn_v5\")\n",
    "\n",
    "#plt.plot(range(EPISODE_NUM), smooth(qlearn_rewards_v1_3,100), label = \"qlearn_v1_3\",color='c')\n",
    "#plt.plot(range(EPISODE_NUM), smooth(qlearn_rewards_v4_3,100), label = \"qlearn_v4_3\",color='g')\n",
    "\n",
    "#plt.plot(range(EPISODE_NUM), smooth(pr_rewards_v4_3, 100), label = \"pr_v4_3\")\n",
    "#plt.plot(range(EPISODE_NUM), smooth(pr_rewards_v1_3, 100), label = \"pr_v1_3\")\n",
    "\n",
    "plt.plot(range(EPISODE_NUM), smooth(ops_rewards_v4_003, 100), label = \"ops_v4_003\", color='r')\n",
    "plt.plot(range(EPISODE_NUM), smooth(ops_rewards_v1_003, 100), label = \"ops_v1_003\", color='b')\n",
    "\n",
    "plt.plot(range(EPISODE_NUM), smooth(ops_rewards_v14_003, 100), label = \"ops_v14_003\", color='y')\n",
    "plt.plot(range(EPISODE_NUM), smooth(CAPS_rewards_v14_003, 100), label = \"CAPS_v14_003\", color='navy')\n",
    "\n",
    "plt.title(\"episode-reward graph\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"rewards\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "RATIO = 8\n",
    "SMOOTH_RADIUS = 25\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(qlearn_rewards_v003[:EPISODE_NUM//RATIO], SMOOTH_RADIUS), label = \"qlearn_v003\", color='k')\n",
    "#plt.plot(range(EPISODE_NUM), smooth(qlearn_rewards_v5, 100), label = \"qlearn_v5\")\n",
    "\n",
    "#plt.plot(range(EPISODE_NUM), smooth(qlearn_rewards_v1_3,100), label = \"qlearn_v1_3\",color='c')\n",
    "#plt.plot(range(EPISODE_NUM), smooth(qlearn_rewards_v4_3,100), label = \"qlearn_v4_3\",color='g')\n",
    "\n",
    "#plt.plot(range(EPISODE_NUM), smooth(pr_rewards_v4_3, 100), label = \"pr_v4_3\")\n",
    "#plt.plot(range(EPISODE_NUM), smooth(pr_rewards_v1_3, 100), label = \"pr_v1_3\")\n",
    "\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v4_003[:EPISODE_NUM//RATIO], SMOOTH_RADIUS), label = \"ops_v4_003\", color='r')\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v1_003[:EPISODE_NUM//RATIO], SMOOTH_RADIUS), label = \"ops_v1_003\", color='b')\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v14_003[:EPISODE_NUM//RATIO], SMOOTH_RADIUS), label = \"ops_v14_003\", color='y')\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(CAPS_rewards_v14_003[:EPISODE_NUM//RATIO], SMOOTH_RADIUS), label = \"CAPS_v14_003\", color='navy')\n",
    "\n",
    "plt.title(\"episode-reward graph\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"rewards\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RATIO = 8\n",
    "SMOOTH_WINDOW = 20\n",
    "plt.plot(range(EPISODE_NUM//RATIO),smooth(qlearn_episodes_length_v003[:EPISODE_NUM//RATIO],SMOOTH_WINDOW), label = 'qlearn_v3')\n",
    "'''\n",
    "plt.plot(range(train_episodes),qlearn_episodes_length_v1_3, label = 'qlearn_v1_3')\n",
    "plt.plot(range(train_episodes),qlearn_episodes_length_v4_3, label = 'qlearn_v4_3')\n",
    "\n",
    "plt.plot(range(train_episodes),sarsa_episodes_length_v3, label = 'sarsa_v3')\n",
    "plt.plot(range(train_episodes),sarsa_episodes_length_v1_3, label = 'sarsa_v1_3')\n",
    "plt.plot(range(train_episodes),sarsa_episodes_length_v4_3, label = 'sarsa_v4_3')\n",
    "'''\n",
    "plt.plot(range(EPISODE_NUM//RATIO),smooth(ops_episodes_length_v1_003[:EPISODE_NUM//RATIO],SMOOTH_WINDOW), label = 'ops_v1_3')\n",
    "plt.plot(range(EPISODE_NUM//RATIO),smooth(ops_episodes_length_v4_003[:EPISODE_NUM//RATIO],SMOOTH_WINDOW), label = 'ops_v4_3')\n",
    "\n",
    "plt.title(\"episode-length graph\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"length\")\n",
    "plt.legend()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RATIO = 20\n",
    "SMOOTH_RADIUS = 20\n",
    "plt.plot(range(EPISODE_NUM//RATIO),smooth(qlearn_penalties_v03[:EPISODE_NUM//RATIO],SMOOTH_RADIUS), label = 'qlearn_v3')\n",
    "'''\n",
    "plt.plot(range(train_episodes),qlearn_episodes_length_v1_3, label = 'qlearn_v1_3')\n",
    "plt.plot(range(train_episodes),qlearn_episodes_length_v4_3, label = 'qlearn_v4_3')\n",
    "\n",
    "plt.plot(range(train_episodes),sarsa_episodes_length_v3, label = 'sarsa_v3')\n",
    "plt.plot(range(train_episodes),sarsa_episodes_length_v1_3, label = 'sarsa_v1_3')\n",
    "plt.plot(range(train_episodes),sarsa_episodes_length_v4_3, label = 'sarsa_v4_3')\n",
    "'''\n",
    "plt.plot(range(EPISODE_NUM//RATIO),smooth(ops_penalties_v1_03[:EPISODE_NUM//RATIO],SMOOTH_RADIUS), label = 'ops_v1_3')\n",
    "plt.plot(range(EPISODE_NUM//RATIO),smooth(ops_penalties_v4_03[:EPISODE_NUM//RATIO],SMOOTH_RADIUS), label = 'ops_v4_3')\n",
    "\n",
    "plt.title(\"episode-penalties graph\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"penalties\")\n",
    "plt.legend()\n",
    "plt.show\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def visit_all(dic):\n",
    "    for value in dic.values():\n",
    "        if value == 0:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "sum = 0\n",
    "times = 1000\n",
    "for i in range(times):    \n",
    "    dic = {i: 0 for i in range(100)}\n",
    "    while not visit_all(dic):\n",
    "        key = random.choice(range(len(dic)))\n",
    "        dic[key] = 1\n",
    "        sum += 1\n",
    "sum /= times\n",
    "print(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def div_add(n):\n",
    "    sum = 0\n",
    "    for i in range(1,n+1):\n",
    "        sum += 1/i\n",
    "        \n",
    "    return n * sum\n",
    "div_add(3000)/200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum = 0\n",
    "times = 1000\n",
    "for i in range(times):    \n",
    "    dic = {i: 0 for i in range(100)}\n",
    "    while dic[0] == 0:\n",
    "        key = random.choice(range(len(dic)))\n",
    "        dic[key] = 1\n",
    "        sum += 1\n",
    "sum /= times\n",
    "print(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_reuse(env, past_policy, train_episodes, fi = 1, mu = 0.95):\n",
    "        \n",
    "    # Hyper parameters\n",
    "    alpha = 0.05  #(alpha) is the learning rate (0<α≤1) \n",
    "    gamma = 0.95  # (gamma) is the discount factor (0≤γ≤1) \n",
    "    epsilon = 0.1 # the rate to act randomly in the epsilon-greedy alg for expolration in Q-learning\n",
    "    \n",
    "    # To plot learning curve\n",
    "    all_episodes_length = []\n",
    "    all_penalties = []\n",
    "    all_rewards = []\n",
    "\n",
    "    q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    " \n",
    "    past_table = copy.deepcopy(past_policy[0])\n",
    "    \n",
    "    \n",
    "    for i in range(train_episodes):\n",
    "        state = env.reset()\n",
    "\n",
    "        episode_penalties, episode_rewards, episode_length = 0, 0, 0\n",
    "        done = False\n",
    "        f = fi\n",
    "        while not done:\n",
    "            # epsilon greedy alg balancing exporation and exploitation\n",
    "            if random.uniform(0,1) < f:\n",
    "                action = arg_max(past_table[state])\n",
    "                #print(f\"f = {f} in episode {i} \\n\")\n",
    "            elif random.uniform(0,1) < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "                #print(\"random action \\n\")\n",
    "                \n",
    "            else:\n",
    "                action = arg_max(q_table[state])\n",
    "                #print(\"on the policy now \\n\")\n",
    "\n",
    "            # step to next state\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "\n",
    "            # update q-value\n",
    "            q_value = (1-alpha) * q_table[state, action] + alpha * (reward + gamma * np.max(q_table[next_state]))\n",
    "            q_table[state, action] = q_value\n",
    "\n",
    "            state = next_state\n",
    "            f = f*mu\n",
    "            #update data for learning curve\n",
    "            if reward == -10:\n",
    "                episode_penalties +=1\n",
    "\n",
    "            episode_rewards += reward\n",
    "            episode_length += 1\n",
    "\n",
    "        #record data for learning curve\n",
    "        all_episodes_length.append(episode_length)\n",
    "        all_penalties.append(episode_penalties)\n",
    "        all_rewards.append(episode_rewards)\n",
    "\n",
    "        #show training progress\n",
    "        if (i+1) % 100 == 0:\n",
    "            clear_output(wait=True)\n",
    "            print(f\"Episode {i}\")\n",
    "    print(\"Training finished \\n\")\n",
    "    \n",
    "    # return ndarray\n",
    "    all_episodes_length = np.array(all_episodes_length)\n",
    "    all_penalties = np.array(all_penalties)\n",
    "    all_rewards = np.array(all_rewards)\n",
    "    \n",
    "    return all_episodes_length, all_penalties, all_rewards, q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "sns.set(style = \"darkgrid\")\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(range(EPISODE_NUM), smooth(qlearn_rewards_v3, 100), label = \"qlearn_v3\", color='k')\n",
    "#plt.plot(range(EPISODE_NUM), smooth(qlearn_rewards_v5, 100), label = \"qlearn_v5\")\n",
    "\n",
    "#plt.plot(range(EPISODE_NUM), smooth(qlearn_rewards_v1_3,100), label = \"qlearn_v1_3\",color='c')\n",
    "#plt.plot(range(EPISODE_NUM), smooth(qlearn_rewards_v4_3,100), label = \"qlearn_v4_3\",color='g')\n",
    "\n",
    "#plt.plot(range(EPISODE_NUM), smooth(pr_rewards_v4_3, 100), label = \"pr_v4_3\")\n",
    "#plt.plot(range(EPISODE_NUM), smooth(pr_rewards_v1_3, 100), label = \"pr_v1_3\")\n",
    "\n",
    "plt.plot(range(EPISODE_NUM), smooth(ops_rewards_v4_3, 100), label = \"ops_v4_3\", color='r')\n",
    "plt.plot(range(EPISODE_NUM), smooth(ops_rewards_v1_3, 100), label = \"ops_v1_3\", color='b')\n",
    "\n",
    "plt.title(\"episode-reward graph\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"rewards\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "# training v03 from scratch\n",
    "\n",
    "qlearn_episodes_length_v03, qlearn_penalties_v03, qlearn_rewards_v03, qlearn_q_table_v03 = transform(target_rl_algo = q_learning,\n",
    "                                                                                     episode_num = EPISODE_NUM,  \n",
    "                                                                                     repeat_times = REPEAT_TIMES,\n",
    "                                                                                     target_task = \"Taxi-v03\",\n",
    "                                                                                     source_task = None\n",
    "                                                                                    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "# training v01 from scratch\n",
    "\n",
    "qlearn_episodes_length_v01, qlearn_penalties_v01, qlearn_rewards_v01, qlearn_q_table_v01 = transform(target_rl_algo = q_learning,\n",
    "                                                                                     episode_num = EPISODE_NUM,  \n",
    "                                                                                     repeat_times = REPEAT_TIMES,\n",
    "                                                                                     target_task = \"Taxi-v01\",\n",
    "                                                                                     source_task = None\n",
    "                                                                                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "# training v04 from scratch\n",
    "\n",
    "qlearn_episodes_length_v04, qlearn_penalties_v04, qlearn_rewards_v04, qlearn_q_table_v04 = transform(target_rl_algo = q_learning,\n",
    "                                                                                     episode_num = EPISODE_NUM,  \n",
    "                                                                                     repeat_times = REPEAT_TIMES,\n",
    "                                                                                     target_task = \"Taxi-v04\",\n",
    "                                                                                     source_task = None\n",
    "                                                                                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#ops_v04_03\n",
    "ops_episodes_length_v04_03, ops_penalties_v04_03,ops_rewards_v04_03, ops_q_table_v04_03 = transform(target_rl_algo= OPS_TL,\n",
    "                                                                                       episode_num = EPISODE_NUM,\n",
    "                                                                                       repeat_times = REPEAT_TIMES,\n",
    "                                                                                       target_task = \"Taxi-v03\",\n",
    "                                                                                       source_task = \"Taxi-v04\",\n",
    "                                                                                       policy_library = [[q_func] for q_func in qlearn_q_table_v04])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "#ops_v01_03\n",
    "ops_episodes_length_v01_03, ops_penalties_v01_03,ops_rewards_v01_03, ops_q_table_v01_03 = transform(target_rl_algo= OPS_TL,\n",
    "                                                                                       episode_num = EPISODE_NUM,\n",
    "                                                                                       repeat_times = REPEAT_TIMES,\n",
    "                                                                                       target_task = \"Taxi-v03\",\n",
    "                                                                                       source_task = \"Taxi-v01\",\n",
    "                                                                                       policy_library = [[q_func] for q_func in qlearn_q_table_v01])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(500), smooth(qlearn_rewards_v5[:500], 20), label = \"qlearn_v5\")\n",
    "#plt.plot(range(EPISODE_NUM), smooth(qlearn_rewards_v5, 100), label = \"qlearn_v5\")\n",
    "\n",
    "#plt.plot(range(EPISODE_NUM), smooth(qlearn_rewards_v1_3,100), label = \"qlearn_v1_3\")\n",
    "#plt.plot(range(EPISODE_NUM), smooth(qlearn_rewards_v4_3,100), label = \"qlearn_v4_3\")\n",
    "\n",
    "#plt.plot(range(EPISODE_NUM), smooth(pr_rewards_v4_3, 100), label = \"pr_v4_3\")\n",
    "#plt.plot(range(EPISODE_NUM), smooth(pr_rewards_v1_3, 100), label = \"pr_v1_3\")\n",
    "\n",
    "plt.plot(range(500), smooth(ops_rewards_v4_5[:500], 20), label = \"ops_v4_5\")\n",
    "plt.plot(range(500), smooth(ops_rewards_v1_5[:500], 20), label = \"ops_v1_5\")\n",
    "\n",
    "plt.title(\"episode-reward graph\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"rewards\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in qlearn_rewards_v3:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Taxi-v3\")\n",
    "r,_, _ = policy_evaluate(env, np.zeros([env.observation_space.n,env.action_space.n]), 1)\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(range(EPISODE_NUM//10 ),smooth(pr_rewards_v4_3[-EPISODE_NUM//10: ],100), label = 'pr_v4_3')\n",
    "plt.plot(range(EPISODE_NUM//10 ),smooth(pr_rewards_v4_3[-EPISODE_NUM//10: ],100), label = 'prql_v4_3')\n",
    "plt.plot(range(EPISODE_NUM//10), smooth(qlearn_rewards_v3[-EPISODE_NUM//10:], 100), label = \"qlearn_v3\")\n",
    "plt.plot(range(EPISODE_NUM//10), smooth(qlearn_rewards_v4[-EPISODE_NUM//10:], 100), label = \"qlearn_v4\")\n",
    "plt.plot(range(EPISODE_NUM//10), smooth(qlearn_rewards_v1_3[-EPISODE_NUM//10:],100), label = \"qlearn_v1_3\")\n",
    "\n",
    "plt.title(\"smoothed episode-reward graph: last {} episodes\".format(EPISODE_NUM//10))\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"reward\")\n",
    "plt.legend()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ops_episodes_length_v4_3, ops_penalties_v4_3,ops_rewards_v4_3, ops_q_table_v4_3 = transform(target_rl_algo= OPS_TL,\n",
    "                                                                                       episode_num = EPISODE_NUM,\n",
    "                                                                                       repeat_times = 10,\n",
    "                                                                                       target_task = \"Taxi-v3\",\n",
    "                                                                                       source_task = \"Taxi-v4\",\n",
    "                                                                                       policy_library = [[q_func] for q_func in qlearn_q_table_v4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "train_episodes = EPISODE_NUM\n",
    "\n",
    "plt.plot(range(train_episodes),smooth(qlearn_rewards_v3,100), 'k', label = 'qlearn_v3')\n",
    "plt.plot(range(train_episodes),smooth(qlearn_rewards_v1_3,100), label = 'qlearn_v1_3')\n",
    "plt.plot(range(train_episodes),smooth(qlearn_rewards_v4_3,100), label = 'qlearn_v4_3')\n",
    "\n",
    "\n",
    "plt.plot(range(train_episodes),smooth(pr_rewards_v1_3,100), label = 'pr_v1_3')\n",
    "plt.plot(range(train_episodes),smooth(pr_rewards_v4_3,100), label = 'pr_v4_3')\n",
    "\n",
    "\n",
    "plt.plot(range(train_episodes),smooth(ops_rewards_v1_3,100), label = 'ops_v1_3')\n",
    "plt.plot(range(train_episodes),smooth(ops_rewards_v4_3,100), label = 'ops_v4_3')\n",
    "\n",
    "plt.title(\"episode-reward graph\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"reward\")\n",
    "plt.legend()\n",
    "plt.show\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(train_episodes//10 ),smooth(qlearn_rewards_v3[-train_episodes//10: ],100), 'k',label = 'qlearn_v3')\n",
    "plt.plot(range(train_episodes//10 ),smooth(qlearn_rewards_v1_3[-train_episodes//10: ],100), label = 'qlearn_v1_3')\n",
    "plt.plot(range(train_episodes//10 ),smooth(qlearn_rewards_v4_3[-train_episodes//10: ],100), label = 'qlearn_v1_3')\n",
    "\n",
    "#plt.plot(range(train_episodes//10 ),smooth(sarsa_rewards_v3[-train_episodes//10: ],100), label = 'sarsa_v3')\n",
    "#plt.plot(range(train_episodes//10 ),smooth(sarsa_rewards_v1_3[-train_episodes//10: ],100), label = 'sarsa_v1_3')\n",
    "#plt.plot(range(train_episodes//10 ),smooth(sarsa_rewards_v4_3[-train_episodes//10: ],100), label = 'sarsa_v1_3')\n",
    "\n",
    "plt.plot(range(train_episodes//10 ),smooth(pr_rewards_v1_3[-train_episodes//10: ],100), label = 'pr_v1_3')\n",
    "plt.plot(range(train_episodes//10 ),smooth(pr_rewards_v4_3[-train_episodes//10: ],100), label = 'pr_v4_3')\n",
    "\n",
    "plt.plot(range(train_episodes//10 ),smooth(pr1_rewards_v1_3[-train_episodes//10: ],100), label = 'pr1_v1_3')\n",
    "plt.plot(range(train_episodes//10 ),smooth(pr1_rewards_v4_3[-train_episodes//10: ],100), label = 'pr1_v4_3')\n",
    "\n",
    "\n",
    "plt.title(\"smoothed episode-reward graph: last 1000 episodes\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"reward\")\n",
    "plt.legend()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(range(train_episodes//100),qlearn_rewards_v3[:train_episodes//100], label = 'qlearn_v3')\n",
    "plt.plot(range(train_episodes//100),qlearn_rewards_v1_3[:train_episodes//100], label = 'qlearn_v1_3')\n",
    "plt.plot(range(train_episodes//100),qlearn_rewards_v4_3[:train_episodes//100], label = 'qlearn_v4_3')\n",
    "\n",
    "plt.plot(range(train_episodes//100),sarsa_rewards_v3[:train_episodes//100], label = 'sarsa_v3')\n",
    "plt.plot(range(train_episodes//100),sarsa_rewards_v1_3[:train_episodes//100], label = 'sarsa_v1_3')\n",
    "plt.plot(range(train_episodes//100),sarsa_rewards_v4_3[:train_episodes//100], label = 'sarsa_v4_3')\n",
    "\n",
    "plt.title(\"episode-reward graph: first 1000 episodes\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"reward\")\n",
    "plt.legend()\n",
    "plt.show\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(train_episodes//100),smooth(qlearn_rewards_v3[:train_episodes//100],10), label = 'qlearn_v3')\n",
    "plt.plot(range(train_episodes//100),smooth(qlearn_rewards_v1_3[:train_episodes//100],10), label = 'qlearn_v1_3')\n",
    "plt.plot(range(train_episodes//100),smooth(qlearn_rewards_v4_3[:train_episodes//100],10), label = 'qlearn_v4_3')\n",
    "\n",
    "plt.plot(range(train_episodes//100),smooth(sarsa_rewards_v3[:train_episodes//100],10), label = 'sarsa_v3')\n",
    "plt.plot(range(train_episodes//100),smooth(sarsa_rewards_v1_3[:train_episodes//100],10), label = 'sarsa_v1_3')\n",
    "plt.plot(range(train_episodes//100),smooth(sarsa_rewards_v4_3[:train_episodes//100],10), label = 'sarsa_v4_3')\n",
    "\n",
    "plt.title(\"smoothed episode-reward graph: first 1000 episodes\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"reward\")\n",
    "plt.legend()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(train_episodes//100),smooth(qlearn_rewards_v3[:train_episodes//100],10), label = 'qlearn_v3')\n",
    "plt.plot(range(train_episodes//100),smooth(qlearn_rewards_v1_3[:train_episodes//100],10), label = 'qlearn_v1_3')\n",
    "plt.plot(range(train_episodes//100),smooth(qlearn_rewards_v4_3[:train_episodes//100],10), label = 'qlearn_v4_3')\n",
    "\n",
    "\n",
    "plt.title(\"q-learning smoothed episode-reward graph: first 1000 episodes\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"reward\")\n",
    "plt.legend()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(train_episodes//100),qlearn_episodes_length_v3[:train_episodes//100], label = 'qlearn_v3')\n",
    "plt.plot(range(train_episodes//100),qlearn_episodes_length_v1_3[:train_episodes//100], label = 'qlearn_v1_3')\n",
    "plt.plot(range(train_episodes//100),qlearn_episodes_length_v4_3[:train_episodes//100], label = 'qlearn_v4_3')\n",
    "\n",
    "plt.plot(range(train_episodes//100),sarsa_episodes_length_v3[:train_episodes//100], label = 'sarsa_v3')\n",
    "plt.plot(range(train_episodes//100),sarsa_episodes_length_v1_3[:train_episodes//100], label = 'sarsa_v1_3')\n",
    "plt.plot(range(train_episodes//100),sarsa_episodes_length_v4_3[:train_episodes//100], label = 'sarsa_v4_3')\n",
    "\n",
    "plt.title(\"episode-length graph: first 1000 episodes\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"length\")\n",
    "plt.legend()\n",
    "plt.show\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(train_episodes//100),smooth(qlearn_episodes_length_v3[:train_episodes//100],10), label = 'qlearn_v3')\n",
    "plt.plot(range(train_episodes//100),smooth(qlearn_episodes_length_v1_3[:train_episodes//100],10), label = 'qlearn_v1_3')\n",
    "plt.plot(range(train_episodes//100),smooth(qlearn_episodes_length_v4_3[:train_episodes//100],10), label = 'qlearn_v4_3')\n",
    "\n",
    "plt.plot(range(train_episodes//100),smooth(sarsa_episodes_length_v3[:train_episodes//100],10), label = 'sarsa_v3')\n",
    "plt.plot(range(train_episodes//100),smooth(sarsa_episodes_length_v1_3[:train_episodes//100],10), label = 'sarsa_v1_3')\n",
    "plt.plot(range(train_episodes//100),smooth(sarsa_episodes_length_v4_3[:train_episodes//100],10), label = 'sarsa_v4_3')\n",
    "\n",
    "plt.title(\"smoothed episode-length graph: first 1000 episodes\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"length\")\n",
    "plt.legend()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(train_episodes//100),smooth(qlearn_episodes_length_v3[:train_episodes//100],10), label = 'qlearn_v3')\n",
    "plt.plot(range(train_episodes//100),smooth(qlearn_episodes_length_v1_3[:train_episodes//100],10), label = 'qlearn_v1_3')\n",
    "plt.plot(range(train_episodes//100),smooth(qlearn_episodes_length_v4_3[:train_episodes//100],10), label = 'qlearn_v4_3')\n",
    "\n",
    "plt.title(\"q-learning smoothed episode-length graph: first 1000 episodes\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"length\")\n",
    "plt.legend()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(train_episodes//100),smooth(sarsa_episodes_length_v3[:train_episodes//100],10), label = 'sarsa_v3')\n",
    "plt.plot(range(train_episodes//100),smooth(sarsa_episodes_length_v1_3[:train_episodes//100],10), label = 'sarsa_v1_3')\n",
    "plt.plot(range(train_episodes//100),smooth(sarsa_episodes_length_v4_3[:train_episodes//100],10), label = 'sarsa_v4_3')\n",
    "\n",
    "plt.title(\"sarsa smoothed episode-length graph: first 1000 episodes\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"length\")\n",
    "plt.legend()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(train_episodes),qlearn_penalties_v3, label = 'qlearn_v3')\n",
    "plt.plot(range(train_episodes),qlearn_penalties_v1_3, label = 'qlearn_v1_3')\n",
    "plt.plot(range(train_episodes),qlearn_penalties_v4_3, label = 'qlearn_v4_3')\n",
    "\n",
    "plt.plot(range(train_episodes),sarsa_penalties_v3, label = 'sarsa_v3')\n",
    "plt.plot(range(train_episodes),sarsa_penalties_v1_3, label = 'sarsa_v1_3')\n",
    "plt.plot(range(train_episodes),sarsa_penalties_v4_3, label = 'sarsa_v4_3')\n",
    "\n",
    "plt.title(\"episode-penalties graph\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"penalties\")\n",
    "plt.legend()\n",
    "plt.show\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(train_episodes//100),qlearn_penalties_v3[:train_episodes//100], label = 'qlearn_v3')\n",
    "plt.plot(range(train_episodes//100),qlearn_penalties_v1_3[:train_episodes//100], label = 'qlearn_v1_3')\n",
    "plt.plot(range(train_episodes//100),qlearn_penalties_v4_3[:train_episodes//100], label = 'qlearn_v4_3')\n",
    "\n",
    "plt.plot(range(train_episodes//100),sarsa_penalties_v3[:train_episodes//100], label = 'sarsa_v3')\n",
    "plt.plot(range(train_episodes//100),sarsa_penalties_v1_3[:train_episodes//100], label = 'sarsa_v1_3')\n",
    "plt.plot(range(train_episodes//100),sarsa_penalties_v4_3[:train_episodes//100], label = 'sarsa_v4_3')\n",
    "\n",
    "plt.title(\"episode-penalties graph: first 1000 episodes\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"penalties\")\n",
    "plt.legend()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(train_episodes//100),smooth(qlearn_penalties_v3[:train_episodes//100],10), label = 'qlearn_v3')\n",
    "plt.plot(range(train_episodes//100),smooth(qlearn_penalties_v1_3[:train_episodes//100],10), label = 'qlearn_v1_3')\n",
    "plt.plot(range(train_episodes//100),smooth(qlearn_penalties_v4_3[:train_episodes//100],10), label = 'qlearn_v4_3')\n",
    "\n",
    "plt.plot(range(train_episodes//100),smooth(sarsa_penalties_v3[:train_episodes//100],10), label = 'sarsa_v3')\n",
    "plt.plot(range(train_episodes//100),smooth(sarsa_penalties_v1_3[:train_episodes//100],10), label = 'sarsa_v1_3')\n",
    "plt.plot(range(train_episodes//100),smooth(sarsa_penalties_v4_3[:train_episodes//100],10), label = 'sarsa_v4_3')\n",
    "\n",
    "plt.title(\"smoothed episode-penalties graph: first 1000 episodes\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"penalties \")\n",
    "plt.legend()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(train_episodes//100),smooth(qlearn_penalties_v3[:train_episodes//100],10), label = 'qlearn_v3')\n",
    "plt.plot(range(train_episodes//100),smooth(qlearn_penalties_v1_3[:train_episodes//100],10), label = 'qlearn_v1_3')\n",
    "plt.plot(range(train_episodes//100),smooth(qlearn_penalties_v4_3[:train_episodes//100],10), label = 'qlearn_v4_3')\n",
    "\n",
    "\n",
    "plt.title(\"q-learning smoothed episode-penalties graph: first 1000 episodes\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"penalties \")\n",
    "plt.legend()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(train_episodes//100),smooth(sarsa_penalties_v3[:train_episodes//100],10), label = 'sarsa_v3')\n",
    "plt.plot(range(train_episodes//100),smooth(sarsa_penalties_v1_3[:train_episodes//100],10), label = 'sarsa_v1_3')\n",
    "plt.plot(range(train_episodes//100),smooth(sarsa_penalties_v4_3[:train_episodes//100],10), label = 'sarsa_v4_3')\n",
    "\n",
    "plt.title(\"sarsa smoothed episode-penalties graph: first 1000 episodes\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"penalties \")\n",
    "plt.legend()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Exp1: similarity diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 799800\n",
      "Training finished\n",
      "CPU times: user 8min, sys: 1min 42s, total: 9min 42s\n",
      "Wall time: 8min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#qlearn_v004_003\n",
    "qlearn_episodes_length_v004_003, qlearn_penalties_v004_003,qlearn_rewards_v004_003, _, qlearn_q_table_v004_003 = transform(target_rl_algo= q_learning,\n",
    "                                                                                       episode_num = EPISODE_NUM,\n",
    "                                                                                       repeat_times = REPEAT_TIMES,\n",
    "                                                                                       target_task = \"Taxi-v003\",\n",
    "                                                                                       \n",
    "                                                                                       policy_library =  qlearn_q_table_v004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 799800\n",
      "Training finished\n",
      "CPU times: user 21min 52s, sys: 5min 52s, total: 27min 45s\n",
      "Wall time: 24min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#qlearn_v001_003\n",
    "qlearn_episodes_length_v001_003, qlearn_penalties_v001_003,qlearn_rewards_v001_003, _, qlearn_q_table_v001_003 = transform(target_rl_algo= q_learning,\n",
    "                                                                                       episode_num = EPISODE_NUM,\n",
    "                                                                                       repeat_times = REPEAT_TIMES,\n",
    "                                                                                       target_task = \"Taxi-v003\",\n",
    "                                                                                       \n",
    "                                                                                       policy_library =  qlearn_q_table_v001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 799800\n",
      "Training finished \n",
      "\n",
      "CPU times: user 23min 8s, sys: 8min 17s, total: 31min 26s\n",
      "Wall time: 27min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#sarsa_v001_003\n",
    "sarsa_episodes_length_v001_003, sarsa_penalties_v001_003,sarsa_rewards_v001_003, _, sarsa_q_table_v001_003 = transform(target_rl_algo= SARSA,\n",
    "                                                                                       episode_num = EPISODE_NUM,\n",
    "                                                                                       repeat_times = REPEAT_TIMES,\n",
    "                                                                                       target_task = \"Taxi-v003\",\n",
    "                                                                                       \n",
    "                                                                                       policy_library =  sarsa_q_table_v001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 799800\n",
      "Training finished \n",
      "\n",
      "CPU times: user 7min 31s, sys: 1min 47s, total: 9min 18s\n",
      "Wall time: 8min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#sarsa_v004_003\n",
    "sarsa_episodes_length_v004_003, sarsa_penalties_v004_003,sarsa_rewards_v004_003, _, sarsa_q_table_v004_003 = transform(target_rl_algo= SARSA,\n",
    "                                                                                       episode_num = EPISODE_NUM,\n",
    "                                                                                       repeat_times = REPEAT_TIMES,\n",
    "                                                                                       target_task = \"Taxi-v003\",\n",
    "                                                                                       \n",
    "                                                                                       policy_library =  sarsa_q_table_v004)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PRQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 799800\n",
      "Training finished \n",
      "\n",
      "CPU times: user 7min 4s, sys: 1min 9s, total: 8min 13s\n",
      "Wall time: 7min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#pr_v001_003\n",
    "pr_episodes_length_v001_003, pr_penalties_v001_003,pr_rewards_v001_003, _, pr_q_table_v001_003 = transform(target_rl_algo= prql,\n",
    "                                                                                       episode_num = EPISODE_NUM,\n",
    "                                                                                       repeat_times = REPEAT_TIMES,\n",
    "                                                                                       target_task = \"Taxi-v003\",\n",
    "                                                                                       \n",
    "                                                                                       policy_library =  qlearn_q_table_v001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 799800\n",
      "Training finished \n",
      "\n",
      "CPU times: user 7min 14s, sys: 1min 28s, total: 8min 42s\n",
      "Wall time: 7min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#pr_v004_003\n",
    "pr_episodes_length_v004_003, pr_penalties_v004_003,pr_rewards_v004_003, _, pr_q_table_v004_003 = transform(target_rl_algo= prql,\n",
    "                                                                                       episode_num = EPISODE_NUM,\n",
    "                                                                                       repeat_times = REPEAT_TIMES,\n",
    "                                                                                       target_task = \"Taxi-v003\",\n",
    "                                                                                       \n",
    "                                                                                       policy_library =  qlearn_q_table_v004)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OPS-TL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 306600\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#ops_v001_003\n",
    "ops_episodes_length_v001_003, ops_penalties_v001_003,ops_rewards_v001_003, _, ops_q_table_v001_003 = transform(target_rl_algo= OPS_TL,\n",
    "                                                                                       episode_num = EPISODE_NUM,\n",
    "                                                                                   repeat_times = REPEAT_TIMES,\n",
    "                                                                                       target_task = \"Taxi-v003\",\n",
    "                                                                                       \n",
    "                                                                                       policy_library =  [[qfunc] for qfunc in qlearn_q_table_v001])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#ops_v004_003\n",
    "ops_episodes_length_v004_003, ops_penalties_v004_003,ops_rewards_v004_003, _, ops_q_table_v004_003 = transform(target_rl_algo= OPS_TL,\n",
    "                                                                                       episode_num = EPISODE_NUM,\n",
    "                                                                                       repeat_times = REPEAT_TIMES,\n",
    "                                                                                       target_task = \"Taxi-v003\",\n",
    "                                                                                       \n",
    "                                                                                       policy_library =  [[qfunc] for qfunc in qlearn_q_table_v004])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CAPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#caps_v001_003\n",
    "caps_episodes_length_v001_003, caps_penalties_v001_003,caps_rewards_v001_003, _, caps_q_table_v001_003 = transform(target_rl_algo= CAPS,\n",
    "                                                                                       episode_num = EPISODE_NUM,\n",
    "                                                                                       repeat_times = REPEAT_TIMES,\n",
    "                                                                                       target_task = \"Taxi-v003\",\n",
    "                                                                                       \n",
    "                                                                                       policy_library =  [[qfunc] for qfunc in qlearn_q_table_v001])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#caps_v004_003\n",
    "caps_episodes_length_v004_003, caps_penalties_v004_003,caps_rewards_v004_003, _, caps_q_table_v004_003 = transform(target_rl_algo= CAPS,\n",
    "                                                                                       episode_num = EPISODE_NUM,\n",
    "                                                                                       repeat_times = REPEAT_TIMES,\n",
    "                                                                                       target_task = \"Taxi-v003\",\n",
    "                                                                                       \n",
    "                                                                                       policy_library =  [[qfunc] for qfunc in qlearn_q_table_v004])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"qlearn_NoTransfer\": qlearn_rewards_v003,\n",
    "        \"qlearn_v001_v003\": qlearn_rewards_v001_003,\n",
    "        \"qlearn_v004_v003\": qlearn_rewards_v004_003\n",
    "       }\n",
    "path = \"./similarity_diff_qlearn\" + time.strftime(\"%Y%m%d_%H\", time.localtime()) \n",
    "saveData(data,path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-8e7a3d57a5bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstyle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"darkgrid\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPISODE_NUM\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mRATIO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmooth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mEPISODE_NUM\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mRATIO\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSMOOTH_RADIUS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "import seaborn as sns\n",
    "RATIO = 1\n",
    "SMOOTH_RADIUS = 50\n",
    "sns.set(style = \"darkgrid\")\n",
    "\n",
    "for k,v in data.items():\n",
    "    plt.plot(range(EPISODE_NUM//RATIO), smooth(v[:EPISODE_NUM//RATIO], SMOOTH_RADIUS), label = k)\n",
    "\n",
    "#plt.plot(range(EPISODE_NUM//RATIO), smooth(qlearn_rewards_v003[:EPISODE_NUM//RATIO], SMOOTH_RADIUS), label = \"qlearn_NoTransfer\")\n",
    "#plt.plot(range(EPISODE_NUM//RATIO), smooth(qlearn_rewards_v001_003[:EPISODE_NUM//RATIO],SMOOTH_RADIUS), label = \"qlearn_v001_v003\")\n",
    "#plt.plot(range(EPISODE_NUM//RATIO), smooth(qlearn_rewards_v004_003[:EPISODE_NUM//RATIO], SMOOTH_RADIUS), label = \"qlearn_v001_v003\")\n",
    "\n",
    "plt.title(\"Q-learning: 0-4000 episode-reward graph\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"rewards\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"sarsa_NoTransfer\": sarsa_rewards_v003,\n",
    "        \"sarsa_v001_v003\": sarsa_rewards_v001_003,\n",
    "        \"sarsa_v004_v003\": sarsa_rewards_v004_003\n",
    "       }\n",
    "path = \"./similarity_diff_sarsa\" + time.strftime(\"%Y%m%d_%H\", time.localtime()) \n",
    "saveData(data,path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3a0a138bbe24d3e9d96d33ff1d92a71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "import seaborn as sns\n",
    "RATIO = 1\n",
    "SMOOTH_RADIUS = 50\n",
    "sns.set(style = \"darkgrid\")\n",
    "\n",
    "for k,v in data.items():\n",
    "    plt.plot(range(EPISODE_NUM//RATIO), smooth(v[:EPISODE_NUM//RATIO], SMOOTH_RADIUS), label = k)\n",
    "\n",
    "#plt.plot(range(EPISODE_NUM//RATIO), smooth(qlearn_rewards_v003[:EPISODE_NUM//RATIO], SMOOTH_RADIUS), label = \"qlearn_NoTransfer\")\n",
    "#plt.plot(range(EPISODE_NUM//RATIO), smooth(qlearn_rewards_v001_003[:EPISODE_NUM//RATIO],SMOOTH_RADIUS), label = \"qlearn_v001_v003\")\n",
    "#plt.plot(range(EPISODE_NUM//RATIO), smooth(qlearn_rewards_v004_003[:EPISODE_NUM//RATIO], SMOOTH_RADIUS), label = \"qlearn_v001_v003\")\n",
    "\n",
    "plt.title(\"SARSA: 0-4000 episode-reward graph\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"rewards\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Exp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 155800\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-abc5b5252d29>\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(target_rl_algo, episode_num, repeat_times, target_task, source_task, source_rl_algo, policy_library)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mepisodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpenalties\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mknowledge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_rl_algo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-c83c33ebef2c>\u001b[0m in \u001b[0;36mq_learning\u001b[0;34m(env, train_episodes, init_q_table)\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m200\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0;31m#evaluate policy for learning curve after each episode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                 \u001b[0mepisode_rewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_penalties\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_copy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_table\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEVALUATION_TIMES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0mall_episodes_length\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-4d8b12d466fa>\u001b[0m in \u001b[0;36mpolicy_evaluate\u001b[0;34m(env, policy, times)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAX_EPISODE_LEN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg_max\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;31m#update data for learning curve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gym/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gym/gym/envs/toy_text/discrete.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mtransitions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcategorical_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransitions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtransitions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gym/gym/envs/toy_text/discrete.py\u001b[0m in \u001b[0;36mcategorical_sample\u001b[0;34m(prob_n, np_random)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprob_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mcsprob_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcsprob_n\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mnp_random\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# training Taxi-v3 from scratch\n",
    "qlearn_episodes_length_v3, qlearn_penalties_v3, qlearn_rewards_v3, _, qlearn_q_table_v3 = transform(target_rl_algo = q_learning,\n",
    "                                                                                     episode_num = EPISODE_NUM, \n",
    "                                                                                     repeat_times = REPEAT_TIMES,\n",
    "                                                                                     target_task = \"Taxi-v3\",\n",
    "                                                                                     source_task = None\n",
    "                                                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 799800\n",
      "Training finished\n",
      "CPU times: user 8min 48s, sys: 1min 21s, total: 10min 9s\n",
      "Wall time: 9min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# training Taxi-v1 from scratch\n",
    "qlearn_episodes_length_v1, qlearn_penalties_v1, qlearn_rewards_v1,_, qlearn_q_table_v1 = transform(target_rl_algo = q_learning,\n",
    "                                                                                     episode_num = EPISODE_NUM, \n",
    "                                                                                     repeat_times = REPEAT_TIMES,\n",
    "                                                                                     target_task = \"Taxi-v1\",\n",
    "                                                                                     source_task = None\n",
    "                                                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 799800\n",
      "Training finished\n",
      "CPU times: user 9min 6s, sys: 1min 13s, total: 10min 19s\n",
      "Wall time: 9min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# training Taxi-v4 from scratch\n",
    "qlearn_episodes_length_v4, qlearn_penalties_v4, qlearn_rewards_v4,_, qlearn_q_table_v4 = transform(target_rl_algo = q_learning,\n",
    "                                                                                     episode_num = EPISODE_NUM, \n",
    "                                                                                     repeat_times = REPEAT_TIMES,\n",
    "                                                                                     target_task = \"Taxi-v4\",\n",
    "                                                                                     source_task = None\n",
    "                                                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 799800\n",
      "Training finished\n",
      "CPU times: user 9min 5s, sys: 1min 23s, total: 10min 28s\n",
      "Wall time: 9min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# training Taxi-v03 from scratch\n",
    "qlearn_episodes_length_v03, qlearn_penalties_v03, qlearn_rewards_v03, _, qlearn_q_table_v03 = transform(target_rl_algo = q_learning,\n",
    "                                                                                     episode_num = EPISODE_NUM, \n",
    "                                                                                     repeat_times = REPEAT_TIMES,\n",
    "                                                                                     target_task = \"Taxi-v03\",\n",
    "                                                                                     source_task = None\n",
    "                                                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 799800\n",
      "Training finished\n",
      "CPU times: user 8min 48s, sys: 1min 21s, total: 10min 9s\n",
      "Wall time: 9min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# training Taxi-v01 from scratch\n",
    "qlearn_episodes_length_v01, qlearn_penalties_v01, qlearn_rewards_v01,_, qlearn_q_table_v01 = transform(target_rl_algo = q_learning,\n",
    "                                                                                     episode_num = EPISODE_NUM, \n",
    "                                                                                     repeat_times = REPEAT_TIMES,\n",
    "                                                                                     target_task = \"Taxi-v01\",\n",
    "                                                                                     source_task = None\n",
    "                                                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 799800\n",
      "Training finished\n",
      "CPU times: user 9min 6s, sys: 1min 13s, total: 10min 19s\n",
      "Wall time: 9min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# training Taxi-v04 from scratch\n",
    "qlearn_episodes_length_v04, qlearn_penalties_v04, qlearn_rewards_v04,_, qlearn_q_table_v04 = transform(target_rl_algo = q_learning,\n",
    "                                                                                     episode_num = EPISODE_NUM, \n",
    "                                                                                     repeat_times = REPEAT_TIMES,\n",
    "                                                                                     target_task = \"Taxi-v04\",\n",
    "                                                                                     source_task = None\n",
    "                                                                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Exp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 154400\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-9fc5cc68f80b>\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(target_rl_algo, episode_num, repeat_times, target_task, source_task, source_rl_algo, policy_library)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mpast_policies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_library\u001b[0m \u001b[0;31m# policy_library = [ [[policy11][policy21]...] [[policy12][policy22]...]...]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtarget_rl_algo\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mCAPS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m                 \u001b[0mepisodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpenalties\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfrequency\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mknowledge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_rl_algo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_episodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast_policies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mepisodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpenalties\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mknowledge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_rl_algo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_episodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast_policies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-0307c0ff7df1>\u001b[0m in \u001b[0;36mCAPS\u001b[0;34m(env, train_episodes, rng, past_policies)\u001b[0m\n\u001b[1;32m     53\u001b[0m             episode_penalties, episode_rewards, episode_length,i = policy_reuse(timesteps, env, q_table, state, i,\n\u001b[1;32m     54\u001b[0m                                                                               \u001b[0mall_episodes_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mall_penalties\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mall_rewards\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m                                                                               past_policy = past_policies[j])\n\u001b[0m\u001b[1;32m     56\u001b[0m             \u001b[0mpolicy_expect\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpolicy_expect\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpolicy_times\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mepisode_rewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mpolicy_times\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mpolicy_times\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-5d9eb6ae084e>\u001b[0m in \u001b[0;36mpolicy_reuse\u001b[0;34m(timesteps, env, q_table, initial_state, i, all_episodes_length, all_penalties, all_rewards, past_policy, fi, mu, alpha, gamma)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# update q-value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mq_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mq_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mq_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mamax\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spinningup/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mamax\u001b[0;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2704\u001b[0m     \"\"\"\n\u001b[1;32m   2705\u001b[0m     return _wrapreduction(a, np.maximum, 'max', axis, None, out,\n\u001b[0;32m-> 2706\u001b[0;31m                           keepdims=keepdims, initial=initial, where=where)\n\u001b[0m\u001b[1;32m   2707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spinningup/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#caps_v014_003\n",
    "caps_episodes_length_v014_003, caps_penalties_v014_003,caps_rewards_v014_003,caps_fre_v014_003, caps_q_table_v014_003 = transform(target_rl_algo= CAPS,\n",
    "                                                                                       episode_num = EPISODE_NUM,\n",
    "                                                                                       repeat_times = REPEAT_TIMES,\n",
    "                                                                                       target_task = \"Taxi-v003\",\n",
    "                                                                                      \n",
    "                                                                                       policy_library = [[q_func1,q_func2,q_func3 ] for q_func1,q_func2,q_func3 in zip(qlearn_q_table_v000, qlearn_q_table_v001, qlearn_q_table_v004)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 's_table' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-9fc5cc68f80b>\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(target_rl_algo, episode_num, repeat_times, target_task, source_task, source_rl_algo, policy_library)\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0mepisodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpenalties\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfrequency\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mknowledge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_rl_algo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_episodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast_policies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m                 \u001b[0mepisodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpenalties\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mknowledge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_rl_algo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_episodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast_policies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;31m#Situation 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0msource_task\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-720e1c574ecd>\u001b[0m in \u001b[0;36mprql\u001b[0;34m(env, train_episodes, rng, past_policy)\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m200\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                 \u001b[0;31m#evaluate policy for learning curve after each episode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m                 \u001b[0mepisode_rewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_penalties\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_copy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_table\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEVALUATION_TIMES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mall_episodes_length\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 's_table' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#pr_v014_003\n",
    "pr_episodes_length_v014_003, pr_penalties_v014_003,pr_rewards_v014_003, _, pr_q_table_v014_003 = transform(target_rl_algo= prql,\n",
    "                                                                                       episode_num = EPISODE_NUM,\n",
    "                                                                                       repeat_times = REPEAT_TIMES,\n",
    "                                                                                       target_task = \"Taxi-v003\",\n",
    "                                                                                                                                                                            policy_library =  qlearn_q_table_v004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 112000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-9fc5cc68f80b>\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(target_rl_algo, episode_num, repeat_times, target_task, source_task, source_rl_algo, policy_library)\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0mepisodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpenalties\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfrequency\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mknowledge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_rl_algo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_episodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast_policies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m                 \u001b[0mepisodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpenalties\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mknowledge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_rl_algo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_episodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast_policies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;31m#Situation 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0msource_task\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-50-0c97c6ab7cc8>\u001b[0m in \u001b[0;36mOPS_TL\u001b[0;34m(env, train_episodes, rng, past_policies)\u001b[0m\n\u001b[1;32m     50\u001b[0m             episode_penalties, episode_rewards, episode_length,i = policy_reuse(timesteps, env, q_table, state, i,\n\u001b[1;32m     51\u001b[0m                                                                               \u001b[0mall_episodes_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mall_penalties\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mall_rewards\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                                                                               past_policy = past_policies[j])\n\u001b[0m\u001b[1;32m     53\u001b[0m             \u001b[0mpolicy_expect\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpolicy_expect\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpolicy_times\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mepisode_rewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mpolicy_times\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mpolicy_times\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-5d9eb6ae084e>\u001b[0m in \u001b[0;36mpolicy_reuse\u001b[0;34m(timesteps, env, q_table, initial_state, i, all_episodes_length, all_penalties, all_rewards, past_policy, fi, mu, alpha, gamma)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mepisode_penalties\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_rewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0m_env_copy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtimesteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mfi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spinningup/lib/python3.6/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    178\u001b[0m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_reconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;31m# If is its own copy, don't memoize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spinningup/lib/python3.6/copy.py\u001b[0m in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdeep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__setstate__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spinningup/lib/python3.6/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spinningup/lib/python3.6/copy.py\u001b[0m in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0mmemo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spinningup/lib/python3.6/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    178\u001b[0m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_reconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;31m# If is its own copy, don't memoize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spinningup/lib/python3.6/copy.py\u001b[0m in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdeep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__setstate__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spinningup/lib/python3.6/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spinningup/lib/python3.6/copy.py\u001b[0m in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0mmemo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spinningup/lib/python3.6/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spinningup/lib/python3.6/copy.py\u001b[0m in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0mmemo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spinningup/lib/python3.6/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spinningup/lib/python3.6/copy.py\u001b[0m in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0mmemo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spinningup/lib/python3.6/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spinningup/lib/python3.6/copy.py\u001b[0m in \u001b[0;36m_deepcopy_list\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0mmemo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0mappend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m         \u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#ops_v014_003\n",
    "tl_episodes_length_v014_003, tl_penalties_v014_003,tl_rewards_v014_003,tl_fre_v014_003, tl_q_table_v014_003 = transform(target_rl_algo= OPS_TL,\n",
    "                                                                                       episode_num = EPISODE_NUM,\n",
    "                                                                                       repeat_times = REPEAT_TIMES,\n",
    "                                                                                       target_task = \"Taxi-v003\",\n",
    "                                                                                      \n",
    "                                                                                       policy_library = [[q_func1,q_func2,q_func3 ] for q_func1,q_func2,q_func3 in zip(qlearn_q_table_v000, qlearn_q_table_v001, qlearn_q_table_v004)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "taxiEnvs = [gym.make(\"Taxi-v000\"),gym.make(\"Taxi-v001\"),None,gym.make(\"Taxi-v003\"),gym.make(\"Taxi-v004\")]\n",
    "for i in [0,1,3,4]:\n",
    "    _, _ ,nrewards[i] ,option_policies[i], option_terminations[i], policy_over_options[i],critic[i]= option_critic( taxiEnvs[i], 4000, noptions=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "source_lib = [0,1,4]\n",
    "_,_, trans_rewards_012, _, _, _,_= option_critic( four_room_envs[3], 4000, [option_policies[i][0] for i in source_lib], noptions=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(run_times, q_table = None ,task = \"Taxi-v003\", mode = 'learning agent', show_frame = True):\n",
    "    \"\"\"Evaluate agent's performance after policy updates\"\"\"\n",
    "    env = gym.make(task)\n",
    "    if q_table is None:\n",
    "        list_length = 1\n",
    "    else:\n",
    "        list_length = len(q_table)\n",
    "    all_epochs, all_penalties, all_rewards, all_success_rate =  np.zeros(list_length), np.zeros(list_length),np.zeros(list_length),np.zeros(list_length)\n",
    "    all_epochs_std, all_penalties_std, all_rewards_std = np.zeros(list_length), np.zeros(list_length),np.zeros(list_length)\n",
    "    \n",
    "    \n",
    "        \n",
    "    for i in range( list_length ):\n",
    "        table_epochs, table_penalties, table_rewards = np.zeros(run_times), np.zeros(run_times),np.zeros(run_times)\n",
    "        if q_table is not None:\n",
    "            table = q_table[i]\n",
    "        success = 0\n",
    "        for j in range(run_times):\n",
    "            state = env.reset()\n",
    "            epochs, penalties, reward, sum_reward = 0, 0, 0, 0\n",
    "\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "                if mode == 'learning agent':\n",
    "                    action = arg_max(table[state])\n",
    "                elif mode == 'random agent':\n",
    "                    action = np.random.randint(0,env.action_space.n)\n",
    "                state, reward, done, info = env.step(action)\n",
    "\n",
    "                if reward == -10:\n",
    "                    penalties += 1\n",
    "                elif reward == 20:\n",
    "                    success += 1\n",
    "\n",
    "                epochs += 1\n",
    "                sum_reward += reward\n",
    "                if show_frame:\n",
    "                    show_frames(env,j, epochs,sum_reward )\n",
    "            table_penalties[j] = penalties\n",
    "            table_epochs[j] = epochs\n",
    "            table_rewards[j] = sum_reward\n",
    "\n",
    "        all_success_rate[i] = success/run_times    \n",
    "        all_epochs[i], all_penalties[i], all_rewards[i]  = np.mean(table_epochs), np.mean(table_penalties), np.mean(table_rewards)\n",
    "        all_epochs_std[i], all_penalties_std[i], all_rewards_std[i] = np.std(table_epochs), np.std(table_penalties), np.std(table_rewards)\n",
    "      \n",
    "    print(f\"Results over {run_times} evaluating episodes:\")\n",
    "    print(f\"Success rate : {np.mean(all_success_rate)}\")\n",
    "    print(f\"Average  episode length : {np.mean(all_epochs)} ± {np.mean(all_epochs_std)}\")\n",
    "    print(f\"Average penalties per episode: {np.mean(all_penalties)} ± {np.mean(all_penalties_std)}\")\n",
    "    print(f\"Average rewards per episode: {np.mean(all_rewards)} ± {np.mean(all_rewards_std)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def op_evaluate(run_times,option_policies, option_terminations, policy_over_options, q_table = None ,task = \"Taxi-v003\", mode = 'learning agent'):\n",
    "    \"\"\"Evaluate agent's performance after policy updates\"\"\"\n",
    "    env = gym.make(task)\n",
    "    if q_table is None:\n",
    "        list_length = 1\n",
    "    else:\n",
    "        list_length = len(q_table)\n",
    "    all_epochs, all_penalties, all_rewards, all_success_rate =  np.zeros(list_length), np.zeros(list_length),np.zeros(list_length),np.zeros(list_length)\n",
    "    all_epochs_std, all_penalties_std, all_rewards_std = np.zeros(list_length), np.zeros(list_length),np.zeros(list_length)\n",
    "    \n",
    "    \n",
    "        \n",
    "    for i in range( list_length ):\n",
    "        table_epochs, table_penalties, table_rewards = np.zeros(run_times), np.zeros(run_times),np.zeros(run_times)\n",
    "        if q_table is not None:\n",
    "            table = q_table[i]\n",
    "        success = 0\n",
    "        for j in range(run_times):\n",
    "            state = env.reset()\n",
    "            epochs, penalties, reward, sum_reward = 0, 0, 0, 0\n",
    "\n",
    "            done = False\n",
    "            option = policy_over_options.evaluate(state)\n",
    "            while not done:\n",
    "                if mode == 'learning agent':\n",
    "                    if option_terminations[option].sample(state):\n",
    "                        option = policy_over_options.evaluate(state)\n",
    "\n",
    "                    action = option_policies[option].evaluate(state)\n",
    "                    \n",
    "                    \n",
    "                elif mode == 'random agent':\n",
    "                    action = np.random.randint(0,env.action_space.n)\n",
    "                state, reward, done, info = env.step(action)\n",
    "\n",
    "                if reward == -10:\n",
    "                    penalties += 1\n",
    "                elif reward == 20:\n",
    "                    success += 1\n",
    "\n",
    "                epochs += 1\n",
    "                sum_reward += reward\n",
    "                show_frames(env,j,epochs,sum_reward )\n",
    "            table_penalties[j] = penalties\n",
    "            table_epochs[j] = epochs\n",
    "            table_rewards[j] = sum_reward\n",
    "\n",
    "        all_success_rate[i] = success/run_times    \n",
    "        all_epochs[i], all_penalties[i], all_rewards[i]  = np.mean(table_epochs), np.mean(table_penalties), np.mean(table_rewards)\n",
    "        all_epochs_std[i], all_penalties_std[i], all_rewards_std[i] = np.std(table_epochs), np.std(table_penalties), np.std(table_rewards)\n",
    "      \n",
    "    print(f\"Results after {run_times} runs:\")\n",
    "    print(f\"Success rate : {np.mean(all_success_rate)}\")\n",
    "    print(f\"Average  episode length : {np.mean(all_epochs)} ± {np.mean(all_epochs_std)}\")\n",
    "    print(f\"Average penalties per episode: {np.mean(all_penalties)} ± {np.mean(all_penalties_std)}\")\n",
    "    print(f\"Average rewards per episode: {np.mean(all_rewards)} ± {np.mean(all_rewards_std)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 79800\n",
      "Training finished\n",
      "CPU times: user 12.3 s, sys: 675 ms, total: 13 s\n",
      "Wall time: 12.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#ops_v0_003\n",
    "ops_episodes_length_v0_003, ops_penalties_v0_003,ops_rewards_v0_003,ops_fre_v0_003, ops_q_table_v0_003 = transform(target_rl_algo= CAPS,\n",
    "                                                                                       episode_num = EPISODE_NUM//10,\n",
    "                                                                                       repeat_times = REPEAT_TIMES,\n",
    "                                                                                       target_task = \"Taxi-v003\",\n",
    "                                                                                      \n",
    "                                                                                       policy_library = [[q_func1] for q_func1 in qlearn_q_table_v000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 79800\n",
      "Training finished\n",
      "CPU times: user 13 s, sys: 527 ms, total: 13.5 s\n",
      "Wall time: 13.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#ops_v1_003\n",
    "ops_episodes_length_v1_003, ops_penalties_v1_003,ops_rewards_v1_003,ops_fre_v1_003, ops_q_table_v1_003 = transform(target_rl_algo= CAPS,\n",
    "                                                                                       episode_num = EPISODE_NUM//10,\n",
    "                                                                                       repeat_times = REPEAT_TIMES,\n",
    "                                                                                       target_task = \"Taxi-v003\",\n",
    "                                                                                      \n",
    "                                                                                       policy_library = [[q_func1] for q_func1 in qlearn_q_table_v001])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 399\n",
      "Training finished \n",
      "\n",
      "CPU times: user 2.21 s, sys: 106 µs, total: 2.21 s\n",
      "Wall time: 2.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pr_episodes_length_v1_003, pr_penalties_v1_003,pr_rewards_v1_003, _, pr_q_table_v1_003 = transform(target_rl_algo= prql,\n",
    "                                                                                       episode_num = EPISODE_NUM//10,\n",
    "                                                                                       repeat_times = REPEAT_TIMES,\n",
    "                                                                                       target_task = \"Taxi-v003\",\n",
    "                                                                                       \n",
    "                                                                                       policy_library =  qlearn_q_table_v001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 79800\n",
      "Training finished\n",
      "CPU times: user 3.7 s, sys: 455 ms, total: 4.15 s\n",
      "Wall time: 3.81 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "qlearn_episodes_length_v3, qlearn_penalties_v3, qlearn_rewards_v3, _, qlearn_q_table_v3 = transform(target_rl_algo = q_learning,\n",
    "                                                                                     episode_num = EPISODE_NUM//10, \n",
    "                                                                                     repeat_times = 1,\n",
    "                                                                                     target_task = \"Taxi-v003\",\n",
    "                                                                                     source_task = None\n",
    "                                                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56d5b099cde549529106f6714c0b7732",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show>"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "RATIO = 10\n",
    "SMOOTH_RADIUS = 20\n",
    "plt.plot(range(EPISODE_NUM//RATIO),smooth(qlearn_rewards_v3[:EPISODE_NUM//RATIO],SMOOTH_RADIUS), label = 'qlearn_v3')\n",
    "\n",
    "plt.plot(range(EPISODE_NUM//RATIO),smooth(pr_rewards_v1_003[:EPISODE_NUM//RATIO],SMOOTH_RADIUS), label = 'pr_v1_3')\n",
    "plt.plot(range(EPISODE_NUM//RATIO),smooth(ops_rewards_v1_003[:EPISODE_NUM//RATIO],SMOOTH_RADIUS), label = 'ops_v1_3')\n",
    "\n",
    "plt.title(\"episode-penalties graph\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"penalties\")\n",
    "plt.legend()\n",
    "plt.show\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results over 1000 evaluating episodes:\n",
      "Success rate : 0.779\n",
      "Average  episode length : 84.576 ± 75.92020958875179\n",
      "Average penalties per episode: 0.0 ± 0.0\n",
      "Average rewards per episode: 15.58 ± 8.298409486160585\n"
     ]
    }
   ],
   "source": [
    "evaluate(1000,qlearn_q_table_v3,show_frame = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "episode 3  step 6  rewards=20\n",
      "Results over 3 evaluating episodes:\n",
      "Success rate : 1.0\n",
      "Average  episode length : 17.0 ± 9.41629792788369\n",
      "Average penalties per episode: 0.0 ± 0.0\n",
      "Average rewards per episode: 20.0 ± 0.0\n"
     ]
    }
   ],
   "source": [
    "evaluate(3,ops_q_table_v1_003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results over 1000 evaluating episodes:\n",
      "Success rate : 0.988\n",
      "Average  episode length : 18.847 ± 25.006750908504685\n",
      "Average penalties per episode: 0.0 ± 0.0\n",
      "Average rewards per episode: 19.76 ± 2.1777052142105915\n"
     ]
    }
   ],
   "source": [
    "evaluate(3,pr_q_table_v1_003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results over 1000 evaluating episodes:\n",
      "Success rate : 0.706\n",
      "Average  episode length : 85.098 ± 82.33279052722554\n",
      "Average penalties per episode: 0.0 ± 0.0\n",
      "Average rewards per episode: 14.12 ± 9.111838453352869\n"
     ]
    }
   ],
   "source": [
    "evaluate(1000,pr_q_table_v1_003,show_frame = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(1000, mode = 'random agent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# training Taxi-v3 reusing policy from Taxi-v1\n",
    "pr1_rewards_v1_3 = np.zeros(10000)\n",
    "for i in range(10):\n",
    "    pr1_episodes_length_v1_3, pr1_penalties_v1_3,rewards, pr1_q_table_v1_3 = prql(gym.make(\"Taxi-v3\"), qlearn_q_table_v1[i], 10000)\n",
    "    pr1_rewards_v1_3 += rewards\n",
    "pr1_rewards_v1_3/=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# training Taxi-v3 reusing policy from Taxi-v1\n",
    "pr1_rewards_v4_3 = np.zeros(10000)\n",
    "for i in range(10):\n",
    "    pr1_episodes_length_v4_3, pr1_penalties_v4_3,rewards, pr1_q_table_v4_3 = prql(gym.make(\"Taxi-v3\"), qlearn_q_table_v4[i], 10000)\n",
    "    pr1_rewards_v4_3 += rewards\n",
    "pr1_rewards_v4_3/=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "env = gym.make(\"Taxi-v4\")\n",
    "# training Taxi-v4 from scratch\n",
    "lprofiler = LineProfiler(q_learning)\n",
    "lprofiler.run('qlearn_episodes_length_v41, qlearn_penalties_v41, qlearn_rewards_v41, qlearn_q_table_v41 = q_learning(env,np.zeros([env.observation_space.n,env.action_space.n]),EPISODE_NUM)')\n",
    "lprofiler.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spinningup",
   "language": "python",
   "name": "spinningup"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
