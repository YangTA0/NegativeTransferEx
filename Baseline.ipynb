{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import copy\n",
    "import time\n",
    "import pandas as pd\n",
    "from line_profiler import LineProfiler\n",
    "from option_critic.utils import *\n",
    "from matplotlib import pyplot as plt\n",
    "from option_critic.fourrooms_copy import FourRooms\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "ename": "DeprecatedEnv",
     "evalue": "Env Taxi-v01 not found (valid versions include ['Taxi-v3', 'Taxi-v0', 'Taxi-v1', 'Taxi-v4', 'Taxi-v00', 'Taxi-v04', 'Taxi-v000', 'Taxi-v001', 'Taxi-v003', 'Taxi-v004'])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/gym/gym/envs/registration.py\u001b[0m in \u001b[0;36mspec\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_specs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Taxi-v01'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mDeprecatedEnv\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-205-9dbe656dc23b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTaxi_v00\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTaxi_v01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTaxi_v03\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTaxi_v04\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTaxi_v000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTaxi_v001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTaxi_v003\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTaxi_v004\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Taxi-v01\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \"\"\"\n",
      "\u001b[0;32m~/gym/gym/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(id, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gym/gym/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(self, path, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Making new env: %s'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;31m# We used to have people override _reset/_step rather than\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gym/gym/envs/registration.py\u001b[0m in \u001b[0;36mspec\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    127\u001b[0m                              if env_name == valid_env_spec._env_name]\n\u001b[1;32m    128\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmatching_envs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDeprecatedEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Env {} not found (valid versions include {})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatching_envs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnregisteredEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No registered env with id: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDeprecatedEnv\u001b[0m: Env Taxi-v01 not found (valid versions include ['Taxi-v3', 'Taxi-v0', 'Taxi-v1', 'Taxi-v4', 'Taxi-v00', 'Taxi-v04', 'Taxi-v000', 'Taxi-v001', 'Taxi-v003', 'Taxi-v004'])"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#[env.id for env in gym.envs.registry.all() if env.id.startswith('Taxi')]\n",
    "import Taxi_v0, Taxi_v1, Taxi_v4\n",
    "import Taxi_v00, Taxi_v01, Taxi_v03, Taxi_v04\n",
    "import Taxi_v000,Taxi_v001,Taxi_v003,Taxi_v004\n",
    "env = gym.make(\"Taxi-v01\")\n",
    "env.render()\n",
    "\"\"\"\n",
    "Here's our restructured problem statement (from Gym docs):\n",
    "\n",
    "\"There are 4 locations (labeled by different letters), and our job is to pick up the passenger \n",
    "at one location and drop him off at another. We receive +20 points for a successful drop-off and \n",
    "lose 1 point for every time-step it takes. There is also a 10 point penalty for illegal pick-up \n",
    "and drop-off actions.\"\n",
    "\n",
    "- The filled square represents the taxi, which is yellow without a passenger and green with a passenger.\n",
    "- The pipe (\"|\") represents a wall which the taxi cannot cross.\n",
    "- R, G, Y, B are the possible pickup and destination locations. The blue letter represents the current passenger\n",
    "  pick-up location, and the purple letter is the current destination.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "def16f7d60e44660a8f0323d5244b96b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "%autoreload 2\n",
    "#goals = [(8,8), (8,20), (20, 8), (20,20)]\n",
    "goals = [(27,27), (18,24), (24, 18), (20,20)]\n",
    "four_room_envs =[None] * 4\n",
    "for i in range(len(goals)):\n",
    "    four_room_envs[i] = FourRooms()\n",
    "    four_room_envs[i].reset()\n",
    "    four_room_envs[i].goal =  four_room_envs[i].tostate[goals[i]]\n",
    "    clear_output(True)\n",
    "    plt.subplot(2,2,i+1)\n",
    "    plt.imshow(four_room_envs[i].render(show_goal=True), cmap='Blues')\n",
    "    plt.axis('off')\n",
    "    plt.title('level ' + str(i))\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R:\u001b[43m \u001b[0m| : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env=gym.make(\"Taxi-v003\")\n",
    "env.seed(0)\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R:\u001b[43m \u001b[0m| : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | :\u001b[43m \u001b[0m: |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gym.make(\"Taxi-v003\").render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "| : | : :R|\n",
      "| : |\u001b[43m \u001b[0m: :B|\n",
      "| : : : :\u001b[35mY\u001b[0m|\n",
      "| | : | : |\n",
      "| | : | :\u001b[34;1mG\u001b[0m|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gym.make(\"Taxi-v000\").render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gym.make(\"Taxi-v3\").render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "\n",
      "Action space = Discrete(6)\n",
      "State space = Discrete(500)\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "env.render()\n",
    "print(\"Action space = {}\".format(env.action_space))\n",
    "print(\"State space = {}\".format(env.observation_space))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(1.0, 428, 0, False)],\n",
       " 1: [(1.0, 228, 0, False)],\n",
       " 2: [(1.0, 348, 0, False)],\n",
       " 3: [(1.0, 328, 0, False)],\n",
       " 4: [(1.0, 328, 0, False)],\n",
       " 5: [(1.0, 328, 0, False)]}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.P[328] # {action: [(probability, nextstate, reward, done)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map1 = gym.make(\"Taxi-v003\").desc.flatten()\n",
    "map2 = gym.make(\"Taxi-v001\").desc.flatten()\n",
    "from pyxdameraulevenshtein import damerau_levenshtein_distance, normalized_damerau_levenshtein_distance\n",
    "damerau_levenshtein_distance(map1, map2)  # expected result: 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def state_dis(p1,p2,epsilon = 0.01, nsteps = 1000):\n",
    "    # p {state:{action:{[(1.0, new_state, reward, done)]}}} \n",
    "    assert len(p1[0]) == len(p2[0]),'transitions have different action spaces'\n",
    "    actions = range(len(p1[0]))\n",
    "    states_dis, next_states_dis = np.zeros([len(p1), len(p2)]), np.zeros([len(p1), len(p2)])\n",
    "    epsilon = epsilon\n",
    "    gamma = 0.95\n",
    "    step = 0\n",
    "    \n",
    "    for step in tqdm(range(nsteps)):\n",
    "        for i  in range(len(p1)):\n",
    "            for j in range(len(p2)):\n",
    "                # d(si,sj) = max{cr*|rsi - rsj| + ct*Tk(d)(si,sj)} with ct =gamma, cr = 1-gamma , Tkd A.K.A emd\n",
    "                dists = []\n",
    "                for action in actions:\n",
    "                    # print(i,j,action)\n",
    "                    state_action1=p1[i][action][0] # contains (1.0, new_state, reward, done)\n",
    "                    state_action2=p2[j][action][0]\n",
    "                    \n",
    "                    emd = abs(states_dis[state_action1[1]][state_action2[1]]) # d(s_i^{next_state} , s_j^{next_state})\n",
    "                    dis = (1-gamma)*abs(state_action1[2] - state_action2[2]) + gamma*emd \n",
    "#                     if dis > 0:\n",
    "#                          print(i,j,dis)\n",
    "                    dists.append(dis)\n",
    "                next_states_dis[i][j] = max(dists)\n",
    "        if np.max(abs(next_states_dis - states_dis) ) < epsilon:\n",
    "\n",
    "            break   \n",
    "        states_dis = next_states_dis\n",
    "        next_states_dis = np.zeros([len(p1), len(p2)])\n",
    "        step += 1 \n",
    "#         clear_output(wait=True)\n",
    "#         print(f\"step: {step}\")\n",
    "\n",
    "    return next_states_dis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyxdameraulevenshtein import damerau_levenshtein_distance, normalized_damerau_levenshtein_distance\n",
    "import random\n",
    "random.seed(1234)\n",
    "np.random.seed(1234)\n",
    "\n",
    "dl_list= []\n",
    "pass_locs = [(i,j*2) for i in range(6) for j in range(5)]\n",
    "obs_locs = [(i,j*2+1) for i in range(6) for j in range(4)]\n",
    "for i in range(10000):    \n",
    "    map1,map2 = np.zeros([6,10]),np.zeros([6,10])\n",
    "    for mapi in [map1,map2]:\n",
    "        for pos in  np.random.choice(range(30),4,replace = False):\n",
    "            pos = pass_locs[pos]\n",
    "            mapi[pos[0]][pos[1]] = 1\n",
    "        for pos in obs_locs:\n",
    "            mapi[pos[0]][pos[1]] = np.random.randint(2,4)\n",
    "    dl_list.append(damerau_levenshtein_distance(map1.flatten(), map2.flatten())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_P(MAP):\n",
    "    desc = np.asarray(MAP, dtype='c')\n",
    "    locR, locG, locY, locB = np.asarray(desc == b'R').nonzero(), np.asarray(desc == b'G').nonzero(),np.asarray(desc == b'Y').nonzero(),np.asarray(desc == b'B').nonzero()\n",
    "    #(loc[0][0] , loc[1][0] ) contains (array([x]),array([y])\n",
    "    locs = [(locR[0][0] - 1 , locR[1][0] //2), (locG[0][0] -1 , locG[1][0] //2 ), (locY[0][0] -1  , locY[1][0] //2), (locB[0][0] -1 , locB[1][0] //2)]\n",
    "\n",
    "    num_states = 500\n",
    "    num_rows = 5\n",
    "    num_columns = 5\n",
    "    max_row = num_rows - 1\n",
    "    max_col = num_columns - 1\n",
    "    num_actions = 6\n",
    "    P = {state: {action: []\n",
    "                 for action in range(num_actions)} for state in range(num_states)}\n",
    "    for row in range(num_rows):\n",
    "        for col in range(num_columns):\n",
    "            for pass_idx in range(len(locs) + 1):  # +1 for being inside taxi\n",
    "                for dest_idx in range(len(locs)):\n",
    "                    state = encode(row, col, pass_idx, dest_idx)\n",
    "                    for action in range(num_actions):\n",
    "                        # defaults\n",
    "                        new_row, new_col, new_pass_idx = row, col, pass_idx\n",
    "                        reward = 0  # default reward when there is no pickup/dropoff\n",
    "                        done = False\n",
    "                        taxi_loc = (row, col)\n",
    "\n",
    "                        if action == 0:\n",
    "                            new_row = min(row + 1, max_row)\n",
    "                        elif action == 1:\n",
    "                            new_row = max(row - 1, 0)\n",
    "                        if action == 2 and desc[1 + row, 2 * col + 2] == b\":\":\n",
    "                            new_col = min(col + 1, max_col)\n",
    "                        elif action == 3 and desc[1 + row, 2 * col] == b\":\":\n",
    "                            new_col = max(col - 1, 0)\n",
    "                        elif action == 4:  # pickup\n",
    "                            if (pass_idx < 4 and taxi_loc == locs[pass_idx]):\n",
    "                                new_pass_idx = 4\n",
    "                            else:  # passenger not at location\n",
    "                                reward = 0\n",
    "                        elif action == 5:  # dropoff\n",
    "                            if (taxi_loc == locs[dest_idx]) and pass_idx == 4:\n",
    "                                new_pass_idx = dest_idx\n",
    "                                done = True\n",
    "                                reward = 20\n",
    "                            elif (taxi_loc in locs) and pass_idx == 4:\n",
    "                                new_pass_idx = locs.index(taxi_loc)\n",
    "                            else:  # dropoff at wrong location\n",
    "                                reward = 0\n",
    "                        new_state = encode(\n",
    "                            new_row, new_col, new_pass_idx, dest_idx)\n",
    "                        P[state][action].append(\n",
    "                            (1.0, new_state, reward, done))\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode( taxi_row, taxi_col, pass_loc, dest_idx):\n",
    "    # (5) 5, 5, 4\n",
    "    i = taxi_row\n",
    "    i *= 5\n",
    "    i += taxi_col\n",
    "    i *= 5\n",
    "    i += pass_loc\n",
    "    i *= 4\n",
    "    i += dest_idx\n",
    "    return i\n",
    "\n",
    "def decode(i):\n",
    "    out = []\n",
    "    out.append(i % 4)\n",
    "    i = i // 4\n",
    "    out.append(i % 5)\n",
    "    i = i // 5\n",
    "    out.append(i % 5)\n",
    "    i = i // 5\n",
    "    out.append(i)\n",
    "    assert 0 <= i < 5\n",
    "    return reversed(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_metric(dist_table):\n",
    "    return np.sum(np.min(dist_table,axis= 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_random_maps(num,seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    dl_list= []\n",
    "    pass_locs = [(i+1,j*2+1) for i in range(6) for j in range(5)]\n",
    "    obs_locs = [(i+1,j*2+2) for i in range(6) for j in range(4)]\n",
    "    obs = [':','|']\n",
    "    maps = []\n",
    "    for i in range(num):    \n",
    "        MAP =  [\n",
    "        \"+---------+\",\n",
    "        \"| : | : : |\",\n",
    "        \"| : | : : |\",\n",
    "        \"| : : : : |\",\n",
    "        \"| | : | : |\",\n",
    "        \"| | : | : |\",\n",
    "        \"+---------+\",\n",
    "        ]  \n",
    "        MAP = np.asarray(MAP,dtype = 'c')\n",
    "        for pos, loc in  zip( np.random.choice(range(30),4,replace = False), ['R','G','Y','B']):\n",
    "            pos = pass_locs[pos]\n",
    "            MAP[pos[0]][pos[1]] = loc\n",
    "        for pos in obs_locs:\n",
    "            MAP[pos[0]][pos[1]] = obs[np.random.randint(0,2)]\n",
    "        maps.append(MAP)\n",
    "    return maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 18x19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 90/1000 [02:22<24:02,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7h 21min 12s, sys: 1min 3s, total: 7h 22min 15s\n",
      "Wall time: 7h 23min 15s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyxdameraulevenshtein import damerau_levenshtein_distance, normalized_damerau_levenshtein_distance\n",
    "dl_dists= []\n",
    "huasdorff_dists = []\n",
    "kt_dists = []\n",
    "my_dists = []\n",
    "next_states_diststs = [ [None for j in range(len(maps))] for i in range(len(maps))]\n",
    "maps = make_random_maps(20,2323)\n",
    "for i in range(len(maps)):\n",
    "    for j in range(i+1, len(maps)):\n",
    "        clear_output(wait = True)\n",
    "        print(f\"iteration: {i}x{j}\")        \n",
    "        \n",
    "        next_states_dis = state_dis(generate_P(maps[i]), generate_P(maps[j]),nsteps=1000)\n",
    "        \n",
    "        kt_dis = np.trace(next_states_dis)\n",
    "        hausdorff_dis = max( np.max(np.min(next_states_dis,axis = 0)), np.max(np.min(next_states_dis,axis = 1)) )  \n",
    "        my_dis = my_metric(next_states_dis)\n",
    "        \n",
    "        huasdorff_dists.append(hausdorff_dis)\n",
    "        kt_dists.append(kt_dis)\n",
    "        dl_dists.append(damerau_levenshtein_distance(maps[i].flatten(), maps[j].flatten()))\n",
    "        my_dists.append(my_dis)\n",
    "        next_states_diststs[i][j] = next_states_dis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000,)"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data['Close_transfer_caps'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 18x9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 90/1000 [02:26<24:40,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7h 35min 57s, sys: 13.9 s, total: 7h 36min 11s\n",
      "Wall time: 7h 38min 20s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "np.random.RandomState(1234)\n",
    "random.seed(1234)\n",
    "dl_dists2= []\n",
    "huasdorff_dists2 = []\n",
    "kt_dists2 = []\n",
    "my_dists2 = []\n",
    "\n",
    "maps1 = make_random_maps(19,1234)\n",
    "maps2 = make_random_maps(10,4567)\n",
    "next_states_diststs2 = [ [None for j in range(len(maps2))] for i in range(len(maps1))]\n",
    "for i in range(len(maps1)):\n",
    "    for j in range(len(maps2)):\n",
    "        clear_output(wait = True)\n",
    "        print(f\"iteration: {i}x{j}\")  \n",
    "        next_states_dis = state_dis(generate_P(maps1[i]), generate_P(maps2[j]))\n",
    "        \n",
    "        kt_dis = np.trace(next_states_dis)\n",
    "        hausdorff_dis = max( np.max(np.min(next_states_dis,axis = 0)), np.max(np.min(next_states_dis,axis = 1)) )  \n",
    "        my_dis = my_metric(next_states_dis)\n",
    "        \n",
    "        huasdorff_dists2.append(hausdorff_dis)\n",
    "        kt_dists2.append(kt_dis)\n",
    "        dl_dists2.append(damerau_levenshtein_distance(maps1[i].flatten(), maps2[j].flatten()))\n",
    "        my_dists2.append(my_dis)\n",
    "        next_states_diststs2[i][j] = next_states_dis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19, 10, 500, 500)"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.asarray(next_states_diststs2)\n",
    "# np.argmin(a)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autolabel(ax, rects):\n",
    "    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n",
    "    heights = rects[0]\n",
    "    edges = rects[1]\n",
    "    for i in range(len(heights)):\n",
    "        height = heights[i]\n",
    "        if height == 0:\n",
    "            continue\n",
    "        edge_left = edges[i]\n",
    "        edge_right = edges[i+1]\n",
    "        ax.annotate('{}'.format(int(height)),\n",
    "                    xy=((edge_left + edge_right) / 2, height),\n",
    "                    xytext=(0, -2),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: module://ipympl.backend_nbagg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "744de6e134a8424fbe8a2d9dab43db2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib\n",
    "fig,ax = plt.subplots(1,4,figsize = (15,3))\n",
    "fig.suptitle('Distance between 20 random generated levels (190 instance) ')\n",
    "rect = ax[0].hist(dl_dists,10)\n",
    "ax[0].set_ylabel('counts')\n",
    "ax[0].set_xlabel('damerau levenshtein distance')\n",
    "autolabel(ax[0],rect)\n",
    "rect = ax[1].hist(huasdorff_dists,10)\n",
    "ax[1].set_ylabel('counts')\n",
    "ax[1].set_xlabel('hausdorff distance')\n",
    "autolabel(ax[1],rect)\n",
    "rect = ax[2].hist(kt_dists,10)\n",
    "ax[2].set_ylabel('counts')\n",
    "ax[2].set_xlabel('simple kantorovich distance')\n",
    "autolabel(ax[2],rect)\n",
    "plt.savefig(fname = './3_dist_20maps ',dpi = 600)\n",
    "rect = ax[3].hist(my_dists,10)\n",
    "ax[3].set_ylabel('counts')\n",
    "ax[3].set_xlabel('my distance')\n",
    "autolabel(ax[3],rect)\n",
    "plt.savefig(fname = './3_dist_20maps ',dpi = 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: module://ipympl.backend_nbagg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97522cfaddfe47e0ac0a745bd73767e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib\n",
    "fig,ax = plt.subplots(1,4,figsize = (15,3))\n",
    "fig.suptitle('Distance between 19*10 random generated levels (190 instance) ')\n",
    "rect = ax[0].hist(dl_dists2,10)\n",
    "ax[0].set_ylabel('counts')\n",
    "ax[0].set_xlabel('damerau levenshtein distance')\n",
    "autolabel(ax[0],rect)\n",
    "rect = ax[1].hist(huasdorff_dists2,10)\n",
    "ax[1].set_ylabel('counts')\n",
    "ax[1].set_xlabel('hausdorff distance')\n",
    "autolabel(ax[1],rect)\n",
    "rect = ax[2].hist(kt_dists2,10)\n",
    "ax[2].set_ylabel('counts')\n",
    "ax[2].set_xlabel('simple kantorovich distance')\n",
    "autolabel(ax[2],rect)\n",
    "rect = ax[3].hist(my_dists2,10)\n",
    "ax[3].set_ylabel('counts')\n",
    "ax[3].set_xlabel('my distance')\n",
    "autolabel(ax[3],rect)\n",
    "plt.savefig(fname = './3_dist_19*10 maps ',dpi = 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: module://ipympl.backend_nbagg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59ca3ba0beb14ad5aeb439ee19ab8f7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'damerau levenshtein distance')"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib\n",
    "fig,ax = plt.subplots()\n",
    "ax.hist(dl_dists2,10)\n",
    "ax.set_ylabel('counts')\n",
    "ax.set_xlabel('damerau levenshtein distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/1000 [00:04<24:45,  1.49s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-334-cae90d7746be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0menv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Taxi-v001\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0menv2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Taxi-v003\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mstate_dis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0menv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-327-488e9e354376>\u001b[0m in \u001b[0;36mstate_dis\u001b[0;34m(p1, p2, epsilon, nsteps)\u001b[0m\n\u001b[1;32m     14\u001b[0m                 \u001b[0;31m# d(si,sj) = max{cr*|rsi - rsj| + ct*Tk(d)(si,sj)} with ct =gamma, cr = 1-gamma , Tkd A.K.A emd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                 \u001b[0mdists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m                     \u001b[0;31m# print(i,j,action)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                     \u001b[0mstate_action1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# contains (1.0, new_state, reward, done)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env1 = gym.make(\"Taxi-v001\")\n",
    "env2 = gym.make(\"Taxi-v003\")\n",
    "state_dis(env1.P,env2.P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "def saveData(data, path):\n",
    "    output = open(path, 'wb')\n",
    "    # Pickle dictionary using protocol 0.\n",
    "    pickle.dump(data, output)\n",
    "    output.close()\n",
    "    \n",
    "def loadData(path):\n",
    "    pkl_file = open(path, 'rb')\n",
    "    segContent = pickle.load(pkl_file)\n",
    "    pkl_file.close()\n",
    "    return segContent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_rate(lc_no_trans, lc_trans):\n",
    "    max_no_trans = np.max(lc_no_trans)\n",
    "    max_trans = np.max(lc_trans)\n",
    "    c_no = 2*abs(max_no_trans) + 1\n",
    "    c = 2*abs(max_trans) + 1\n",
    "    if max_trans > max_no_trans:\n",
    "        return (max_trans + c_no)/(max_no_trans +c_no)\n",
    "    elif max_trans < max_no_trans:\n",
    "        return - (max_no_trans + c)/(max_trans + c)\n",
    "    else:\n",
    "       \n",
    "        time_trans = np.min( np.where(lc_trans >= max_no_trans))\n",
    "        time_no_trans = np.min( np.where(lc_no_trans >= max_no_trans) )\n",
    "        if time_trans < time_no_trans:\n",
    "            return (time_no_trans - time_trans) / time_no_trans\n",
    "        elif time_trans ==  time_no_trans: \n",
    "            return 0\n",
    "        else:\n",
    "            return (time_no_trans - time_trans) / (len(lc_no_trans) - time_no_trans)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### show continuality of TR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000,)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from matplotlib.animation import FuncAnimation \n",
    "from  matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import HTML\n",
    "algs = ['qlearn','pr','ops','caps']\n",
    "data = [None] * len(algs)\n",
    "all_data = {}\n",
    "for i,alg in enumerate(algs):\n",
    "    data[i] = loadData('./past_ex_data/' + \"similarity_diff\" + '_' + alg + '_'  + '20210429' )\n",
    "    all_data.update(data[i])\n",
    "lc_no_trans = all_data['No_transfer_qlearn'][0][0:500]\n",
    "lc_trans_3long = np.concatenate((np.linspace(-20, 0 ,500),lc_no_trans,[lc_no_trans[-1]]*500,np.linspace(20,40,500)))\n",
    "lc_trans_3long.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2807"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(err_acc_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trans_rate: 0.0\n",
      "jumpstart: 0.15196975761618858\n",
      "asympotic: 0.15239759839893263\n",
      "time_to_threshold: 0.02259106070713809\n",
      "acc_rewards: 0.0024967756281965756\n"
     ]
    }
   ],
   "source": [
    "err_trans_rate, err_jumpstart, err_asympotic, err_time_to_threshold, err_acc_rewards = [],[],[],[],[]\n",
    "metrics = ['trans_rate', 'jumpstart', 'asympotic', 'time_to_threshold', 'acc_rewards']\n",
    "sample_len = len(lc_trans_3long) - len(lc_trans)\n",
    "for i in range(sample_len):\n",
    "    for j in range(i,sample_len):\n",
    "        lc_no_trans= lc_trans_3long[j:j+len(lc_trans)]\n",
    "        lc_trans = lc_trans_3long[i:i+len(lc_trans)]\n",
    "        trans_rate, jumpstart, asympotic, time_to_threshold, acc_rewards = get_metrics_val(lc_no_trans,lc_trans)\n",
    "        transfer_flag = None\n",
    "        if np.max(lc_trans) > np.max(lc_no_trans):\n",
    "            transfer_flag = 1\n",
    "        elif np.max(lc_trans) < np.max(lc_no_trans):\n",
    "            transfer_flag = -1\n",
    "        else:\n",
    "            if time_to_threshold > 0: \n",
    "                transfer_flag = 1\n",
    "            elif time_to_threshold < 0:\n",
    "                 transfer_flag = -1\n",
    "            else: transfer_flag = 0\n",
    "        for metric in metrics:\n",
    "            if locals()[metric] * transfer_flag < 0 :\n",
    "                locals()['err_'+ metric].append([i,j])\n",
    "            elif locals()[metric] * transfer_flag == 0 and locals()[metric] + transfer_flag != 0:\n",
    "                locals()['err_'+ metric].append([i,j])\n",
    "for metric in metrics:\n",
    "    print(metric + ': ' , end ='')\n",
    "    print(len( locals()['err_'+ metric] ) / (sample_len*(sample_len - 1)/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d363125dc01e4ed8acae941e6742e796",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = 850\n",
    "j = 877\n",
    "fig,ax = plt.subplots()\n",
    "lc_trans_i = lc_trans_3long[i:i+len(lc_trans)]\n",
    "lc_trans_j = lc_trans_3long[j:j+len(lc_trans)]\n",
    "t_train = len(lc_trans)\n",
    "time_trans_i = np.min(np.where(lc_trans_i == np.max(lc_trans_i))) #the first point that reach the max value in lc_trans\n",
    "time_trans_j = np.min(np.where(lc_trans_j == np.max(lc_trans_j)))\n",
    "ax.plot(range(t_train), lc_trans_i)\n",
    "ax.plot(range(t_train), lc_trans_j)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 2.0, 0.0, 0, 168.0)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_metrics_val(lc_trans_i,lc_trans_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max = 0\n",
    "id_max = 0\n",
    "for err in err_acc_rewards:\n",
    "    if err[0] - err[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ea6fc50418d441181007cd606c58a02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "t_train = len(lc_no_trans)\n",
    "fig, ax = plt.subplots() \n",
    "lc_no_trans = lc_trans_3long[t_train:2*t_train]\n",
    "\n",
    "# def init():\n",
    "# #     line_no, = plt.plot(range(t_train), lc_no_trans,color='blue', label = 'No_transfer')\n",
    "#     line_trans, = plt.plot(range(t_train), lc_trans_3long[:t_train])\n",
    "#     ax.set_xlim(0, 500)\n",
    "#     ax.set_ylim(-20, 40)    \n",
    "#     return line_trans\n",
    "\n",
    "#calculate transfer rate before animation in case of calculation delay between frames\n",
    "tr = []\n",
    "for frame_idx in range(3*t_train):\n",
    "    lc_trans = lc_trans_3long[frame_idx : frame_idx + t_train]\n",
    "    tr.append( transfer_rate(lc_no_trans,lc_trans) )\n",
    " \n",
    "def update(frame_idx):\n",
    "    plt.cla()\n",
    "    ax.set_xlim(0, t_train)\n",
    "    ax.set_ylim(-20, 40) \n",
    "    lc_trans = lc_trans_3long[frame_idx : frame_idx + t_train]\n",
    "    line_trans, = ax.plot(range(t_train), lc_no_trans,color = 'blue', label='No_Transfer')\n",
    "    line_trans=ax.plot(range(t_train), lc_trans,color = 'orange',label = 'Transfer')    \n",
    "    time_trans = np.min(np.where(lc_trans == np.max(lc_trans))) #the first point that reach the max value in lc_trans\n",
    "    ax.axhline(y=max(lc_trans),xmin=0,xmax = time_trans/t_train, color ='r',linestyle='--')\n",
    "    yticks = ax.get_yticks()\n",
    "    yticks_  = np.append(yticks,max(lc_trans))\n",
    "    ax.set_yticks(yticks_)\n",
    "#     colors = ['b']*len(yticks)\n",
    "#     colors.append('red')\n",
    "    ax.set_yticklabels(np.append(yticks,f'p* = {max(lc_trans):.1f}'))\n",
    "    plt.title(f\"Transfer Rate = {tr[frame_idx]:+.4f}\")  \n",
    "    plt.legend(loc = 'lower right')\n",
    "    return line_trans\n",
    "ani = FuncAnimation(fig, update, frames=3*t_train, \n",
    "                   interval=1,repeat = False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85630a6212f249fb894f61c31359e116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Orange learning curve')"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig, ax = plt.subplots() \n",
    "ax.plot(range(len(lc_trans_3long)), lc_trans_3long,color = 'orange')\n",
    "ax.set_xlim(right = 2000)\n",
    "plt.title(f'Orange learning curve')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35452bde5b404ddfa8a7fab422ff3bd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Transfer rate during 1500 frames')"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig, ax = plt.subplots() \n",
    "ax.plot(range(len(tr)), tr)\n",
    "ax.set_xlim(right = 1600)\n",
    "plt.title(f'Transfer rate during {len(tr)} frames')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1124250.0"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1500*1499/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import animation, rc\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cda1e2a6e555449bbcdc45b605be9f46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# First set up the figure, the axis, and the plot element we want to animate\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.set_xlim(( 0, 2))\n",
    "ax.set_ylim((-2, 2))\n",
    "\n",
    "line, = ax.plot([], [], lw=2)\n",
    "# initialization function: plot the background of each frame\n",
    "def init():\n",
    "    line.set_data([], [])\n",
    "    return (line,)\n",
    "# animation function. This is called sequentially\n",
    "def animate(i):\n",
    "    x = np.linspace(0, 2, 1000)\n",
    "    y = np.sin(2 * np.pi * (x - 0.01 * i))\n",
    "    line.set_data(x, y)\n",
    "    return (line,)\n",
    "# call the animator. blit=True means only re-draw the parts that have changed.\n",
    "anim = FuncAnimation(fig, animate, init_func=init,\n",
    "                               frames=100, interval=20, blit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics_val(learning_curve_no_trans, learning_curve_trans):\n",
    "    \"\"\"\n",
    "    get the value of 5 metrics\n",
    "    1.jumpstart\n",
    "    2.asympotic performance \n",
    "    3.time to threshold\n",
    "    4.accumulated rewards\n",
    "    5.transfer rate\n",
    "    \"\"\"\n",
    "    lc_no_trans = learning_curve_no_trans\n",
    "    lc_trans = learning_curve_trans\n",
    "    \n",
    "    #jumpstart\n",
    "    jumpstart = lc_trans[0] - lc_no_trans[0]\n",
    "    \n",
    "    #asympotic performance \n",
    "    asympotic = lc_trans[-1] - lc_no_trans[-1]\n",
    "    \n",
    "    # time to threshold(convergence)\n",
    "    threshold = min( np.max(lc_no_trans),np.max(lc_trans)  )\n",
    "   \n",
    "    time_trans = np.min( np.where(lc_trans >= threshold))\n",
    "    time_no_trans = np.min( np.where(lc_no_trans >= threshold) )\n",
    "    time_to_threshold = time_no_trans - time_trans\n",
    "    \n",
    "    # accumulated rewards\n",
    "    acc_rewards = np.sum(lc_trans - lc_no_trans)\n",
    "    \n",
    "    #transfer rate \n",
    "    trans_rate = transfer_rate(lc_no_trans, lc_trans)\n",
    "    \n",
    "    return trans_rate, jumpstart, asympotic, time_to_threshold, acc_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(lc_no_transfer, lc_transfer):\n",
    "    assert lc_no_transfer.shape[0] == lc_transfer.shape[0],\\\n",
    "    \"two learning curve must have same run times\"\n",
    "    nruns =  lc_no_transfer.shape[0]\n",
    "    trans_rate, jumpstart, asympotic, time_to_threshold, acc_rewards = [np.zeros(nruns), np.zeros(nruns),np.zeros(nruns),np.zeros(nruns),np.zeros(nruns)]\n",
    "    for i in range(nruns):\n",
    "        trans_rate[i], jumpstart[i], asympotic[i], time_to_threshold[i], acc_rewards[i] = get_metrics_val(lc_no_transfer[i], lc_transfer[i])\n",
    "    res = { 'Transfer rate': trans_rate,\n",
    "            'Jumpstart': jumpstart,\n",
    "            'Asympotic performace': asympotic,\n",
    "            'Time to threshold': time_to_threshold,\n",
    "            'Accumulated rewards': acc_rewards       \n",
    "           }\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_color_dict(data):\n",
    "    keys = [key for key in data.keys()]\n",
    "    \n",
    "    trans_settings = set()\n",
    "    algs = set()\n",
    "    for key in keys:\n",
    "        temp = key.split('_')\n",
    "        trans_settings.add(temp[0])\n",
    "        algs.add(temp[2])\n",
    "    trans_settings = list(trans_settings) \n",
    "    algs = list(algs)\n",
    "    trans_settings.sort(reverse = True)\n",
    "    algs.sort(reverse = True)\n",
    "    cmap_names =['Blues', 'Reds', 'Greens']\n",
    "    color_dict = {}\n",
    "    for i, trans_setting in enumerate(trans_settings):\n",
    "        for j, alg in enumerate(algs):\n",
    "            assert len(algs) > 0, 'data is empty'\n",
    "            color = plt.get_cmap(cmap_names[i%len(cmap_names)])(0.8 - 0.8 / len(algs) * j +0.1)\n",
    "            if trans_setting not in color_dict:\n",
    "                color_dict.update({trans_setting:{alg: color}})\n",
    "            elif alg not in color_dict[trans_setting]:\n",
    "                color_dict[trans_setting].update({alg: color})\n",
    "    return color_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_result(data, nepisodes, smooth_radius=50, combine_figures = True, average_group = True, show_std = True, figsize = None, title=\"episode-reward graph\", xlabel=\"episode\", ylabel = \"rewards\"):\n",
    "   \n",
    "    plt.rcParams['savefig.dpi'] = 600\n",
    "    data = copy.deepcopy(data)\n",
    "    color_dict = make_color_dict(data)\n",
    "    # plot n runs as only 1 curve\n",
    "    if average_group:\n",
    "        for label,rewards in data.items():\n",
    "            mean = np.mean(rewards, axis = 0)\n",
    "            std = np.std(rewards, axis = 0)\n",
    "            data[label]= {'mean': mean, 'std': std}\n",
    "            \n",
    "   \n",
    "    if combine_figures:\n",
    "        f, axarr = plt.subplots(1, 1, sharex=False, squeeze=False, figsize=figsize)\n",
    "        ax= axarr[0][0]\n",
    "        assert average_group == True, \"average_group should be True when combine_figures is True\"\n",
    "        for label in data.keys():\n",
    "                color = color_dict[label.split('_')[0]][label.split('_')[2]]\n",
    "                y = data[label]['mean'][:nepisodes]\n",
    "                ystd = data[label]['std'][:nepisodes]\n",
    "                #print(content.shape)\n",
    "                ax.plot(range(nepisodes), smooth(y,smooth_radius),color = color,label = label)\n",
    "                if show_std:\n",
    "                    ax.fill_between(range(nepisodes), smooth(y -  ystd,smooth_radius), smooth(y +  ystd,smooth_radius), color = color, alpha = 0.2)\n",
    "                ax.set_xlabel(xlabel)\n",
    "                ax.set_ylabel(ylabel)\n",
    "                if title == \"episode-reward graph\":\n",
    "                    ax.set_title(label.split('_')[2] +' ' + label.split('_')[-2] + \":  \" +title)\n",
    "                else:\n",
    "                    ax.set_title(title)\n",
    "                ax.legend()\n",
    "    else:\n",
    "        f, axarr = plt.subplots(len(data), 1, sharex=True, sharey=True, squeeze=False, figsize=figsize)\n",
    "        for idx,label in enumerate(data.keys()):\n",
    "            ax = axarr[idx][0]            \n",
    "            color = color_dict[label.split('_')[0]][label.split('_')[2]]\n",
    "            \n",
    "            \n",
    "            if average_group:\n",
    "               \n",
    "                y = data[label]['mean'][:nepisodes]\n",
    "                ystd = data[label]['std'][:nepisodes]\n",
    "                ax.plot(range(nepisodes), smooth(data[label]['mean'][:nepisodes],smooth_radius),color = color,label = label)\n",
    "                if show_std:\n",
    "                    ax.fill_between(range(nepisodes), smooth(y -  ystd,smooth_radius), smooth(y +  ystd,smooth_radius), color = color, alpha = 0.2)\n",
    "                ax.set_xlabel(xlabel)\n",
    "                ax.set_ylabel(ylabel)\n",
    "                ax.set_title(label+ \":  \" +title)\n",
    "                ax.legend()\n",
    "                \n",
    "            else:\n",
    "                for i,reward in enumerate(data[label]):\n",
    "                    if i==0:\n",
    "                        ax.plot(range(nepisodes),smooth(reward[:nepisodes],smooth_radius),label = label)\n",
    "                        ax.set_xlabel(xlabel)\n",
    "                        ax.set_ylabel(ylabel)\n",
    "                        ax.set_title(label+ \":  \" +title)\n",
    "                        ax.legend()\n",
    "                    else:\n",
    "                        ax.plot(range(nepisodes),smooth(reward[:nepisodes],smooth_radius),label = None)\n",
    "\n",
    "    plt.tight_layout()\n",
    "                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arg_max(state_action):\n",
    "    max_index_list = []\n",
    "    max_value = state_action[0]\n",
    "    for index, value in enumerate(state_action):\n",
    "        if value > max_value:\n",
    "            max_index_list.clear()\n",
    "            max_value = value\n",
    "            max_index_list.append(index)\n",
    "        elif value == max_value:\n",
    "            max_index_list.append(index)\n",
    "    return random.choice(max_index_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_zero_state(q_table):\n",
    "    zero_state = 0\n",
    "    for arr_2d in q_table:\n",
    "         zero_state += np.sum(np.sum(arr_2d,axis = 1) == 0)\n",
    "    return zero_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### consts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODE_NUM = 4000\n",
    "MAX_EPISODE_LEN = 200\n",
    "REPEAT_TIMES = 10  # train agent REAPEAT_TIMES to get averaged learning curves\n",
    "EVALUATION_TIMES = 10 # evaluate target policy EVALUATION_TIMES after x updates in off-policy RL algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "def show_frames(env,j ,i, episode_rewards ):\n",
    "    clear_output(wait=True)\n",
    "    env.render()\n",
    "    print(f\"episode {j+1}  step {i}  rewards={episode_rewards}\")\n",
    "    sleep(0.1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(y, radius):\n",
    "    '''\n",
    "    smooth data y by averaging the values in each window [max{index-radius,0}, min{(index+radius), len(y)-1}] \n",
    "    \n",
    "    '''\n",
    "    if (len(y) < 2 * radius + 1):\n",
    "        return np.mean(y) * np.ones_like(y)\n",
    "    else:\n",
    "        convkernel = np.ones(2 * radius + 1)\n",
    "        out = np.convolve(y, convkernel, mode = 'same') / np.convolve(np.ones_like(y), convkernel, mode = 'same')\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluate(env, policy, times = 10):\n",
    "    # store data, make average and return\n",
    "    all_length, all_penalties, all_rewards = np.zeros(times), np.zeros(times), np.zeros(times)\n",
    "\n",
    "    for i in range(times):\n",
    "\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        episode_penalties, episode_rewards, episode_length = 0, 0, 0\n",
    "        \n",
    "        for j in range(MAX_EPISODE_LEN):\n",
    "            action = arg_max(policy[state])\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            state = next_state\n",
    "            #update data for learning curve\n",
    "            if reward == -10:\n",
    "                episode_penalties +=1\n",
    "\n",
    "            episode_rewards += reward\n",
    "            episode_length += 1\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        all_rewards[i] = episode_rewards\n",
    "        all_penalties[i] = episode_penalties\n",
    "        all_length[i] = episode_length\n",
    "    return np.mean(all_rewards), np.mean(all_penalties), np.mean(all_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.randint(0,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qlearn(env, train_episodes, init_q_table=None):\n",
    "    \"\"\"\n",
    "    Training the agent\n",
    "    Q(state,action)←(1−α)Q(state,action)+α(reward+γmaxaQ(next state,all actions))\n",
    "    \"\"\"  \n",
    "    \n",
    "    # Hyper parameters\n",
    "    alpha = 0.05  #(alpha) is the learning rate (0<α≤1) \n",
    "    gamma = 0.95  # (gamma) is the discount factor (0≤γ≤1) \n",
    "    epsilon = 0.1 # the rate to act randomly in the epsilon-greedy alg for expolration in Q-learning\n",
    "\n",
    "    timesteps =  train_episodes * MAX_EPISODE_LEN\n",
    "\n",
    "    # To plot learning curve\n",
    "    all_episodes_length = []\n",
    "    all_penalties = []\n",
    "    all_rewards = []\n",
    "    \n",
    "    if init_q_table is None:\n",
    "        q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "    else:\n",
    "        q_table = copy.deepcopy(init_q_table)\n",
    "    \n",
    "    env_copy = copy.deepcopy(env) # for policy_evaluate\n",
    "    i = 0\n",
    "    while i < timesteps:\n",
    "        env.seed(random.randint(0,timesteps))\n",
    "        state = env.reset()\n",
    "\n",
    "        episode_penalties, episode_rewards, episode_length = 0, 0, 0\n",
    "        done = False\n",
    "\n",
    "        while not done and i < timesteps:\n",
    "            # epsilon greedy alg balancing exporation and exploitation\n",
    "            if random.uniform(0,1)< epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = arg_max(q_table[state])\n",
    "\n",
    "            # step to next state\n",
    "            next_state, reward, done, info = env.step(action)       \n",
    "            \n",
    "            # update q-value\n",
    "            q_value = (1-alpha) * q_table[state, action] + alpha * (reward + gamma * np.max(q_table[next_state]))\n",
    "            q_table[state, action] = q_value\n",
    "\n",
    "            state = next_state\n",
    "            if i%200 == 0:\n",
    "                #evaluate policy for learning curve after each episode\n",
    "                episode_rewards, episode_penalties, episode_length = policy_evaluate(env_copy, q_table, EVALUATION_TIMES)\n",
    "                \n",
    "                all_episodes_length.append(episode_length)\n",
    "                all_penalties.append(episode_penalties)\n",
    "                all_rewards.append(episode_rewards)\n",
    "                \n",
    "                #show training progress\n",
    "                clear_output(wait=True)\n",
    "                print(f\"timesteps: {i}\")\n",
    "            i += 1\n",
    "         \n",
    "    print(f\"Training finished\")\n",
    "    \n",
    "    # return ndarray\n",
    "    all_episodes_length = np.array(all_episodes_length)\n",
    "    all_penalties = np.array(all_penalties)\n",
    "    all_rewards = np.array(all_rewards)\n",
    "    \n",
    "    return all_episodes_length, all_penalties, all_rewards, q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(env,  train_episodes,init_s_table = None):\n",
    "    \"\"\"Training the agent\n",
    "    Q(state,action)←(1−α)Q(state,action)+α(reward+γQ(next state,next action))\n",
    "\n",
    "    \"\"\"  \n",
    "    # Hyper parameters\n",
    "    alpha = 0.05  #(alpha) is the learning rate (0<α≤1) \n",
    "    gamma = 0.95  # (gamma) is the discount factor (0≤γ≤1) \n",
    "    epsilon = 0.1 # the rate to act randomly in the epsilon-greedy alg for expolration in Q-learning\n",
    "\n",
    "    train_episodes =  train_episodes \n",
    "    timesteps =  train_episodes * MAX_EPISODE_LEN\n",
    "    # To plot learning curve\n",
    "    all_episodes_length = []\n",
    "    all_penalties = []\n",
    "    all_rewards = []\n",
    "    \n",
    "    \n",
    "    if init_s_table is None:\n",
    "        s_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "    else:\n",
    "        s_table = copy.deepcopy(init_s_table)\n",
    "    env_copy = copy.deepcopy(env)\n",
    "    \n",
    "    i = 0\n",
    "    while i < timesteps:\n",
    "        env.seed(random.randint(0,timesteps))\n",
    "        state = env.reset()\n",
    "\n",
    "        episode_penalties, episode_rewards, episode_length = 0, 0, 0\n",
    "        done = False\n",
    "\n",
    "        # epsilon greedy alg balancing exporation and exploitation\n",
    "        if random.uniform(0,1)< epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = arg_max(s_table[state])\n",
    "\n",
    "        while not done and i < timesteps:\n",
    "\n",
    "            # step to next state\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "\n",
    "            # choose next action\n",
    "            if random.uniform(0,1)< epsilon:\n",
    "                next_action = env.action_space.sample()\n",
    "            else:\n",
    "                next_action = arg_max(s_table[next_state])\n",
    "\n",
    "            # update q-value\n",
    "            q_value = (1-alpha) * s_table[state, action] + alpha * (reward + gamma * s_table[next_state,next_action])\n",
    "            s_table[state, action] = q_value\n",
    "\n",
    "            state = next_state\n",
    "            action = next_action\n",
    "\n",
    "            if i%200 == 0:\n",
    "                #evaluate policy for learning curve after each episode\n",
    "                episode_rewards, episode_penalties, episode_length = policy_evaluate(env_copy, s_table, EVALUATION_TIMES)\n",
    "                \n",
    "                all_episodes_length.append(episode_length)\n",
    "                all_penalties.append(episode_penalties)\n",
    "                all_rewards.append(episode_rewards)\n",
    "                \n",
    "                #show training progress\n",
    "                clear_output(wait=True)\n",
    "                print(f\"timesteps: {i}\")\n",
    "            i += 1\n",
    "    print(\"Training finished \\n\")\n",
    "    \n",
    "    # return ndarray\n",
    "    all_episodes_length = np.array(all_episodes_length)\n",
    "    all_penalties = np.array(all_penalties)\n",
    "    all_rewards = np.array(all_rewards)\n",
    "    \n",
    "    return all_episodes_length, all_penalties, all_rewards, s_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 PRQL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prql(env, train_episodes, past_policy):\n",
    "        \n",
    "    # Hyper parameters\n",
    "    alpha = 0.05  #(alpha) is the learning rate (0<α≤1) \n",
    "    gamma = 0.95  # (gamma) is the discount factor (0≤γ≤1) \n",
    "    epsilon = 0.1 # the rate to act randomly in the epsilon-greedy alg for expolration in Q-learning\n",
    "    fi = 1 # the intial probability to reuse past policy in policy-reuse algo, decays after each step in one episode\n",
    "    mu = 0.95 # the decaying rate of fi\n",
    "    \n",
    "    # To plot learning curve\n",
    "    all_episodes_length = []\n",
    "    all_penalties = []\n",
    "    all_rewards = []\n",
    "\n",
    "    timesteps =  train_episodes * MAX_EPISODE_LEN\n",
    "    q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    " \n",
    "    past_table = copy.deepcopy(past_policy)\n",
    "    env_copy = copy.deepcopy(env)\n",
    "    \n",
    "    i = 0\n",
    "    while i < timesteps:\n",
    "        env.seed(random.randint(0,timesteps))\n",
    "        state = env.reset()\n",
    "\n",
    "        episode_penalties, episode_rewards, episode_length = 0, 0, 0\n",
    "        done = False\n",
    "        f = fi\n",
    "        while not done and i < timesteps:\n",
    "            # epsilon greedy alg balancing exporation and exploitation\n",
    "            if random.uniform(0,1) < f:\n",
    "                action = arg_max(past_table[state])\n",
    "                #print(f\"f = {f} in episode {i} \\n\")\n",
    "            elif random.uniform(0,1) < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "                #print(\"random action \\n\")\n",
    "                \n",
    "            else:\n",
    "                action = arg_max(q_table[state])\n",
    "                #print(\"on the policy now \\n\")\n",
    "\n",
    "            # step to next state\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "\n",
    "            # update q-value\n",
    "            q_value = (1-alpha) * q_table[state, action] + alpha * (reward + gamma * np.max(q_table[next_state]))\n",
    "            q_table[state, action] = q_value\n",
    "\n",
    "            state = next_state\n",
    "            f = f*mu\n",
    "            \n",
    "            if i%200 == 0:\n",
    "                #evaluate policy for learning curve after each episode\n",
    "                episode_rewards, episode_penalties, episode_length = policy_evaluate(env_copy, q_table, EVALUATION_TIMES)\n",
    "                \n",
    "                all_episodes_length.append(episode_length)\n",
    "                all_penalties.append(episode_penalties)\n",
    "                all_rewards.append(episode_rewards)\n",
    "                \n",
    "                #show training progress\n",
    "                clear_output(wait=True)\n",
    "                print(f\"timesteps: {i}\")\n",
    "            i += 1\n",
    "    print(\"Training finished \\n\")\n",
    "    \n",
    "    # return ndarray\n",
    "    all_episodes_length = np.array(all_episodes_length)\n",
    "    all_penalties = np.array(all_penalties)\n",
    "    all_rewards = np.array(all_rewards)\n",
    "    \n",
    "    return all_episodes_length, all_penalties, all_rewards, q_table    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 OPS-TL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ops(env,train_episodes,past_policies):\n",
    "    \"\"\"\n",
    "    Training the agent using algorithm in paper \"An Optimal Online Method of Selecting \n",
    "    Source Policies for Reinforcement Learning\"\n",
    "    \"\"\"\n",
    "    # Hyper parameters\n",
    "    alpha = 0.05 #(alpha) is the learning rate (0<α≤1) \n",
    "    gamma = 0.95 # (gamma) is the discount factor (0≤γ≤1)\n",
    "    epsilon = 0.1 # the rate to act randomly in the epsilon-greedy alg for expolration in Q-learning\n",
    "    fi = 0.95 # the intial probability to reuse past policy in policy-reuse algo, decays after each step in one episode\n",
    "    c = 0.0049 # the factor in UCB-tuned deciding the rate of exploration, a lager c will lead to a higher exploration rate\n",
    "    \n",
    "    # To plot learning curve\n",
    "    all_episodes_length = []\n",
    "    all_penalties = []\n",
    "    all_rewards = []\n",
    "   \n",
    "    #win = {'reuse': [], 'new': []}\n",
    "    timesteps = train_episodes * MAX_EPISODE_LEN\n",
    "   \n",
    "    q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "    env_copy = copy.deepcopy(env)\n",
    "    \n",
    "    # pt = 1-k/(k+1500)\n",
    "    \n",
    "    #initialize the expected reward of every bandit/past_policy\n",
    "    policy_expect = np.zeros([len(past_policies)])\n",
    "    policy_times = np.zeros([len(past_policies)])\n",
    "    for j in range(len(past_policies)):\n",
    "        t = 0\n",
    "        env.reset()\n",
    "        _,policy_expect[j],_,_= policy_reuse(timesteps, env,q_table, env.reset(), t,[],[],[],past_policies[j])\n",
    "        policy_times[j] += 1\n",
    "        \n",
    "    # training strat  \n",
    "    i = 0\n",
    "    while i < timesteps:\n",
    "        env.seed(random.randint(0,timesteps))\n",
    "        state = env.reset()\n",
    "        \n",
    "        episode_penalties, episode_rewards, episode_length = 0, 0, 0\n",
    "        done = False\n",
    "        \n",
    "        k = i // MAX_EPISODE_LEN  + 1\n",
    "        pt = 1-k/(k+1500)\n",
    "        \n",
    "        # use UCB1 to solve MAB(multi armed bandit problem\n",
    "        if random.uniform(0,1) < pt:\n",
    "            j = arg_max(policy_expect + np.sqrt(c * np.log(np.sum(policy_times)) / policy_times )) # UCB1-tuned\n",
    "            episode_penalties, episode_rewards, episode_length,i = policy_reuse(timesteps, env, q_table, state, i,\n",
    "                                                                              all_episodes_length,all_penalties,all_rewards,\n",
    "                                                                              past_policy = past_policies[j])\n",
    "            policy_expect[j] = (policy_expect[j]*policy_times[j] + episode_rewards)/( policy_times[j] +1)\n",
    "            policy_times[j] += 1\n",
    "#            if episode_length < 200:\n",
    "#                win['reuse'].append(k)\n",
    "        # follow episilon-greedy strategy    \n",
    "        else:\n",
    "            episode_penalties, episode_rewards, episode_length,i = epsilon_greedy(timesteps, env, q_table, state, i , \n",
    "                                                                                all_episodes_length,all_penalties,all_rewards,\n",
    "                                                                                epsilon, alpha, gamma)\n",
    "#            if episode_length < 200:\n",
    "#                win['new'].append(k)        \n",
    "\n",
    "\n",
    "        #show training progress\n",
    "        if (k) % 100 == 0:\n",
    "            clear_output(wait=True)\n",
    "            print(f\"Episode {k}\")\n",
    "    print(f\"Training finished\")\n",
    "    \n",
    "    # return ndarray\n",
    "    all_episodes_length = np.array(all_episodes_length)\n",
    "    all_penalties = np.array(all_penalties)\n",
    "    all_rewards = np.array(all_rewards)\n",
    "    \n",
    "    return all_episodes_length, all_penalties, all_rewards, q_table    \n",
    "            \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(timesteps, env, q_table, initial_state, i,\n",
    "                   all_episodes_length,all_penalties,all_rewards,epsilon = 0.1, alpha = 0.05,  gamma = 0.95 ):\n",
    "    done = False\n",
    "    state = initial_state\n",
    "    episode_penalties, episode_rewards, episode_length = 0, 0, 0\n",
    "  \n",
    "    _env_copy = copy.deepcopy(env)\n",
    "    while not done and i < timesteps:\n",
    "        if random.uniform(0,1) < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = arg_max(q_table[state])\n",
    "            \n",
    "        # step to next state\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # update q-value\n",
    "        q_value = (1-alpha) * q_table[state, action] + alpha * (reward + gamma * np.max(q_table[next_state]))\n",
    "        q_table[state, action] = q_value\n",
    "\n",
    "        state = next_state\n",
    "      \n",
    "        #update data for learning curve\n",
    "        if reward == -10:\n",
    "            episode_penalties +=1\n",
    "\n",
    "        episode_rewards += reward\n",
    "        episode_length += 1\n",
    "        \n",
    "        if i%200 == 0:\n",
    "            #evaluate policy for learning curve after each episode\n",
    "            _episode_rewards, _episode_penalties, _episode_length = policy_evaluate(_env_copy, q_table, EVALUATION_TIMES)\n",
    "\n",
    "            all_episodes_length.append(_episode_length)\n",
    "            all_penalties.append(_episode_penalties)\n",
    "            all_rewards.append(_episode_rewards)\n",
    "\n",
    "            #show training progress\n",
    "            clear_output(wait=True)\n",
    "            print(f\"timesteps: {i}\")\n",
    "        i += 1\n",
    "\n",
    "    return episode_penalties, episode_rewards, episode_length,i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_reuse(timesteps,env, q_table, initial_state, i, \n",
    "                 all_episodes_length,all_penalties,all_rewards, past_policy,  fi = 0.95, mu=0.95, alpha = 0.05,  gamma = 0.95 ):\n",
    "    done = False\n",
    "    state = initial_state\n",
    "    episode_penalties, episode_rewards, episode_length = 0, 0, 0\n",
    "    \n",
    "    _env_copy = copy.deepcopy(env)\n",
    "    while not done and i < timesteps:\n",
    "        if random.uniform(0,1) < fi:\n",
    "            action = arg_max(past_policy[state])\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "            \n",
    "        # step to next state\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # update q-value\n",
    "        q_value = (1-alpha) * q_table[state, action] + alpha * (reward + gamma * np.max(q_table[next_state]))\n",
    "        q_table[state, action] = q_value\n",
    "\n",
    "        state = next_state\n",
    "        fi = fi*mu\n",
    "        #update data for learning curve\n",
    "        if reward == -10:\n",
    "            episode_penalties +=1\n",
    "\n",
    "        episode_rewards += reward\n",
    "        episode_length += 1\n",
    "        \n",
    "        if i%200 == 0:\n",
    "            #evaluate policy for learning curve after each episode\n",
    "            _episode_rewards, _episode_penalties, _episode_length = policy_evaluate(_env_copy, q_table, EVALUATION_TIMES)\n",
    "\n",
    "            all_episodes_length.append(_episode_length)\n",
    "            all_penalties.append(_episode_penalties)\n",
    "            all_rewards.append(_episode_rewards)\n",
    "\n",
    "            #show training progress\n",
    "            clear_output(wait=True)\n",
    "            print(f\"timesteps: {i}\")\n",
    "        i += 1\n",
    "        \n",
    "    return episode_penalties, episode_rewards, episode_length,i\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 caps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caps(env,train_episodes,past_policies):\n",
    "    \"\"\"\n",
    "    Training the agent using algorithm in paper \"Context-aware policy reuse\"\n",
    "    \"\"\"\n",
    "    # Hyper parameters\n",
    "    alpha = 0.05 #(alpha) is the learning rate (0<α≤1) \n",
    "    gamma = 0.95 # (gamma) is the discount factor (0≤γ≤1)\n",
    "    epsilon = 0.1 # the rate to act randomly in the epsilon-greedy alg for expolration in Q-learning\n",
    "    fi = 0.95 # the intial probability to reuse past policy in policy-reuse algo, decays after each step in one episode\n",
    "    c = 0.0049 # the factor in UCB-tuned deciding the rate of exploration, a lager c will lead to a higher exploration rate\n",
    "    \n",
    "    # To plot learning curve\n",
    "    all_episodes_length = []\n",
    "    all_penalties = []\n",
    "    all_rewards = []\n",
    "    all_frequency = np.zeros([len(past_policies)+1, train_episodes])\n",
    "#   win = {'reuse': [], 'new': []}\n",
    "    timesteps = train_episodes * MAX_EPISODE_LEN\n",
    "   \n",
    "    q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "    env_copy = copy.deepcopy(env)\n",
    "    \n",
    "    # pt = 1-k/(k+1500)\n",
    "    \n",
    "    #initialize the expected reward of every bandit/past_policy\n",
    "    frequency = np.zeros([len(past_policies)+1, train_episodes])\n",
    "    policy_expect = np.zeros([len(past_policies)])\n",
    "    policy_times = np.zeros([len(past_policies)])\n",
    "    for j in range(len(past_policies)):\n",
    "        t = 0\n",
    "        env.reset()\n",
    "        for _ in range(5):\n",
    "            _,temp,_,_= policy_reuse(timesteps, env,q_table, env.reset(), t,[],[],[],past_policies[j])\n",
    "            policy_expect[j] += temp\n",
    "        policy_expect[j] /= 10\n",
    "        policy_times[j] += 1\n",
    "        \n",
    "    # training strat  \n",
    "    i = 0\n",
    "    while i < timesteps:\n",
    "        env.seed(random.randint(0,timesteps))\n",
    "        state = env.reset()\n",
    "        \n",
    "        episode_penalties, episode_rewards, episode_length = 0, 0, 0\n",
    "        done = False\n",
    "        \n",
    "        k = i // MAX_EPISODE_LEN  + 1\n",
    "        pt = 1-k/(k+1500)\n",
    "        \n",
    "        # use UCB1 to solve MAB(multi armed bandit problem\n",
    "        if random.uniform(0,1) < pt:\n",
    "            j = arg_max(policy_expect/20 + np.sqrt(c * np.log(np.sum(policy_times)) / policy_times )) # UCB1-tuned\n",
    "            episode_penalties, episode_rewards, episode_length,i = policy_reuse(timesteps, env, q_table, state, i,\n",
    "                                                                              all_episodes_length,all_penalties,all_rewards,\n",
    "                                                                              past_policy = past_policies[j])\n",
    "            policy_expect[j] = (policy_expect[j]*policy_times[j] + episode_rewards)/( policy_times[j] +1)\n",
    "            policy_times[j] += 1\n",
    "            frequency[j][k-1] += 1\n",
    "#            if episode_length < 200:\n",
    "#                win['reuse'].append(k)\n",
    "        # follow episilon-greedy strategy    \n",
    "        else:\n",
    "            episode_penalties, episode_rewards, episode_length,i = epsilon_greedy(timesteps, env, q_table, state, i , \n",
    "                                                                                all_episodes_length,all_penalties,all_rewards,\n",
    "                                                                                epsilon, alpha, gamma)\n",
    "#            if episode_length < 200:\n",
    "#                win['new'].append(k)        \n",
    "            frequency[len(past_policies)][k-1] += 1\n",
    "\n",
    "\n",
    "    print(f\"Training finished\")\n",
    "    \n",
    "    # return ndarray\n",
    "    all_episodes_length = np.array(all_episodes_length)\n",
    "    all_penalties = np.array(all_penalties)\n",
    "    all_rewards = np.array(all_rewards)\n",
    "    for line in range(train_episodes):\n",
    "        for row in range(len(past_policies)+1):\n",
    "            all_frequency[row][line]=np.sum(frequency[row][:line+1], keepdims= True) /np.sum(np.sum(frequency[:,:line+1]))\n",
    "    return all_episodes_length, all_penalties, all_rewards, all_frequency, q_table    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 OC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oc(env ,episode_num ,  option_policies_lib = None, \n",
    "                  option_terminations_lib = None, policy_over_options = None, critic = None, noptions = 4,seed = 1 ):\n",
    "    \n",
    "    # Discount\n",
    "    discount = 0.99\n",
    "    \n",
    "\n",
    "    # Learning rates - termination, intra-option, critic\n",
    "    lr_term = 0.25\n",
    "    lr_intra = 0.25\n",
    "    lr_critic = 0.5\n",
    "\n",
    "    # Epsilon for epsilon-greedy for policy over options\n",
    "    epsilon = 1e-1\n",
    "\n",
    "    # Temperature for softmax\n",
    "    temperature = 0.01\n",
    "    \n",
    "    # To plot learning curve\n",
    "    all_episodes_length = []\n",
    "    all_penalties = []\n",
    "    all_rewards = []\n",
    "    \n",
    "\n",
    "    rng = np.random.RandomState(seed)\n",
    "    #trainning timesteps\n",
    "    timesteps =  episode_num * MAX_EPISODE_LEN\n",
    "\n",
    "    nstates = env.observation_space.n\n",
    "    nactions = env.action_space.n\n",
    "    \n",
    "    # for option_evaluate\n",
    "    env.seed(random.randint(0,timesteps))\n",
    "    state = env.reset()\n",
    "    env_copy = copy.deepcopy(env) \n",
    "\n",
    "    # Following three belong to the Actor\n",
    "\n",
    "    # 1. The intra-option policies - linear softmax functions\n",
    "    if option_policies_lib is None:\n",
    "        option_policies = [SoftmaxPolicy(rng, lr_intra, nstates, nactions, epsilon, temperature) for _ in range(noptions)]\n",
    "      \n",
    "       \n",
    "    else:\n",
    "        option_policies = [None] * noptions\n",
    "        for i in range(noptions):\n",
    "            option_policies[i] = copy.deepcopy(option_policies_lib[i])\n",
    "\n",
    "\n",
    "    # 2. The termination function - linear sigmoid function\n",
    "    if option_terminations_lib is None:\n",
    "        option_terminations = [SigmoidTermination(rng, lr_term, nstates) for _ in range(noptions)]\n",
    "        \n",
    "    else:\n",
    "        option_terminations =  [None] * noptions\n",
    "        for i in range(noptions):\n",
    "            option_terminations[i] = copy.deepcopy(option_terminations_lib[i])\n",
    "\n",
    "\n",
    "\n",
    "    # 3. The epsilon-greedy policy over options\n",
    "    if policy_over_options is None:\n",
    "        policy_over_options = EpsGreedyPolicy(rng, nstates, noptions, epsilon)\n",
    "\n",
    "        \n",
    "    else:\n",
    "        policy_over_options = copy.deepcopy(policy_over_options)\n",
    "\n",
    "\n",
    "    # Critic\n",
    "    if critic is None:\n",
    "        critic = Critic(lr_critic, discount, policy_over_options.Q_Omega_table, nstates, noptions, nactions)\n",
    "    else:\n",
    "        critic = copy.deepcopy(critic)\n",
    "\n",
    "   \n",
    "    \n",
    "    i=0\n",
    "    while i < timesteps:\n",
    "\n",
    "        # Change goal location after 1000 episodes \n",
    "        # Comment it for not doing transfer experiments\n",
    "#        if episode == 1000:\n",
    "#            env.goal = rng.choice(possible_next_goals)\n",
    "#            print('New goal: ', env.goal)\n",
    "\n",
    "        state = env.reset()\n",
    "\n",
    "        option = policy_over_options.sample(state)\n",
    "        action = option_policies[option].sample(state)\n",
    "\n",
    "        critic.cache(state, option, action)\n",
    "        \n",
    "        episode_penalties, episode_rewards, episode_length = 0, 0, 0\n",
    "        done = False\n",
    "        \n",
    "        \n",
    "        while not done and i < timesteps:\n",
    "            state, reward, done, _ = env.step(action)\n",
    "\n",
    "            # Termination might occur upon entering new state\n",
    "            if option_terminations[option].sample(state):\n",
    "                option = policy_over_options.sample(state)\n",
    "\n",
    "\n",
    "            action = option_policies[option].sample(state)\n",
    "\n",
    "            # Critic update\n",
    "            critic.update_Qs(state, option, action, reward, done, option_terminations)\n",
    "\n",
    "            # Intra-option policy update with baseline\n",
    "            Q_U = critic.Q_U(state, option, action)\n",
    "            Q_U = Q_U - critic.Q_Omega(state, option)\n",
    "            option_policies[option].update(state, action, Q_U)\n",
    "\n",
    "            # Termination condition update\n",
    "            option_terminations[option].update(state, critic.A_Omega(state, option))\n",
    "            \n",
    "          \n",
    "            if i%200 == 0:\n",
    "                #evaluate policy for learning curve after each episode\n",
    "                episode_rewards, episode_penalties, episode_length = option_evaluate(env_copy, option_policies, option_terminations, policy_over_options, EVALUATION_TIMES)\n",
    "                \n",
    "                all_episodes_length.append(episode_length)\n",
    "                all_penalties.append(episode_penalties)\n",
    "                all_rewards.append(episode_rewards)\n",
    "\n",
    "                #show training progress\n",
    "                clear_output(wait=True)\n",
    "                print(f\"timesteps: {i}\")\n",
    "            i += 1\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"Training finished\")\n",
    "    \n",
    "    # return ndarray\n",
    "    all_episodes_length = np.array(all_episodes_length)\n",
    "    all_penalties = np.array(all_penalties)\n",
    "    all_rewards = np.array(all_rewards)\n",
    "    \n",
    "    return all_episodes_length, all_penalties, all_rewards, option_policies, option_terminations, policy_over_options,critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def option_evaluate(env,  option_policies, option_terminations, policy_over_options , times):\n",
    "    # store data, make average and return\n",
    "    all_length, all_penalties, all_rewards = np.zeros(times), np.zeros(times), np.zeros(times)\n",
    "\n",
    "    for i in range(times):\n",
    "\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        episode_penalties, episode_rewards, episode_length = 0, 0, 0\n",
    "        \n",
    "        option = policy_over_options.evaluate(state)\n",
    "        action = option_policies[option].evaluate(state)\n",
    "       \n",
    "        for j in range(MAX_EPISODE_LEN):\n",
    "            # Termination might occur upon entering new state\n",
    "            if option_terminations[option].sample(state):\n",
    "                option = policy_over_options.evaluate(state)\n",
    "\n",
    "\n",
    "            action = option_policies[option].evaluate(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            state = next_state\n",
    "            #update data for learning curve\n",
    "            if reward == -10:\n",
    "                episode_penalties +=1\n",
    "\n",
    "            episode_rewards += reward\n",
    "            episode_length += 1\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        all_rewards[i] = episode_rewards\n",
    "        all_penalties[i] = episode_penalties\n",
    "        all_length[i] = episode_length\n",
    "    return np.mean(all_rewards), np.mean(all_penalties), np.mean(all_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  tranform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform( tl_algo, episode_num, repeat_times, target_task, \n",
    "              source_task= None, source_rl_algo = None, policy_library = None):\n",
    "    \n",
    "    \"\"\"\n",
    "    return all_episodes_length, all_penalties, all_rewards, all_frequency, all_trans_knowledge\n",
    "    \n",
    "    Situation 1:\n",
    "    no source task available, train agent on target task from scratch \n",
    "    \n",
    "    Situation 2:\n",
    "    source task is available but source policy unavailable, train agent on source task to get source policy, \n",
    "    then train agent on target task using knowledge from source policy\n",
    "    \n",
    "    Situation 3:\n",
    "    source policies are availale, reuse source policies to train agent on target task\n",
    "    \"\"\"\n",
    "    train_episodes = episode_num    \n",
    "    # data collected during trainning\n",
    "    all_episodes_length = np.zeros([repeat_times,train_episodes])\n",
    "    all_penalties = np.zeros([repeat_times,train_episodes])\n",
    "    all_rewards = np.zeros([repeat_times,train_episodes])\n",
    "    if tl_algo is caps and policy_library is not None:\n",
    "        all_frequency = np.zeros([len(policy_library[0])+1, train_episodes])\n",
    "    else:\n",
    "        all_frequency = None\n",
    "    \n",
    "    all_trans_knowledge = []\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in range(repeat_times):\n",
    "       \n",
    "        np.random.seed(i)\n",
    "        random.seed(i)\n",
    "        #Situation 3    \n",
    "        if policy_library is not None: \n",
    "            \n",
    "            if type(target_task) is str:\n",
    "                env = gym.make(target_task)\n",
    "            else:\n",
    "                env = target_task\n",
    "            past_policies = policy_library # policy_library = [ [[policy11][policy21]...] [[policy12][policy22]...]...]\n",
    "            if tl_algo is caps:\n",
    "                episodes,penalties,rewards,frequency, *knowledge = tl_algo(env, train_episodes, past_policies[i]) \n",
    "            else:\n",
    "                episodes,penalties,rewards, *knowledge = tl_algo(env, train_episodes,  past_policies[i]) \n",
    "        #Situation 2   \n",
    "        elif source_task is not None:\n",
    "            if source_rl_algo is None:\n",
    "                source_rl_algo = tl_algo\n",
    "            if type(source_task) is str:\n",
    "                env = gym.make(source_task)\n",
    "            else:\n",
    "                env = source_task\n",
    "            _, _,_ ,*knowledge = source_rl_algo(env,  train_episodes)\n",
    "            \n",
    "            if type(target_task) is str:\n",
    "                env = gym.make(target_task)\n",
    "            else:\n",
    "                env = target_task\n",
    "            episodes,penalties,rewards,*knowledge = tl_algo(env, train_episodes , *knowledge)\n",
    "         #Situation 1\n",
    "        else: \n",
    "            if type(target_task) is str:\n",
    "                env = gym.make(target_task)\n",
    "            else:\n",
    "                env = target_task\n",
    "                \n",
    "           \n",
    "            episodes,penalties,rewards,*knowledge = tl_algo(env, train_episodes)\n",
    "            \n",
    "\n",
    "        all_episodes_length[i] = episodes\n",
    "        all_penalties[i] = penalties\n",
    "        all_rewards[i] = rewards\n",
    "        if tl_algo is caps and policy_library is not None:\n",
    "            all_frequency += frequency\n",
    "        if len(knowledge) > 1:\n",
    "            all_trans_knowledge.append(knowledge)\n",
    "        else:\n",
    "            all_trans_knowledge.append(*knowledge)\n",
    "\n",
    "    \n",
    "    if tl_algo is caps and policy_library is not None:\n",
    "        all_frequency /= repeat_times\n",
    "    \n",
    "    return all_episodes_length, all_penalties, all_rewards, all_frequency, all_trans_knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0.])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a= np.zeros([2,3])\n",
    "a[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CAPS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-abc5b5252d29>\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(target_rl_algo, episode_num, repeat_times, target_task, source_task, source_rl_algo, policy_library)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mall_penalties\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrepeat_times\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_episodes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mall_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrepeat_times\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_episodes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mtarget_rl_algo\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mCAPS\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mpolicy_library\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mall_frequency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_library\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_episodes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CAPS' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "qlearn_episodes_length_fourroom, qlearn_penalties_fourroom, qlearn_rewards_fourroom, qlearn_q_table_fourroom = [[None] * len(four_room_envs) for _ in range(4)]\n",
    "for i in range(len(four_room_envs)):\n",
    "    qlearn_episodes_length_fourroom[i], qlearn_penalties_fourroom[i], qlearn_rewards_fourroom[i], _, qlearn_q_table_fourroom[i] = transform(tl_algo = q_learning,\n",
    "                                                                                     episode_num = EPISODE_NUM, \n",
    "                                                                                   repeat_times = REPEAT_TIMES,\n",
    "                                                                                     target_task = four_room_envs[i],\n",
    "                                                                                     source_task = None\n",
    "                                                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CAPS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-abc5b5252d29>\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(target_rl_algo, episode_num, repeat_times, target_task, source_task, source_rl_algo, policy_library)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mall_penalties\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrepeat_times\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_episodes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mall_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrepeat_times\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_episodes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mtarget_rl_algo\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mCAPS\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mpolicy_library\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mall_frequency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_library\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_episodes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CAPS' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# training Taxi-v003 from scratch\n",
    "qlearn_episodes_length_v003, qlearn_penalties_v003, qlearn_rewards_v003, _, qlearn_q_table_v003 = transform(tl_algo = q_learning,\n",
    "                                                                                     episode_num = EPISODE_NUM, \n",
    "                                                                                     repeat_times = REPEAT_TIMES,\n",
    "                                                                                     target_task = \"Taxi-v003\",\n",
    "                                                                                     source_task = None\n",
    "                                                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 799800\n",
      "Training finished\n",
      "CPU times: user 12min 15s, sys: 1min 8s, total: 13min 24s\n",
      "Wall time: 14min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# training Taxi-v000 from scratch\n",
    "qlearn_episodes_length_v000, qlearn_penalties_v000, qlearn_rewards_v000, _, qlearn_q_table_v000 = transform(tl_algo = q_learning,\n",
    "                                                                                     episode_num = EPISODE_NUM, \n",
    "                                                                                     repeat_times = REPEAT_TIMES,\n",
    "                                                                                     target_task = \"Taxi-v000\",\n",
    "                                                                                     source_task = None\n",
    "                                                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 799800\n",
      "Training finished\n",
      "CPU times: user 12min 4s, sys: 1min 14s, total: 13min 19s\n",
      "Wall time: 14min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# training Taxi-v001 from scratch\n",
    "qlearn_episodes_length_v001, qlearn_penalties_v001, qlearn_rewards_v001,_, qlearn_q_table_v001 = transform(tl_algo = q_learning,\n",
    "                                                                                     episode_num = EPISODE_NUM, \n",
    "                                                                                     repeat_times = REPEAT_TIMES,\n",
    "                                                                                     target_task = \"Taxi-v001\",\n",
    "                                                                                     source_task = None\n",
    "                                                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 799800\n",
      "Training finished\n",
      "CPU times: user 12min 55s, sys: 1min 27s, total: 14min 22s\n",
      "Wall time: 15min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# training Taxi-v004 from scratch\n",
    "qlearn_episodes_length_v004, qlearn_penalties_v004, qlearn_rewards_v004,_, qlearn_q_table_v004 = transform(tl_algo = q_learning,\n",
    "                                                                                     episode_num = EPISODE_NUM, \n",
    "                                                                                     repeat_times = REPEAT_TIMES,\n",
    "                                                                                     target_task = \"Taxi-v004\",\n",
    "                                                                                     source_task = None\n",
    "                                                                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 799800\n",
      "Training finished \n",
      "\n",
      "CPU times: user 10min 37s, sys: 1min 44s, total: 12min 21s\n",
      "Wall time: 12min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# training Taxi-v003 from scratch\n",
    "sarsa_episodes_length_v003, sarsa_penalties_v003, sarsa_rewards_v003, _, sarsa_q_table_v003 = transform(tl_algo = sarsa,\n",
    "                                                                                     episode_num = EPISODE_NUM, \n",
    "                                                                                     repeat_times = REPEAT_TIMES,\n",
    "                                                                                     target_task = \"Taxi-v003\",\n",
    "                                                                                     source_task = None\n",
    "                                                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 799800\n",
      "Training finished \n",
      "\n",
      "CPU times: user 7min 21s, sys: 2min 3s, total: 9min 24s\n",
      "Wall time: 8min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# training Taxi-v001 from scratch\n",
    "sarsa_episodes_length_v001, sarsa_penalties_v001, sarsa_rewards_v001,_, sarsa_q_table_v001 = transform(tl_algo = sarsa,\n",
    "                                                                                     episode_num = EPISODE_NUM, \n",
    "                                                                                     repeat_times = REPEAT_TIMES,\n",
    "                                                                                     target_task = \"Taxi-v001\",\n",
    "                                                                                     source_task = None\n",
    "                                                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 799800\n",
      "Training finished \n",
      "\n",
      "CPU times: user 7min 58s, sys: 2min 29s, total: 10min 27s\n",
      "Wall time: 8min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# training Taxi-v004 from scratch\n",
    "sarsa_episodes_length_v004, sarsa_penalties_v004, sarsa_rewards_v004,_, sarsa_q_table_v004 = transform(tl_algo = sarsa,\n",
    "                                                                                     episode_num = EPISODE_NUM, \n",
    "                                                                                     repeat_times = REPEAT_TIMES,\n",
    "                                                                                     target_task = \"Taxi-v004\",\n",
    "                                                                                     source_task = None\n",
    "                                                                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 PRQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'qlearn_q_table_fourroom' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'qlearn_q_table_fourroom' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pr_episodes_length_fourroom, pr_penalties_fourroom, pr_rewards_fourroom, pr_q_table_fourroom = [[None] * (len(four_room_envs)-1) for _ in range(4)]\n",
    "\n",
    "#pr_fourroom_v012_3\n",
    "for i in range(len(four_room_envs)-1):\n",
    "    pr_episodes_length_fourroom[i], pr_penalties_fourroom[i], pr_rewards_fourroom[i], _, pr_q_table_fourroom[i] = transform(tl_algo = prql,\n",
    "                                                                                     episode_num = EPISODE_NUM, \n",
    "                                                                                   repeat_times = REPEAT_TIMES,\n",
    "                                                                                     target_task = four_room_envs[3],\n",
    "                                                                                     policy_library = qlearn_q_table_fourroom[i]\n",
    "                                                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "randint() missing 1 required positional argument: 'b'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-abc5b5252d29>\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(target_rl_algo, episode_num, repeat_times, target_task, source_task, source_rl_algo, policy_library)\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0mepisodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpenalties\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfrequency\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mknowledge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_rl_algo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_episodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast_policies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m                 \u001b[0mepisodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpenalties\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mknowledge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_rl_algo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_episodes\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mpast_policies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;31m#Situation 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0msource_task\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-f3c5f7f96693>\u001b[0m in \u001b[0;36mprql\u001b[0;34m(env, train_episodes, past_policy)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtimesteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimesteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: randint() missing 1 required positional argument: 'b'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#pr_v014_003\n",
    "pr_episodes_length_v014_003, pr_penalties_v014_003,pr_rewards_v014_003, _, pr_q_table_v014_003 = transform(tl_algo= prql,\n",
    "                                                                                       episode_num = EPISODE_NUM,\n",
    "                                                                                       repeat_times = REPEAT_TIMES,\n",
    "                                                                                       target_task = \"Taxi-v003\",\n",
    "                                                                                       \n",
    "                                                                                       policy_library =  qlearn_q_table_v004)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3 OPS-TL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'qlearn_q_table_fourroom' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'qlearn_q_table_fourroom' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "ops_episodes_length_fourroom, ops_penalties_fourroom, ops_rewards_fourroom, ops_q_table_fourroom = [[None] * (len(four_room_envs)-1) for _ in range(4)]\n",
    "\n",
    "#ops_fourroom_v012_3\n",
    "\n",
    "ops_episodes_length_fourroom, ops_penalties_fourroom, ops_rewards_fourroom, _, ops_q_table_fourroom = transform(tl_algo = OPS_TL,\n",
    "                                                                                     episode_num = EPISODE_NUM, \n",
    "                                                                                   repeat_times = REPEAT_TIMES,\n",
    "                                                                                     target_task = four_room_envs[3],\n",
    "                                                                                     policy_library =  [[q_func1,q_func2,q_func3 ] for q_func1,q_func2,q_func3 in zip(qlearn_q_table_fourroom[0],qlearn_q_table_fourroom[1],qlearn_q_table_fourroom[2])]\n",
    "                                                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'qlearn_q_table_v000' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'qlearn_q_table_v000' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#ops_v014_003\n",
    "ops_episodes_length_v014_003, ops_penalties_v014_003,ops_rewards_v014_003,ops_fre_v014_003, ops_q_table_v014_003 = transform(tl_algo= OPS_TL,\n",
    "                                                                                       episode_num = EPISODE_NUM,\n",
    "                                                                                       repeat_times = REPEAT_TIMES,\n",
    "                                                                                       target_task = \"Taxi-v003\",\n",
    "                                                                                      \n",
    "                                                                                       policy_library = [[q_func1,q_func2,q_func3 ] for q_func1,q_func2,q_func3 in zip(qlearn_q_table_v000, qlearn_q_table_v001, qlearn_q_table_v004)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 caps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'qlearn_q_table_fourroom' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'qlearn_q_table_fourroom' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "#caps_fourroom_v012_3\n",
    "\n",
    "caps_episodes_length_fourroom, caps_penalties_fourroom, caps_rewards_fourroom, _, caps_q_table_fourroom = transform(tl_algo = caps,\n",
    "                                                                                     episode_num = EPISODE_NUM, \n",
    "                                                                                   repeat_times = REPEAT_TIMES,\n",
    "                                                                                     target_task = four_room_envs[3],\n",
    "                                                                                     policy_library =  [[q_func1,q_func2,q_func3 ] for q_func1,q_func2,q_func3 in zip(qlearn_q_table_fourroom[0],qlearn_q_table_fourroom[1],qlearn_q_table_fourroom[2])]\n",
    "                                                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'qlearn_q_table_v000' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'qlearn_q_table_v000' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#caps_v014_003\n",
    "caps_episodes_length_v014_003, caps_penalties_v014_003,caps_rewards_v014_003,caps_fre_v014_003, caps_q_table_v014_003 = transform(tl_algo= caps,\n",
    "                                                                                       episode_num = EPISODE_NUM,\n",
    "                                                                                       repeat_times = REPEAT_TIMES,\n",
    "                                                                                       target_task = \"Taxi-v003\",\n",
    "                                                                                      \n",
    "                                                                                       policy_library = [[q_func1,q_func2,q_func3 ] for q_func1,q_func2,q_func3 in zip(qlearn_q_table_v000, qlearn_q_table_v001, qlearn_q_table_v004)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 OC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Line magic function `%autoreload` not found.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%autoreload 2\n",
    "option_policies, option_terminations, policy_over_options, critic, nrewards = [ [None]*4 for i in range(5) ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 799800\n",
      "Training finished\n",
      "CPU times: user 13min 44s, sys: 2min 30s, total: 16min 15s\n",
      "Wall time: 14min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for i in [0,1,2,3]:\n",
    "    _, _ ,nrewards[i] ,option_policies[i], option_terminations[i], policy_over_options[i],critic[i]= oc( four_room_envs[i], 4000, noptions=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-253-07e25a4c40f4>\u001b[0m in \u001b[0;36moption_critic\u001b[0;34m(env, episode_num, option_policies_lib, option_terminations_lib, policy_over_options, critic, noptions)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0moption_policies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnoptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0moption_policies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moption_policies_lib\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trans_rewards,trans_terminations, trans_policies = [ [None]*4 for _ in range(3)]\n",
    " \n",
    "for i in [0,1,2]:\n",
    "    _,_, trans_rewards[i], trans_policies[i], trans_terminations[i], _,_= oc( four_room_envs[3], 4000, option_policies[i], option_terminations[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 799800\n",
      "Training finished\n"
     ]
    }
   ],
   "source": [
    "source_lib = [1,2]\n",
    "_,_, trans_rewards_012, _, _, _,_= oc( four_room_envs[3], 4000, [option_policies[i][0] for i in source_lib], noptions=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc810a14b62f456bb382c2c8a2b95a7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "import seaborn as sns\n",
    "RATIO = 1\n",
    "SMOOTH_RADIUS = 50\n",
    "sns.set(style = \"darkgrid\")\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(qlearn_rewards_fourroom[2], SMOOTH_RADIUS), label = \"NoneTransfer\")\n",
    "#plt.plot(range(EPISODE_NUM), smooth(qlearn_rewards_v5, 100), label = \"qlearn_v5\")\n",
    "\n",
    "#plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v000_003,SMOOTH_RADIUS), label = \"caps_v0_003\",color='b')\n",
    "#plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v001_003,SMOOTH_RADIUS), label = \"caps_v1_003\",color='m')\n",
    "#plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v004_003,SMOOTH_RADIUS), label = \"caps_v4_003\",color='y')\n",
    "i = 0\n",
    "#plt.plot(range(EPISODE_NUM//RATIO), smooth(pr_rewards_fourroom[i],SMOOTH_RADIUS), label = \"pr_v014_003\",color='c')\n",
    "#plt.plot(range(EPISODE_NUM//RATIO), smooth(tl_rewards_fourroom,SMOOTH_RADIUS), label = \"ops_v014_003\",color='g')\n",
    "#plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_fourroom,SMOOTH_RADIUS), label = \"caps_v014_003\",color='r')\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(trans_rewards_012,SMOOTH_RADIUS), label = \"oc_v01_3\")\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(nrewards[3], SMOOTH_RADIUS), label = \"oc_v3\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.title(\"0-4000 episode-reward graph\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"rewards\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc98862b686745548d8e84d04a8f09b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'pr_rewards_fourroom' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-242-e296c1cbaef0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v004_003,SMOOTH_RADIUS), label = \"ops_norm_v4_003\",color='y')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPISODE_NUM\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mRATIO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmooth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpr_rewards_fourroom\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mEPISODE_NUM\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mRATIO\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mSMOOTH_RADIUS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"pr_v014_003\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPISODE_NUM\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mRATIO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmooth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtl_rewards_fourroom\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mEPISODE_NUM\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mRATIO\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mSMOOTH_RADIUS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ops_v014_003\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'g'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPISODE_NUM\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mRATIO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmooth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops_rewards_fourroom\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mEPISODE_NUM\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mRATIO\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mSMOOTH_RADIUS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"caps_v014_003\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pr_rewards_fourroom' is not defined"
     ]
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "RATIO = 4\n",
    "SMOOTH_RADIUS = 100\n",
    "sns.set(style = \"darkgrid\")\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(qlearn_rewards_fourroom[3][:EPISODE_NUM//RATIO], SMOOTH_RADIUS), label = \"qlearn_v003\", color='k')\n",
    "#plt.plot(range(EPISODE_NUM), smooth(qlearn_rewards_v5, 100), label = \"qlearn_v5\")\n",
    "\n",
    "#plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v000_003,SMOOTH_RADIUS), label = \"caps_v0_003\",color='b')\n",
    "#plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v001_003,SMOOTH_RADIUS), label = \"caps_v1_003\",color='m')\n",
    "#plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v004_003,SMOOTH_RADIUS), label = \"caps_v4_003\",color='y')\n",
    "\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(pr_rewards_fourroom[0][:EPISODE_NUM//RATIO],SMOOTH_RADIUS), label = \"pr_v014_003\",color='c')\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(tl_rewards_fourroom[:EPISODE_NUM//RATIO],SMOOTH_RADIUS), label = \"ops_v014_003\",color='g')\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_fourroom[:EPISODE_NUM//RATIO],SMOOTH_RADIUS), label = \"caps_v014_003\",color='r')\n",
    "#plt.plot(range(EPISODE_NUM//RATIO), smooth(trans_rewards[3][:EPISODE_NUM//RATIO],SMOOTH_RADIUS), label = \"oc_v014_003\",color='m')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.title(\"0-1000 episode-reward graph\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"rewards\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pr_rewards_v014_003' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-372-8aae36421dd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v004_003,SMOOTH_RADIUS), label = \"ops_norm_v4_003\",color='y')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPISODE_NUM\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mRATIO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmooth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpr_rewards_v014_003\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mSMOOTH_RADIUS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"pr_v014_003\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPISODE_NUM\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mRATIO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmooth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtl_rewards_v014_003\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mSMOOTH_RADIUS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ops_v014_003\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'g'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPISODE_NUM\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mRATIO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmooth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops_rewards_v014_003\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mSMOOTH_RADIUS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"caps_v014_003\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pr_rewards_v014_003' is not defined"
     ]
    }
   ],
   "source": [
    "#%matplotlib widget\n",
    "RATIO = 1\n",
    "SMOOTH_RADIUS = 50\n",
    "sns.set(style = \"darkgrid\")\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(qlearn_rewards_v003, SMOOTH_RADIUS), label = \"qlearn_v003\", color='k')\n",
    "#plt.plot(range(EPISODE_NUM), smooth(qlearn_rewards_v5, 100), label = \"qlearn_v5\")\n",
    "\n",
    "#plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v000_003,SMOOTH_RADIUS), label = \"caps_v0_003\",color='b')\n",
    "#plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v001_003,SMOOTH_RADIUS), label = \"caps_v1_003\",color='m')\n",
    "#plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v004_003,SMOOTH_RADIUS), label = \"caps_v4_003\",color='y')\n",
    "\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(pr_rewards_v014_003,SMOOTH_RADIUS), label = \"pr_v014_003\",color='c')\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(tl_rewards_v014_003,SMOOTH_RADIUS), label = \"ops_v014_003\",color='g')\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v014_003,SMOOTH_RADIUS), label = \"caps_v014_003\",color='r')\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(trans_rewards[3],SMOOTH_RADIUS), label = \"oc_v014_003\",color='m')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.title(\"0-4000 episode-reward graph\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"rewards\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RATIO = 4\n",
    "SMOOTH_RADIUS = 50\n",
    "sns.set(style = \"darkgrid\")\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(qlearn_rewards_v003[:EPISODE_NUM//RATIO], SMOOTH_RADIUS), label = \"qlearn_v003\", color='k')\n",
    "#plt.plot(range(EPISODE_NUM), smooth(qlearn_rewards_v5, 100), label = \"qlearn_v5\")\n",
    "\n",
    "#plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v000_003,SMOOTH_RADIUS), label = \"caps_v0_003\",color='b')\n",
    "#plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v001_003,SMOOTH_RADIUS), label = \"caps_v1_003\",color='m')\n",
    "#plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v004_003,SMOOTH_RADIUS), label = \"caps_v4_003\",color='y')\n",
    "\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(pr_rewards_v014_003[:EPISODE_NUM//RATIO],SMOOTH_RADIUS), label = \"pr_v014_003\",color='c')\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(tl_rewards_v014_003[:EPISODE_NUM//RATIO],SMOOTH_RADIUS), label = \"ops_v014_003\",color='g')\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v014_003[:EPISODE_NUM//RATIO],SMOOTH_RADIUS), label = \"caps_v014_003\",color='r')\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(trans_rewards[3][:EPISODE_NUM//RATIO],SMOOTH_RADIUS), label = \"oc_v014_003\",color='m')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.title(\"0-1000 episode-reward graph\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"rewards\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "RATIO = 1\n",
    "SMOOTH_RADIUS = 100\n",
    "sns.set(style = \"darkgrid\")\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_fre_v01_003[0],SMOOTH_RADIUS), label = 'ploicy_v001')\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_fre_v01_003[1],SMOOTH_RADIUS), label = 'ploicy_v004')\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_fre_v01_003[2],SMOOTH_RADIUS), label = 'target policy')\n",
    "plt.legend()\n",
    "plt.xlabel('episodes')\n",
    "plt.ylabel('frequency')\n",
    "plt.title(\"frequency of policy selection\")\n",
    "plt.show()\n",
    "#plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_fre_v04_003,SMOOTH_RADIUS), label = \"caps_v04_003\",color='g')\n",
    "#plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_fre_v14_003,SMOOTH_RADIUS), label = \"caps_v14_003\",color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RATIO = 10\n",
    "SMOOTH_RADIUS = 20\n",
    "\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(qlearn_rewards_v003[:EPISODE_NUM//RATIO], SMOOTH_RADIUS), label = \"qlearn_v003\", color='k')\n",
    "#plt.plot(range(EPISODE_NUM), smooth(qlearn_rewards_v5, 100), label = \"qlearn_v5\")\n",
    "\n",
    "#plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v000_003[:EPISODE_NUM//RATIO],SMOOTH_RADIUS), label = \"caps_v0_003\",color='b')\n",
    "#plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v001_003[:EPISODE_NUM//RATIO],SMOOTH_RADIUS), label = \"caps_v1_003\",color='m')\n",
    "#plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v004_003[:EPISODE_NUM//RATIO],SMOOTH_RADIUS), label = \"caps_v4_003\",color='y')\n",
    "\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v01_003[:EPISODE_NUM//RATIO],SMOOTH_RADIUS), label = \"caps_v01_003\",color='c')\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v04_003[:EPISODE_NUM//RATIO],SMOOTH_RADIUS), label = \"caps_v04_003\",color='g')\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v14_003[:EPISODE_NUM//RATIO],SMOOTH_RADIUS), label = \"caps_v14_003\",color='r')\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v014_003[:EPISODE_NUM//RATIO], SMOOTH_RADIUS), label = \"caps_v014_003\", color='navy')\n",
    "\n",
    "plt.title(\"first 400 episode-reward graph\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"rewards\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#ops_v1_003\n",
    "ops_episodes_length_v1_003, ops_penalties_v1_003,ops_rewards_v1_003, ops_q_table_v1_003 = transform(tl_algo= OPS_TL,\n",
    "                                                                                       episode_num = EPISODE_NUM,\n",
    "                                                                                       repeat_times = REPEAT_TIMES,\n",
    "                                                                                       target_task = \"Taxi-v003\",\n",
    "                                                                                       source_task = \"Taxi-v1\",\n",
    "                                                                                       policy_library = [[q_func] for q_func in qlearn_q_table_v1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#ops_v4_003\n",
    "ops_episodes_length_v4_003, ops_penalties_v4_003,ops_rewards_v4_003, ops_q_table_v4_003 = transform(tl_algo= OPS_TL,\n",
    "                                                                                       episode_num = EPISODE_NUM,\n",
    "                                                                                       repeat_times = REPEAT_TIMES,\n",
    "                                                                                       target_task = \"Taxi-v003\",\n",
    "                                                                                       source_task = \"Taxi-v4\",\n",
    "                                                                                       policy_library = [[q_func] for q_func in qlearn_q_table_v4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#ops_v14_003\n",
    "ops_episodes_length_v14_003, ops_penalties_v14_003,ops_rewards_v14_003, ops_q_table_v14_003 = transform(tl_algo= OPS_TL,\n",
    "                                                                                       episode_num = EPISODE_NUM,\n",
    "                                                                                       repeat_times = REPEAT_TIMES,\n",
    "                                                                                       target_task = \"Taxi-v003\",\n",
    "                                                                                       source_task = \"Taxi-v4\",\n",
    "                                                                                       policy_library = [[q_func1,q_func2 ] for q_func1,q_func2 in zip(qlearn_q_table_v1, qlearn_q_table_v4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#caps_v14_003\n",
    "caps_episodes_length_v14_003, ops__norm_penalties_v14_003,caps_rewards_v14_003, caps_q_table_v14_003 = transform(tl_algo= caps,\n",
    "                                                                                       episode_num = EPISODE_NUM,\n",
    "                                                                                       repeat_times = 5,\n",
    "                                                                                       target_task = \"Taxi-v003\",\n",
    "                                                                                       source_task = \"Taxi-v4\",\n",
    "                                                                                       policy_library = [[q_func1,q_func2 ] for q_func1,q_func2 in zip(qlearn_q_table_v1, qlearn_q_table_v4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "#caps_v14_003\n",
    "pl=[[q_func1,q_func2 ] for q_func1,q_func2 in zip(qlearn_q_table_v1, qlearn_q_table_v4)]\n",
    "ctuple = (0, 0.0049, 0.1, 0.2, 0.4,0.8, 1, 2, 4, 8, 16)\n",
    "ops_cnorm_episodes_length_v14_003,ops__cnorm_penalties_v14_003,ops_cnorm_rewards_v14_003,ops_cnorm_q_table_v14_003 = [0] * len(ctuple), [0]*len(ctuple),[0]*len(ctuple), [0]*len(ctuple)\n",
    "for i in range(len(ctuple)):\n",
    "    OPS_patial = functools.partial( caps, c = ctuple[i])\n",
    "    ops_cnorm_episodes_length_v14_003[i], ops__cnorm_penalties_v14_003[i],ops_cnorm_rewards_v14_003[i], ops_cnorm_q_table_v14_003[i] = transform(tl_algo= OPS_patial,\n",
    "               episode_num = EPISODE_NUM,\n",
    "               repeat_times = REPEAT_TIMES,\n",
    "               target_task = \"Taxi-v003\",\n",
    "               policy_library = pl)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style = \"darkgrid\")\n",
    "plt.plot(range(EPISODE_NUM), smooth(qlearn_rewards_v003, 100), label = \"qlearn_v003\", color='k')\n",
    "for i in range(len(ctuple)):\n",
    "    plt.plot(range(EPISODE_NUM), smooth(ops_cnorm_rewards_v14_003[i], 100), label = \"c = \"+str(ctuple[i]))\n",
    "    \n",
    "plt.title(\"episode-reward graph\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"rewards\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RATIO = 20\n",
    "SMOOTH_RADIUS = 200\n",
    "\n",
    "\n",
    "for i in range(len(ctuple)):\n",
    "    plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_cnorm_rewards_v14_003[i][:EPISODE_NUM//RATIO], SMOOTH_RADIUS), label = \"c = \"+str(ctuple[i]))\n",
    "    \n",
    "plt.title(\"reward-epsidoe graph using caps different c\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"rewards\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ctuple)):\n",
    "    print(f\"c = {ctuple[i]}, reward = {smooth(ops_cnorm_rewards_v14_003[i][:EPISODE_NUM//20], 200)[0]}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.rcParams['font.sans-serif'] = ['SimHei']\n",
    "matplotlib.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "reward = [smooth( ops_cnorm_rewards_v14_003[i][:EPISODE_NUM//20], 200)[0] for i in range(len(ctuple)) ]\n",
    "\"\"\"\n",
    "绘制水平条形图方法barh\n",
    "参数一：y轴\n",
    "参数二：x轴\n",
    "\"\"\"\n",
    "plt.barh(range(len(ctuple)), reward, height=0.7, color='steelblue', alpha=0.8)      # 从下往上画\n",
    "plt.yticks(range(len(ctuple)), [f\"c={str(ctuple[i])}\" for i in range(len(ctuple))])\n",
    "plt.xlim(15,20)\n",
    "plt.xlabel(\"average reward \")\n",
    "plt.title(\"average reward over first 200 episode under different c\")\n",
    "for x, y in enumerate(reward):\n",
    "    plt.text(y + 0.2, x - 0.1, '%s' % y)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "sns.set(style = \"darkgrid\")\n",
    "\n",
    "\n",
    "plt.plot(range(EPISODE_NUM), smooth(qlearn_rewards_v003, 100), label = \"qlearn_v003\", color='k')\n",
    "#plt.plot(range(EPISODE_NUM), smooth(qlearn_rewards_v5, 100), label = \"qlearn_v5\")\n",
    "\n",
    "#plt.plot(range(EPISODE_NUM), smooth(qlearn_rewards_v1_3,100), label = \"qlearn_v1_3\",color='c')\n",
    "#plt.plot(range(EPISODE_NUM), smooth(qlearn_rewards_v4_3,100), label = \"qlearn_v4_3\",color='g')\n",
    "\n",
    "#plt.plot(range(EPISODE_NUM), smooth(pr_rewards_v4_3, 100), label = \"pr_v4_3\")\n",
    "#plt.plot(range(EPISODE_NUM), smooth(pr_rewards_v1_3, 100), label = \"pr_v1_3\")\n",
    "\n",
    "plt.plot(range(EPISODE_NUM), smooth(ops_rewards_v4_003, 100), label = \"ops_v4_003\", color='r')\n",
    "plt.plot(range(EPISODE_NUM), smooth(ops_rewards_v1_003, 100), label = \"ops_v1_003\", color='b')\n",
    "\n",
    "plt.plot(range(EPISODE_NUM), smooth(ops_rewards_v14_003, 100), label = \"ops_v14_003\", color='y')\n",
    "plt.plot(range(EPISODE_NUM), smooth(caps_rewards_v14_003, 100), label = \"caps_v14_003\", color='navy')\n",
    "\n",
    "plt.title(\"episode-reward graph\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"rewards\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "RATIO = 8\n",
    "SMOOTH_RADIUS = 25\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(qlearn_rewards_v003[:EPISODE_NUM//RATIO], SMOOTH_RADIUS), label = \"qlearn_v003\", color='k')\n",
    "#plt.plot(range(EPISODE_NUM), smooth(qlearn_rewards_v5, 100), label = \"qlearn_v5\")\n",
    "\n",
    "#plt.plot(range(EPISODE_NUM), smooth(qlearn_rewards_v1_3,100), label = \"qlearn_v1_3\",color='c')\n",
    "#plt.plot(range(EPISODE_NUM), smooth(qlearn_rewards_v4_3,100), label = \"qlearn_v4_3\",color='g')\n",
    "\n",
    "#plt.plot(range(EPISODE_NUM), smooth(pr_rewards_v4_3, 100), label = \"pr_v4_3\")\n",
    "#plt.plot(range(EPISODE_NUM), smooth(pr_rewards_v1_3, 100), label = \"pr_v1_3\")\n",
    "\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v4_003[:EPISODE_NUM//RATIO], SMOOTH_RADIUS), label = \"ops_v4_003\", color='r')\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v1_003[:EPISODE_NUM//RATIO], SMOOTH_RADIUS), label = \"ops_v1_003\", color='b')\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(ops_rewards_v14_003[:EPISODE_NUM//RATIO], SMOOTH_RADIUS), label = \"ops_v14_003\", color='y')\n",
    "plt.plot(range(EPISODE_NUM//RATIO), smooth(caps_rewards_v14_003[:EPISODE_NUM//RATIO], SMOOTH_RADIUS), label = \"caps_v14_003\", color='navy')\n",
    "\n",
    "plt.title(\"episode-reward graph\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"rewards\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RATIO = 8\n",
    "SMOOTH_WINDOW = 20\n",
    "plt.plot(range(EPISODE_NUM//RATIO),smooth(qlearn_episodes_length_v003[:EPISODE_NUM//RATIO],SMOOTH_WINDOW), label = 'qlearn_v3')\n",
    "'''\n",
    "plt.plot(range(train_episodes),qlearn_episodes_length_v1_3, label = 'qlearn_v1_3')\n",
    "plt.plot(range(train_episodes),qlearn_episodes_length_v4_3, label = 'qlearn_v4_3')\n",
    "\n",
    "plt.plot(range(train_episodes),sarsa_episodes_length_v3, label = 'sarsa_v3')\n",
    "plt.plot(range(train_episodes),sarsa_episodes_length_v1_3, label = 'sarsa_v1_3')\n",
    "plt.plot(range(train_episodes),sarsa_episodes_length_v4_3, label = 'sarsa_v4_3')\n",
    "'''\n",
    "plt.plot(range(EPISODE_NUM//RATIO),smooth(ops_episodes_length_v1_003[:EPISODE_NUM//RATIO],SMOOTH_WINDOW), label = 'ops_v1_3')\n",
    "plt.plot(range(EPISODE_NUM//RATIO),smooth(ops_episodes_length_v4_003[:EPISODE_NUM//RATIO],SMOOTH_WINDOW), label = 'ops_v4_3')\n",
    "\n",
    "plt.title(\"episode-length graph\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"length\")\n",
    "plt.legend()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RATIO = 20\n",
    "SMOOTH_RADIUS = 20\n",
    "plt.plot(range(EPISODE_NUM//RATIO),smooth(qlearn_penalties_v03[:EPISODE_NUM//RATIO],SMOOTH_RADIUS), label = 'qlearn_v3')\n",
    "'''\n",
    "plt.plot(range(train_episodes),qlearn_episodes_length_v1_3, label = 'qlearn_v1_3')\n",
    "plt.plot(range(train_episodes),qlearn_episodes_length_v4_3, label = 'qlearn_v4_3')\n",
    "\n",
    "plt.plot(range(train_episodes),sarsa_episodes_length_v3, label = 'sarsa_v3')\n",
    "plt.plot(range(train_episodes),sarsa_episodes_length_v1_3, label = 'sarsa_v1_3')\n",
    "plt.plot(range(train_episodes),sarsa_episodes_length_v4_3, label = 'sarsa_v4_3')\n",
    "'''\n",
    "plt.plot(range(EPISODE_NUM//RATIO),smooth(ops_penalties_v1_03[:EPISODE_NUM//RATIO],SMOOTH_RADIUS), label = 'ops_v1_3')\n",
    "plt.plot(range(EPISODE_NUM//RATIO),smooth(ops_penalties_v4_03[:EPISODE_NUM//RATIO],SMOOTH_RADIUS), label = 'ops_v4_3')\n",
    "\n",
    "plt.title(\"episode-penalties graph\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"penalties\")\n",
    "plt.legend()\n",
    "plt.show\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visit_all(dic):\n",
    "    for value in dic.values():\n",
    "        if value == 0:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "sum = 0\n",
    "times = 1000\n",
    "for i in range(times):    \n",
    "    dic = {i: 0 for i in range(100)}\n",
    "    while not visit_all(dic):\n",
    "        key = random.choice(range(len(dic)))\n",
    "        dic[key] = 1\n",
    "        sum += 1\n",
    "sum /= times\n",
    "print(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def div_add(n):\n",
    "    sum = 0\n",
    "    for i in range(1,n+1):\n",
    "        sum += 1/i\n",
    "        \n",
    "    return n * sum\n",
    "div_add(3000)/200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum = 0\n",
    "times = 1000\n",
    "for i in range(times):    \n",
    "    dic = {i: 0 for i in range(100)}\n",
    "    while dic[0] == 0:\n",
    "        key = random.choice(range(len(dic)))\n",
    "        dic[key] = 1\n",
    "        sum += 1\n",
    "sum /= times\n",
    "print(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_reuse(env, past_policy, train_episodes, fi = 1, mu = 0.95):\n",
    "        \n",
    "    # Hyper parameters\n",
    "    alpha = 0.05  #(alpha) is the learning rate (0<α≤1) \n",
    "    gamma = 0.95  # (gamma) is the discount factor (0≤γ≤1) \n",
    "    epsilon = 0.1 # the rate to act randomly in the epsilon-greedy alg for expolration in Q-learning\n",
    "    \n",
    "    # To plot learning curve\n",
    "    all_episodes_length = []\n",
    "    all_penalties = []\n",
    "    all_rewards = []\n",
    "\n",
    "    q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    " \n",
    "    past_table = copy.deepcopy(past_policy[0])\n",
    "    \n",
    "    \n",
    "    for i in range(train_episodes):\n",
    "        state = env.reset()\n",
    "\n",
    "        episode_penalties, episode_rewards, episode_length = 0, 0, 0\n",
    "        done = False\n",
    "        f = fi\n",
    "        while not done:\n",
    "            # epsilon greedy alg balancing exporation and exploitation\n",
    "            if random.uniform(0,1) < f:\n",
    "                action = arg_max(past_table[state])\n",
    "                #print(f\"f = {f} in episode {i} \\n\")\n",
    "            elif random.uniform(0,1) < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "                #print(\"random action \\n\")\n",
    "                \n",
    "            else:\n",
    "                action = arg_max(q_table[state])\n",
    "                #print(\"on the policy now \\n\")\n",
    "\n",
    "            # step to next state\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "\n",
    "            # update q-value\n",
    "            q_value = (1-alpha) * q_table[state, action] + alpha * (reward + gamma * np.max(q_table[next_state]))\n",
    "            q_table[state, action] = q_value\n",
    "\n",
    "            state = next_state\n",
    "            f = f*mu\n",
    "            #update data for learning curve\n",
    "            if reward == -10:\n",
    "                episode_penalties +=1\n",
    "\n",
    "            episode_rewards += reward\n",
    "            episode_length += 1\n",
    "\n",
    "        #record data for learning curve\n",
    "        all_episodes_length.append(episode_length)\n",
    "        all_penalties.append(episode_penalties)\n",
    "        all_rewards.append(episode_rewards)\n",
    "\n",
    "        #show training progress\n",
    "        if (i+1) % 100 == 0:\n",
    "            clear_output(wait=True)\n",
    "            print(f\"Episode {i}\")\n",
    "    print(\"Training finished \\n\")\n",
    "    \n",
    "    # return ndarray\n",
    "    all_episodes_length = np.array(all_episodes_length)\n",
    "    all_penalties = np.array(all_penalties)\n",
    "    all_rewards = np.array(all_rewards)\n",
    "    \n",
    "    return all_episodes_length, all_penalties, all_rewards, q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "sns.set(style = \"darkgrid\")\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(range(EPISODE_NUM), smooth(qlearn_rewards_v3, 100), label = \"qlearn_v3\", color='k')\n",
    "#plt.plot(range(EPISODE_NUM), smooth(qlearn_rewards_v5, 100), label = \"qlearn_v5\")\n",
    "\n",
    "#plt.plot(range(EPISODE_NUM), smooth(qlearn_rewards_v1_3,100), label = \"qlearn_v1_3\",color='c')\n",
    "#plt.plot(range(EPISODE_NUM), smooth(qlearn_rewards_v4_3,100), label = \"qlearn_v4_3\",color='g')\n",
    "\n",
    "#plt.plot(range(EPISODE_NUM), smooth(pr_rewards_v4_3, 100), label = \"pr_v4_3\")\n",
    "#plt.plot(range(EPISODE_NUM), smooth(pr_rewards_v1_3, 100), label = \"pr_v1_3\")\n",
    "\n",
    "plt.plot(range(EPISODE_NUM), smooth(ops_rewards_v4_3, 100), label = \"ops_v4_3\", color='r')\n",
    "plt.plot(range(EPISODE_NUM), smooth(ops_rewards_v1_3, 100), label = \"ops_v1_3\", color='b')\n",
    "\n",
    "plt.title(\"episode-reward graph\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"rewards\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "# training v03 from scratch\n",
    "\n",
    "qlearn_episodes_length_v03, qlearn_penalties_v03, qlearn_rewards_v03, qlearn_q_table_v03 = transform(tl_algo = q_learning,\n",
    "                                                                                     episode_num = EPISODE_NUM,  \n",
    "                                                                                     repeat_times = REPEAT_TIMES,\n",
    "                                                                                     target_task = \"Taxi-v03\",\n",
    "                                                                                     source_task = None\n",
    "                                                                                    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "# training v01 from scratch\n",
    "\n",
    "qlearn_episodes_length_v01, qlearn_penalties_v01, qlearn_rewards_v01, qlearn_q_table_v01 = transform(tl_algo = q_learning,\n",
    "                                                                                     episode_num = EPISODE_NUM,  \n",
    "                                                                                     repeat_times = REPEAT_TIMES,\n",
    "                                                                                     target_task = \"Taxi-v01\",\n",
    "                                                                                     source_task = None\n",
    "                                                                                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "# training v04 from scratch\n",
    "\n",
    "qlearn_episodes_length_v04, qlearn_penalties_v04, qlearn_rewards_v04, qlearn_q_table_v04 = transform(tl_algo = q_learning,\n",
    "                                                                                     episode_num = EPISODE_NUM,  \n",
    "                                                                                     repeat_times = REPEAT_TIMES,\n",
    "                                                                                     target_task = \"Taxi-v04\",\n",
    "                                                                                     source_task = None\n",
    "                                                                                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#ops_v04_03\n",
    "ops_episodes_length_v04_03, ops_penalties_v04_03,ops_rewards_v04_03, ops_q_table_v04_03 = transform(tl_algo= OPS_TL,\n",
    "                                                                                       episode_num = EPISODE_NUM,\n",
    "                                                                                       repeat_times = REPEAT_TIMES,\n",
    "                                                                                       target_task = \"Taxi-v03\",\n",
    "                                                                                       source_task = \"Taxi-v04\",\n",
    "                                                                                       policy_library = [[q_func] for q_func in qlearn_q_table_v04])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "#ops_v01_03\n",
    "ops_episodes_length_v01_03, ops_penalties_v01_03,ops_rewards_v01_03, ops_q_table_v01_03 = transform(tl_algo= OPS_TL,\n",
    "                                                                                       episode_num = EPISODE_NUM,\n",
    "                                                                                       repeat_times = REPEAT_TIMES,\n",
    "                                                                                       target_task = \"Taxi-v03\",\n",
    "                                                                                       source_task = \"Taxi-v01\",\n",
    "                                                                                       policy_library = [[q_func] for q_func in qlearn_q_table_v01])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(500), smooth(qlearn_rewards_v5[:500], 20), label = \"qlearn_v5\")\n",
    "#plt.plot(range(EPISODE_NUM), smooth(qlearn_rewards_v5, 100), label = \"qlearn_v5\")\n",
    "\n",
    "#plt.plot(range(EPISODE_NUM), smooth(qlearn_rewards_v1_3,100), label = \"qlearn_v1_3\")\n",
    "#plt.plot(range(EPISODE_NUM), smooth(qlearn_rewards_v4_3,100), label = \"qlearn_v4_3\")\n",
    "\n",
    "#plt.plot(range(EPISODE_NUM), smooth(pr_rewards_v4_3, 100), label = \"pr_v4_3\")\n",
    "#plt.plot(range(EPISODE_NUM), smooth(pr_rewards_v1_3, 100), label = \"pr_v1_3\")\n",
    "\n",
    "plt.plot(range(500), smooth(ops_rewards_v4_5[:500], 20), label = \"ops_v4_5\")\n",
    "plt.plot(range(500), smooth(ops_rewards_v1_5[:500], 20), label = \"ops_v1_5\")\n",
    "\n",
    "plt.title(\"episode-reward graph\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"rewards\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in qlearn_rewards_v3:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Taxi-v3\")\n",
    "r,_, _ = policy_evaluate(env, np.zeros([env.observation_space.n,env.action_space.n]), 1)\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(range(EPISODE_NUM//10 ),smooth(pr_rewards_v4_3[-EPISODE_NUM//10: ],100), label = 'pr_v4_3')\n",
    "plt.plot(range(EPISODE_NUM//10 ),smooth(pr_rewards_v4_3[-EPISODE_NUM//10: ],100), label = 'prql_v4_3')\n",
    "plt.plot(range(EPISODE_NUM//10), smooth(qlearn_rewards_v3[-EPISODE_NUM//10:], 100), label = \"qlearn_v3\")\n",
    "plt.plot(range(EPISODE_NUM//10), smooth(qlearn_rewards_v4[-EPISODE_NUM//10:], 100), label = \"qlearn_v4\")\n",
    "plt.plot(range(EPISODE_NUM//10), smooth(qlearn_rewards_v1_3[-EPISODE_NUM//10:],100), label = \"qlearn_v1_3\")\n",
    "\n",
    "plt.title(\"smoothed episode-reward graph: last {} episodes\".format(EPISODE_NUM//10))\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"reward\")\n",
    "plt.legend()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ops_episodes_length_v4_3, ops_penalties_v4_3,ops_rewards_v4_3, ops_q_table_v4_3 = transform(tl_algo= OPS_TL,\n",
    "                                                                                       episode_num = EPISODE_NUM,\n",
    "                                                                                       repeat_times = 10,\n",
    "                                                                                       target_task = \"Taxi-v3\",\n",
    "                                                                                       source_task = \"Taxi-v4\",\n",
    "                                                                                       policy_library = [[q_func] for q_func in qlearn_q_table_v4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "train_episodes = EPISODE_NUM\n",
    "\n",
    "plt.plot(range(train_episodes),smooth(qlearn_rewards_v3,100), 'k', label = 'qlearn_v3')\n",
    "plt.plot(range(train_episodes),smooth(qlearn_rewards_v1_3,100), label = 'qlearn_v1_3')\n",
    "plt.plot(range(train_episodes),smooth(qlearn_rewards_v4_3,100), label = 'qlearn_v4_3')\n",
    "\n",
    "\n",
    "plt.plot(range(train_episodes),smooth(pr_rewards_v1_3,100), label = 'pr_v1_3')\n",
    "plt.plot(range(train_episodes),smooth(pr_rewards_v4_3,100), label = 'pr_v4_3')\n",
    "\n",
    "\n",
    "plt.plot(range(train_episodes),smooth(ops_rewards_v1_3,100), label = 'ops_v1_3')\n",
    "plt.plot(range(train_episodes),smooth(ops_rewards_v4_3,100), label = 'ops_v4_3')\n",
    "\n",
    "plt.title(\"episode-reward graph\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"reward\")\n",
    "plt.legend()\n",
    "plt.show\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(train_episodes//10 ),smooth(qlearn_rewards_v3[-train_episodes//10: ],100), 'k',label = 'qlearn_v3')\n",
    "plt.plot(range(train_episodes//10 ),smooth(qlearn_rewards_v1_3[-train_episodes//10: ],100), label = 'qlearn_v1_3')\n",
    "plt.plot(range(train_episodes//10 ),smooth(qlearn_rewards_v4_3[-train_episodes//10: ],100), label = 'qlearn_v1_3')\n",
    "\n",
    "#plt.plot(range(train_episodes//10 ),smooth(sarsa_rewards_v3[-train_episodes//10: ],100), label = 'sarsa_v3')\n",
    "#plt.plot(range(train_episodes//10 ),smooth(sarsa_rewards_v1_3[-train_episodes//10: ],100), label = 'sarsa_v1_3')\n",
    "#plt.plot(range(train_episodes//10 ),smooth(sarsa_rewards_v4_3[-train_episodes//10: ],100), label = 'sarsa_v1_3')\n",
    "\n",
    "plt.plot(range(train_episodes//10 ),smooth(pr_rewards_v1_3[-train_episodes//10: ],100), label = 'pr_v1_3')\n",
    "plt.plot(range(train_episodes//10 ),smooth(pr_rewards_v4_3[-train_episodes//10: ],100), label = 'pr_v4_3')\n",
    "\n",
    "plt.plot(range(train_episodes//10 ),smooth(pr1_rewards_v1_3[-train_episodes//10: ],100), label = 'pr1_v1_3')\n",
    "plt.plot(range(train_episodes//10 ),smooth(pr1_rewards_v4_3[-train_episodes//10: ],100), label = 'pr1_v4_3')\n",
    "\n",
    "\n",
    "plt.title(\"smoothed episode-reward graph: last 1000 episodes\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"reward\")\n",
    "plt.legend()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(range(train_episodes//100),qlearn_rewards_v3[:train_episodes//100], label = 'qlearn_v3')\n",
    "plt.plot(range(train_episodes//100),qlearn_rewards_v1_3[:train_episodes//100], label = 'qlearn_v1_3')\n",
    "plt.plot(range(train_episodes//100),qlearn_rewards_v4_3[:train_episodes//100], label = 'qlearn_v4_3')\n",
    "\n",
    "plt.plot(range(train_episodes//100),sarsa_rewards_v3[:train_episodes//100], label = 'sarsa_v3')\n",
    "plt.plot(range(train_episodes//100),sarsa_rewards_v1_3[:train_episodes//100], label = 'sarsa_v1_3')\n",
    "plt.plot(range(train_episodes//100),sarsa_rewards_v4_3[:train_episodes//100], label = 'sarsa_v4_3')\n",
    "\n",
    "plt.title(\"episode-reward graph: first 1000 episodes\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"reward\")\n",
    "plt.legend()\n",
    "plt.show\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(train_episodes//100),smooth(qlearn_rewards_v3[:train_episodes//100],10), label = 'qlearn_v3')\n",
    "plt.plot(range(train_episodes//100),smooth(qlearn_rewards_v1_3[:train_episodes//100],10), label = 'qlearn_v1_3')\n",
    "plt.plot(range(train_episodes//100),smooth(qlearn_rewards_v4_3[:train_episodes//100],10), label = 'qlearn_v4_3')\n",
    "\n",
    "plt.plot(range(train_episodes//100),smooth(sarsa_rewards_v3[:train_episodes//100],10), label = 'sarsa_v3')\n",
    "plt.plot(range(train_episodes//100),smooth(sarsa_rewards_v1_3[:train_episodes//100],10), label = 'sarsa_v1_3')\n",
    "plt.plot(range(train_episodes//100),smooth(sarsa_rewards_v4_3[:train_episodes//100],10), label = 'sarsa_v4_3')\n",
    "\n",
    "plt.title(\"smoothed episode-reward graph: first 1000 episodes\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"reward\")\n",
    "plt.legend()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(train_episodes//100),smooth(qlearn_rewards_v3[:train_episodes//100],10), label = 'qlearn_v3')\n",
    "plt.plot(range(train_episodes//100),smooth(qlearn_rewards_v1_3[:train_episodes//100],10), label = 'qlearn_v1_3')\n",
    "plt.plot(range(train_episodes//100),smooth(qlearn_rewards_v4_3[:train_episodes//100],10), label = 'qlearn_v4_3')\n",
    "\n",
    "\n",
    "plt.title(\"q-learning smoothed episode-reward graph: first 1000 episodes\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"reward\")\n",
    "plt.legend()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(train_episodes//100),qlearn_episodes_length_v3[:train_episodes//100], label = 'qlearn_v3')\n",
    "plt.plot(range(train_episodes//100),qlearn_episodes_length_v1_3[:train_episodes//100], label = 'qlearn_v1_3')\n",
    "plt.plot(range(train_episodes//100),qlearn_episodes_length_v4_3[:train_episodes//100], label = 'qlearn_v4_3')\n",
    "\n",
    "plt.plot(range(train_episodes//100),sarsa_episodes_length_v3[:train_episodes//100], label = 'sarsa_v3')\n",
    "plt.plot(range(train_episodes//100),sarsa_episodes_length_v1_3[:train_episodes//100], label = 'sarsa_v1_3')\n",
    "plt.plot(range(train_episodes//100),sarsa_episodes_length_v4_3[:train_episodes//100], label = 'sarsa_v4_3')\n",
    "\n",
    "plt.title(\"episode-length graph: first 1000 episodes\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"length\")\n",
    "plt.legend()\n",
    "plt.show\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(train_episodes//100),smooth(qlearn_episodes_length_v3[:train_episodes//100],10), label = 'qlearn_v3')\n",
    "plt.plot(range(train_episodes//100),smooth(qlearn_episodes_length_v1_3[:train_episodes//100],10), label = 'qlearn_v1_3')\n",
    "plt.plot(range(train_episodes//100),smooth(qlearn_episodes_length_v4_3[:train_episodes//100],10), label = 'qlearn_v4_3')\n",
    "\n",
    "plt.plot(range(train_episodes//100),smooth(sarsa_episodes_length_v3[:train_episodes//100],10), label = 'sarsa_v3')\n",
    "plt.plot(range(train_episodes//100),smooth(sarsa_episodes_length_v1_3[:train_episodes//100],10), label = 'sarsa_v1_3')\n",
    "plt.plot(range(train_episodes//100),smooth(sarsa_episodes_length_v4_3[:train_episodes//100],10), label = 'sarsa_v4_3')\n",
    "\n",
    "plt.title(\"smoothed episode-length graph: first 1000 episodes\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"length\")\n",
    "plt.legend()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(train_episodes//100),smooth(qlearn_episodes_length_v3[:train_episodes//100],10), label = 'qlearn_v3')\n",
    "plt.plot(range(train_episodes//100),smooth(qlearn_episodes_length_v1_3[:train_episodes//100],10), label = 'qlearn_v1_3')\n",
    "plt.plot(range(train_episodes//100),smooth(qlearn_episodes_length_v4_3[:train_episodes//100],10), label = 'qlearn_v4_3')\n",
    "\n",
    "plt.title(\"q-learning smoothed episode-length graph: first 1000 episodes\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"length\")\n",
    "plt.legend()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(train_episodes//100),smooth(sarsa_episodes_length_v3[:train_episodes//100],10), label = 'sarsa_v3')\n",
    "plt.plot(range(train_episodes//100),smooth(sarsa_episodes_length_v1_3[:train_episodes//100],10), label = 'sarsa_v1_3')\n",
    "plt.plot(range(train_episodes//100),smooth(sarsa_episodes_length_v4_3[:train_episodes//100],10), label = 'sarsa_v4_3')\n",
    "\n",
    "plt.title(\"sarsa smoothed episode-length graph: first 1000 episodes\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"length\")\n",
    "plt.legend()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(train_episodes),qlearn_penalties_v3, label = 'qlearn_v3')\n",
    "plt.plot(range(train_episodes),qlearn_penalties_v1_3, label = 'qlearn_v1_3')\n",
    "plt.plot(range(train_episodes),qlearn_penalties_v4_3, label = 'qlearn_v4_3')\n",
    "\n",
    "plt.plot(range(train_episodes),sarsa_penalties_v3, label = 'sarsa_v3')\n",
    "plt.plot(range(train_episodes),sarsa_penalties_v1_3, label = 'sarsa_v1_3')\n",
    "plt.plot(range(train_episodes),sarsa_penalties_v4_3, label = 'sarsa_v4_3')\n",
    "\n",
    "plt.title(\"episode-penalties graph\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"penalties\")\n",
    "plt.legend()\n",
    "plt.show\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(train_episodes//100),qlearn_penalties_v3[:train_episodes//100], label = 'qlearn_v3')\n",
    "plt.plot(range(train_episodes//100),qlearn_penalties_v1_3[:train_episodes//100], label = 'qlearn_v1_3')\n",
    "plt.plot(range(train_episodes//100),qlearn_penalties_v4_3[:train_episodes//100], label = 'qlearn_v4_3')\n",
    "\n",
    "plt.plot(range(train_episodes//100),sarsa_penalties_v3[:train_episodes//100], label = 'sarsa_v3')\n",
    "plt.plot(range(train_episodes//100),sarsa_penalties_v1_3[:train_episodes//100], label = 'sarsa_v1_3')\n",
    "plt.plot(range(train_episodes//100),sarsa_penalties_v4_3[:train_episodes//100], label = 'sarsa_v4_3')\n",
    "\n",
    "plt.title(\"episode-penalties graph: first 1000 episodes\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"penalties\")\n",
    "plt.legend()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(train_episodes//100),smooth(qlearn_penalties_v3[:train_episodes//100],10), label = 'qlearn_v3')\n",
    "plt.plot(range(train_episodes//100),smooth(qlearn_penalties_v1_3[:train_episodes//100],10), label = 'qlearn_v1_3')\n",
    "plt.plot(range(train_episodes//100),smooth(qlearn_penalties_v4_3[:train_episodes//100],10), label = 'qlearn_v4_3')\n",
    "\n",
    "plt.plot(range(train_episodes//100),smooth(sarsa_penalties_v3[:train_episodes//100],10), label = 'sarsa_v3')\n",
    "plt.plot(range(train_episodes//100),smooth(sarsa_penalties_v1_3[:train_episodes//100],10), label = 'sarsa_v1_3')\n",
    "plt.plot(range(train_episodes//100),smooth(sarsa_penalties_v4_3[:train_episodes//100],10), label = 'sarsa_v4_3')\n",
    "\n",
    "plt.title(\"smoothed episode-penalties graph: first 1000 episodes\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"penalties \")\n",
    "plt.legend()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(train_episodes//100),smooth(qlearn_penalties_v3[:train_episodes//100],10), label = 'qlearn_v3')\n",
    "plt.plot(range(train_episodes//100),smooth(qlearn_penalties_v1_3[:train_episodes//100],10), label = 'qlearn_v1_3')\n",
    "plt.plot(range(train_episodes//100),smooth(qlearn_penalties_v4_3[:train_episodes//100],10), label = 'qlearn_v4_3')\n",
    "\n",
    "\n",
    "plt.title(\"q-learning smoothed episode-penalties graph: first 1000 episodes\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"penalties \")\n",
    "plt.legend()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(train_episodes//100),smooth(sarsa_penalties_v3[:train_episodes//100],10), label = 'sarsa_v3')\n",
    "plt.plot(range(train_episodes//100),smooth(sarsa_penalties_v1_3[:train_episodes//100],10), label = 'sarsa_v1_3')\n",
    "plt.plot(range(train_episodes//100),smooth(sarsa_penalties_v4_3[:train_episodes//100],10), label = 'sarsa_v4_3')\n",
    "\n",
    "plt.title(\"sarsa smoothed episode-penalties graph: first 1000 episodes\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"penalties \")\n",
    "plt.legend()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Exp1: similarity diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 799800\n",
      "Training finished\n",
      "CPU times: user 8min, sys: 1min 42s, total: 9min 42s\n",
      "Wall time: 8min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#qlearn_v004_003\n",
    "qlearn_episodes_length_v004_003, qlearn_penalties_v004_003,qlearn_rewards_v004_003, _, qlearn_q_table_v004_003 = transform(tl_algo= q_learning,\n",
    "                                                                                       episode_num = EPISODE_NUM,\n",
    "                                                                                       repeat_times = REPEAT_TIMES,\n",
    "                                                                                       target_task = \"Taxi-v003\",\n",
    "                                                                                       \n",
    "                                                                                       policy_library =  qlearn_q_table_v004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 799800\n",
      "Training finished\n",
      "CPU times: user 21min 52s, sys: 5min 52s, total: 27min 45s\n",
      "Wall time: 24min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#qlearn_v001_003\n",
    "qlearn_episodes_length_v001_003, qlearn_penalties_v001_003,qlearn_rewards_v001_003, _, qlearn_q_table_v001_003 = transform(tl_algo= q_learning,\n",
    "                                                                                       episode_num = EPISODE_NUM,\n",
    "                                                                                       repeat_times = REPEAT_TIMES,\n",
    "                                                                                       target_task = \"Taxi-v003\",\n",
    "                                                                                       \n",
    "                                                                                       policy_library =  qlearn_q_table_v001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 799800\n",
      "Training finished \n",
      "\n",
      "CPU times: user 23min 8s, sys: 8min 17s, total: 31min 26s\n",
      "Wall time: 27min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#sarsa_v001_003\n",
    "sarsa_episodes_length_v001_003, sarsa_penalties_v001_003,sarsa_rewards_v001_003, _, sarsa_q_table_v001_003 = transform(tl_algo= sarsa,\n",
    "                                                                                       episode_num = EPISODE_NUM,\n",
    "                                                                                       repeat_times = REPEAT_TIMES,\n",
    "                                                                                       target_task = \"Taxi-v003\",\n",
    "                                                                                       \n",
    "                                                                                       policy_library =  sarsa_q_table_v001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 799800\n",
      "Training finished \n",
      "\n",
      "CPU times: user 7min 31s, sys: 1min 47s, total: 9min 18s\n",
      "Wall time: 8min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#sarsa_v004_003\n",
    "sarsa_episodes_length_v004_003, sarsa_penalties_v004_003,sarsa_rewards_v004_003, _, sarsa_q_table_v004_003 = transform(tl_algo= sarsa,\n",
    "                                                                                       episode_num = EPISODE_NUM,\n",
    "                                                                                       repeat_times = REPEAT_TIMES,\n",
    "                                                                                       target_task = \"Taxi-v003\",\n",
    "                                                                                       \n",
    "                                                                                       policy_library =  sarsa_q_table_v004)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-770.6, -792.8, -707.6, ...,    8.4,    8.6,    8.2],\n",
       "       [-794.2, -730.2, -764.3, ...,    8.3,    7.8,    7.9],\n",
       "       [-794. , -783.2, -736.7, ...,    8.3,    7.6,    8.4],\n",
       "       ...,\n",
       "       [-733.7, -891.2, -970.4, ...,    6.2,    9.7,    7.8],\n",
       "       [-803.9, -693.2, -729.9, ...,    7.6,    7. ,    7.4],\n",
       "       [-756.3, -965.9, -763.4, ...,    8.5,    8.6,    8.1]])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qlearn_rewards_v4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 799800\n",
      "Training finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 3/3 [8:09:35<00:00, 9791.88s/it]   \u001b[A\n",
      "100%|██████████| 2/2 [16:19:21<00:00, 29380.76s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15h 47min 19s, sys: 45min 3s, total: 16h 32min 23s\n",
      "Wall time: 16h 19min 21s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ENV = 'Taxi'\n",
    "rl_algs = ['qlearn','sarsa']\n",
    "#alias\n",
    "qtable = qlearn\n",
    "stable =sarsa\n",
    "tl_algs = [ 'qtable', 'stable', 'prql','ops','caps']\n",
    "levels =['v1', 'v3','v4']\n",
    "for rl_alg in tqdm(rl_algs):    \n",
    "    for level in tqdm(levels):\n",
    "        # source task training from scratch\n",
    "        _,_,locals()[rl_alg + '_rewards_'+level ],_,locals()[rl_alg + '_q_table_'+level ] = transform(tl_algo= eval(rl_alg),\n",
    "                                                                            episode_num = EPISODE_NUM,\n",
    "                                                                            repeat_times = REPEAT_TIMES,\n",
    "                                                                            target_task = ENV + '-' + levels[1],\n",
    "                                                                                  )\n",
    "         # transfer training\n",
    "        if level != levels[1]:\n",
    "            for tl_alg in tl_algs:\n",
    "                if tl_alg not in ['ops','caps']:\n",
    "                    _,_,locals()[tl_alg + '_' + rl_alg +'_rewards_'+level +'_'+ levels[1]],_,_ = transform(tl_algo= eval(tl_alg),\n",
    "                                                                                episode_num = EPISODE_NUM,\n",
    "                                                                                repeat_times = REPEAT_TIMES,\n",
    "                                                                                target_task = ENV + '-' + levels[1],\n",
    "                                                                                policy_library =  eval(rl_alg + '_q_table_' + level))\n",
    "                else:\n",
    "                    _,_,locals()[tl_alg + '_' + rl_alg + '_rewards_'+level +'_'+ levels[1]],_,_ = transform(tl_algo= eval(tl_alg),\n",
    "                                                                                episode_num = EPISODE_NUM,\n",
    "                                                                                repeat_times = REPEAT_TIMES,\n",
    "                                                                                target_task = ENV + '-' + levels[1],\n",
    "                                                                                policy_library =  [[qfunc] for qfunc in eval(rl_alg + '_q_table_' + level)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Close_transfer_caps_qlearn_F1': array([[ -785.9,  -804.8,  -821. , ...,     7.5,     8.9,     9.5],\n",
       "        [ -784.1,  -776. ,  -813.8, ...,     8.8,     7.7,     7.9],\n",
       "        [ -790.4,  -766.1,  -865.1, ...,     8.4,     7.4,     7. ],\n",
       "        ...,\n",
       "        [ -794.9,  -941.6,  -751.3, ...,     8.4,     8.3,     8.4],\n",
       "        [ -744.9,  -810.6,  -804.8, ...,     7.3,     6.6,     7.6],\n",
       "        [ -785. , -1026.2,  -872.3, ...,     7.3,     8.4,     7.6]]),\n",
       " 'Close_transfer_caps_sarsa_F1': array([[-794. , -822.8, -814.7, ...,    7.4,    7.6,    8.1],\n",
       "        [-762.5, -774.2, -821.9, ...,    7.8,    7.5,    8.7],\n",
       "        [-773.3, -896.6, -810.2, ...,    7.2,    8. ,    7.3],\n",
       "        ...,\n",
       "        [-782.3, -827.3, -742.7, ...,    8.3,    7.4,    7.9],\n",
       "        [-796.7, -795.8, -807.5, ...,   10. ,    7.5,    8.4],\n",
       "        [-768.8, -863.3, -815.6, ...,    7.9,    8.5,    6.7]]),\n",
       " 'Close_transfer_ops_qlearn_F1': array([[-766.3, -737.9, -787.7, ...,    7.4,    5.7,    8.7],\n",
       "        [-790.4, -783.2, -839. , ...,    7. ,    6.5,    8.1],\n",
       "        [-783.7, -778.7, -827.3, ...,    8.7,    7.8,    8. ],\n",
       "        ...,\n",
       "        [-788.6, -811.1, -785. , ...,    7.4,    8.2,    9.2],\n",
       "        [-806.6, -809.3, -821.9, ...,    7.3,    6.1,    7.3],\n",
       "        [-760.5, -756.2, -803.9, ...,    8.3,   10.1,    7.3]]),\n",
       " 'Close_transfer_ops_sarsa_F1': array([[-758.7, -788.6, -793.1, ...,    8.8,    6.5,    8. ],\n",
       "        [-696.2, -776.9, -770.2, ...,    7.3,    8.6,    7.8],\n",
       "        [-787.9, -776. , -819.2, ...,    8.4,    7.8,    8.3],\n",
       "        ...,\n",
       "        [-736.7, -780.5, -800.3, ...,    7.6,    8.2,    7.9],\n",
       "        [-785. , -792.2, -800.3, ...,    7.3,    8. ,    9.4],\n",
       "        [-825.5, -790.4, -789.5, ...,    9.3,    7.5,    6.8]]),\n",
       " 'Close_transfer_prql_qlearn_F1': array([[-761.6, -822.8, -816.5, ...,  -32.6,    7.7,  -32.9],\n",
       "        [-823.7, -801.2, -875. , ...,    8.1,  -12. ,  -54. ],\n",
       "        [-768.1, -802.6, -738.2, ...,  -13.1,  -54.6,  -33.3],\n",
       "        ...,\n",
       "        [-685.4, -805.7, -912.8, ...,    7.5,  -52.6,  -11.3],\n",
       "        [-801.2, -796.7, -790.4, ...,  -12.9,  -34.3,  -11.5],\n",
       "        [-799.4, -785.9, -917.3, ...,  -13.1, -211.9,  -11.6]]),\n",
       " 'Close_transfer_prql_sarsa_F1': array([[-775.1, -808.4, -809.3, ...,  -54.2,  -13.1, -235.1],\n",
       "        [-827.3, -735.5, -790.4, ...,    8.3,  -12.2,  -12.2],\n",
       "        [-795.8, -807.5, -785. , ...,  -12.6,  -11.7,  -12.7],\n",
       "        ...,\n",
       "        [-754.4, -739.8, -936.2, ...,  -12.2,  -55.5,  -33.1],\n",
       "        [-801.2, -815.6, -900.2, ...,    7.1,    8.8,    8.3],\n",
       "        [-708.6, -790.4, -816.5, ...,  -33.5,    8.2,    7.9]]),\n",
       " 'Close_transfer_qtable_qlearn_F1': array([[9.8, 7.9, 8.1, ..., 5.1, 8.5, 7.5],\n",
       "        [7.7, 9. , 8.1, ..., 8.1, 8.6, 8.3],\n",
       "        [6.8, 7.6, 7.6, ..., 7.1, 7.5, 7.8],\n",
       "        ...,\n",
       "        [8.8, 6.8, 7.7, ..., 7.4, 6. , 6.9],\n",
       "        [8.2, 9.1, 8.5, ..., 7.2, 7.8, 8.5],\n",
       "        [7.6, 8.9, 7.5, ..., 7.9, 7.2, 8.8]]),\n",
       " 'Close_transfer_qtable_sarsa_F1': array([[ 6.5,  8.5,  8.9, ...,  7.3,  8.8,  7.6],\n",
       "        [ 7.8,  7.8,  7.8, ...,  7.1,  6.6,  7.1],\n",
       "        [ 7.2,  8.5,  7.2, ...,  8.5,  8.5,  9.6],\n",
       "        ...,\n",
       "        [ 8.1,  8.4,  7.3, ...,  8.3,  7. ,  8.6],\n",
       "        [ 7.6,  8.8,  7.2, ...,  8. ,  7.7,  7.5],\n",
       "        [ 8.2,  6.8, 10. , ...,  8.5,  6.8,  8.1]]),\n",
       " 'Close_transfer_stable_qlearn_F1': array([[  8.5,   7.5,   6.8, ...,   8.4,   8.3, -12.7],\n",
       "        [  8.9,   7.9,   9.7, ...,   9.1,   8.4,   8.2],\n",
       "        [  8.5,   8.2,   9.4, ...,   7.6,   8.4,  10.1],\n",
       "        ...,\n",
       "        [  8.1,   8.6,   7.4, ...,   6.9,   7.7,   7.6],\n",
       "        [  8.1,   7.9,   6.6, ...,   7.6, -12.5, -13.8],\n",
       "        [  7.3,   8.2,   7.1, ...,   6.8,   8.4,   7.8]]),\n",
       " 'Close_transfer_stable_sarsa_F1': array([[7.4, 8.1, 8. , ..., 8.6, 6.5, 6.2],\n",
       "        [6.7, 8.6, 8.5, ..., 9.6, 9.5, 7.7],\n",
       "        [6.3, 8. , 7.6, ..., 8.7, 7.6, 7.8],\n",
       "        ...,\n",
       "        [7.7, 7.3, 8.1, ..., 7.9, 7.2, 8.6],\n",
       "        [7.1, 6.3, 7.4, ..., 8. , 8. , 9. ],\n",
       "        [7.8, 7.2, 8.4, ..., 8. , 8.9, 9.1]]),\n",
       " 'Far_transfer_caps_qlearn_F1': array([[-801.2, -836.3, -814.7, ...,    9.4,    8.8,    8.6],\n",
       "        [-768.8, -778.7, -820.1, ...,    6.2,    8. ,    8.2],\n",
       "        [-762.5, -816.5, -812.9, ...,    8.7,    8.2,    6.9],\n",
       "        ...,\n",
       "        [-875. , -825.5, -837.2, ...,    6.8,    6.8,    8.2],\n",
       "        [-802.1, -812. , -807.5, ...,    8.6,    7.3,    6.8],\n",
       "        [-727.3, -840.8, -810.2, ...,    7.4,    9.1,    8.2]]),\n",
       " 'Far_transfer_caps_sarsa_F1': array([[ -749.8,  -914.6,  -895.7, ...,     6.9,     7.5,     8.6],\n",
       "        [ -824.6, -1027.1,  -869.6, ...,     6.9,     7.8,     7.6],\n",
       "        [ -768.8,  -807.5,  -814.7, ...,     7.1,     7.5,     7.7],\n",
       "        ...,\n",
       "        [ -691.4,  -786.8,  -807.5, ...,     7.5,     7.5,     6.2],\n",
       "        [ -908.3,  -823.7,  -785.9, ...,     8.5,     8.4,     7.7],\n",
       "        [ -779.6,  -854.3,  -794.9, ...,     7.9,     6.2,     8.3]]),\n",
       " 'Far_transfer_ops_qlearn_F1': array([[-800.3, -776.9, -787.7, ...,    7.8,    7.4,    9. ],\n",
       "        [-788.6, -754.4, -838.1, ...,    7.8,    6.9,    8.6],\n",
       "        [-770.4, -797.6, -795.8, ...,    7.6,    9.1,    7.7],\n",
       "        ...,\n",
       "        [-793.1, -792.2, -794. , ...,    6.9,    6.7,    7. ],\n",
       "        [-705.7, -917.3, -904.7, ...,    7.3,    7.4,    8.2],\n",
       "        [-822.8, -785.9, -788.6, ...,    7.7,    8.7,    9. ]]),\n",
       " 'Far_transfer_ops_sarsa_F1': array([[-794. , -786.8, -862.4, ...,    8.3,    8.1,    7. ],\n",
       "        [-756.2, -758. , -829.1, ...,    9.4,    7.7,    7.7],\n",
       "        [-795.8, -856.1, -805.7, ...,    6.5,    7.7,    8.6],\n",
       "        ...,\n",
       "        [-795.8, -791.3, -833.6, ...,    7.6,    7.6,    8.1],\n",
       "        [-784.1, -807.5, -792.2, ...,    8.7,    6.7,    7.8],\n",
       "        [-830.9, -794. , -819.2, ...,    7.4,    7.5,    7.6]]),\n",
       " 'Far_transfer_prql_qlearn_F1': array([[-772.4, -818.3, -802.1, ...,    7.9,  -33.4,  -11.6],\n",
       "        [-827.3, -819.2, -778.7, ...,  -33.4,  -12.7,    9.3],\n",
       "        [-786.8, -794.9, -793.1, ...,    8. ,    8.8,  -33.6],\n",
       "        ...,\n",
       "        [-727.8, -926.3, -779.6, ...,  -32.8,  -11.9,  -12.4],\n",
       "        [-745. , -792.2, -806.6, ...,  -12.1,  -54.2,  -54.1],\n",
       "        [-701.1, -760.4, -797.6, ...,  -33.7,  -53.6,  -12.7]]),\n",
       " 'Far_transfer_prql_sarsa_F1': array([[-776.9, -812.9, -806.6, ...,  -53.4,  -12.3,  -33.6],\n",
       "        [-812.9, -819.2, -794. , ...,  -55.1,  -32.5,  -32.2],\n",
       "        [-766.3, -785. , -814.7, ...,  -54.2,  -12.4, -212.7],\n",
       "        ...,\n",
       "        [-744.5, -800.8, -798.5, ...,  -11.1,  -55.1,    8.4],\n",
       "        [-805.7, -795.8, -806.6, ...,    9.1,  -32.8,  -13.1],\n",
       "        [-784.1, -785. , -792.2, ...,  -12.6,  -75. ,  -33.5]]),\n",
       " 'Far_transfer_qtable_qlearn_F1': array([[ 9.7,  8.8,  8. , ...,  8.5,  7.8,  8.8],\n",
       "        [ 8.2,  7.9, 10.1, ...,  8. ,  6.6,  7.7],\n",
       "        [ 7.1,  7.5,  8.3, ...,  7. ,  6.8,  8.7],\n",
       "        ...,\n",
       "        [ 8.4,  7.5,  7.8, ...,  9.2,  7.7,  8.2],\n",
       "        [ 6.6,  7. ,  8.5, ...,  9. ,  9.1,  8.2],\n",
       "        [ 7.9,  7.2,  6.6, ...,  8.9,  7.4,  8.4]]),\n",
       " 'Far_transfer_qtable_sarsa_F1': array([[7.2, 9.1, 6.2, ..., 7.3, 7.1, 9.7],\n",
       "        [7.8, 8.8, 6.9, ..., 9. , 6.8, 8.4],\n",
       "        [7.7, 6.5, 8.4, ..., 7.8, 8.3, 8.1],\n",
       "        ...,\n",
       "        [8. , 7.9, 8.1, ..., 7.4, 8.4, 7.6],\n",
       "        [6.8, 8.5, 7.7, ..., 7.5, 7.8, 7.5],\n",
       "        [7.8, 5.4, 8.7, ..., 9.4, 7.3, 8.5]]),\n",
       " 'Far_transfer_stable_qlearn_F1': array([[  8.2,   8.1,   7.2, ...,   7.2,   6.6,   8.8],\n",
       "        [  6.5,   8.8,   7.1, ...,  10.1, -11.9,   8.2],\n",
       "        [  8.3,   8.8,   7.1, ...,   8.1,   8.8,   6.6],\n",
       "        ...,\n",
       "        [  8.2,   9. ,   8.1, ...,   6.7,   7.7, -13.2],\n",
       "        [  7.7,   8. ,   9. , ...,   6.8,   8.4,   9.1],\n",
       "        [  8.4,   7.6,   7.5, ...,   7.8,   8. ,   8.7]]),\n",
       " 'Far_transfer_stable_sarsa_F1': array([[  7.1,   7.6,   8.4, ...,   7.2,   7.4,   7.8],\n",
       "        [  8.9,   8.9,   7.3, ...,   8.4,   8.5,   7.6],\n",
       "        [  7.9,   8. ,   8.1, ...,   8.2,   7.4,   7.9],\n",
       "        ...,\n",
       "        [  8.7,   9.1,   9.1, ...,   6.7,   8. ,   7.6],\n",
       "        [  7.7,   8.6,   8.2, ...,   8.4,   6. ,   8.3],\n",
       "        [-13.4,   7.6,   8.7, ...,   9.1,   5.7,   9.5]]),\n",
       " 'No_transfer_null_qlearn_F1': array([[-707.4, -823.7, -778.7, ...,    7.2,    7. ,    8.5],\n",
       "        [-771.5, -830.9, -690.7, ...,    7.7,    8.5,    7.7],\n",
       "        [-799.4, -702.9, -743.6, ...,    7.8,    8.1,    7.9],\n",
       "        ...,\n",
       "        [-691.9, -782.3, -740.9, ...,    7.3,    8.9,    7.2],\n",
       "        [-810.2, -714.8, -767.9, ...,    8.6,    8.1,    6.5],\n",
       "        [-800.3, -720.2, -766.6, ...,    7.5,    6.8,    8.1]]),\n",
       " 'No_transfer_null_sarsa_F1': array([[-770.6, -922.7, -723.7, ...,    8.8,    8.8,    7. ],\n",
       "        [-790.2, -769.7, -669.6, ...,    9.2,    9.6,    7.1],\n",
       "        [-771.4, -786.8, -664.4, ...,    7.9,    7.5,    6.6],\n",
       "        ...,\n",
       "        [-723.8, -665.8, -973.1, ...,    7.5,    8.2,    6.7],\n",
       "        [-803.9, -753.9, -790.4, ...,    6.7,    8.8,    7.1],\n",
       "        [-785. , -598.7, -823.7, ...,    8.8,    7.2,    8.5]])}"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl_algs = ['qtable', 'stable', 'prql','ops','caps']\n",
    "rl_algs = ['qlearn','sarsa']\n",
    "data = [None] * (len(tl_algs) * len(rl_algs))\n",
    "all_data = {}\n",
    "i = 0\n",
    "for rl_alg in rl_algs:\n",
    "    for tl_alg in (tl_algs):\n",
    "        data[i] = cache(tl_alg =tl_alg, rl_alg = rl_alg)\n",
    "        all_data.update(data[i])\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def cache(ex_name = \"Taxi\", tl_alg = \"qlearn\", rl_alg = '', all_data = None, reward_mode = 'F1', target_level = 'v3', save = True):\n",
    "    \n",
    "    if os.path.exists(\"./\"+ time.strftime(\"%Y%m%d\", time.localtime())) is not True:\n",
    "        os.mkdir(\"./\"+ time.strftime(\"%Y%m%d\", time.localtime()) )\n",
    "    if all_data is None:\n",
    "        data = {\"No_transfer_null_\" + rl_alg + '_' + reward_mode: eval(rl_alg +\"_rewards_\" + target_level),\n",
    "                \"Far_transfer_\"+tl_alg + '_' + rl_alg + '_' + reward_mode: eval(tl_alg+'_'+ rl_alg +\"_rewards_\"+ target_level[0] + '1' + '_' + target_level),\n",
    "                \"Close_transfer_\"+tl_alg +'_' + rl_alg + '_' + reward_mode: eval(tl_alg+'_'+ rl_alg +\"_rewards_\"+ target_level[0] + '4' + '_' + target_level)\n",
    "               }\n",
    "        if save:\n",
    "            path = \"./\" + time.strftime(\"%Y%m%d\", time.localtime()) +'/' + ex_name + '_' + tl_alg + '_'  + rl_alg \n",
    "            saveData(data,path)\n",
    "    else:\n",
    "        data = all_data\n",
    "        path = \"./\"+ time.strftime(\"%Y%m%d\", time.localtime()) +'/'  + ex_name + '_' +  'alldata_'  + rl_alg \n",
    "        saveData(data,path)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PRQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 799800\n",
      "Training finished \n",
      "\n",
      "CPU times: user 7min 4s, sys: 1min 9s, total: 8min 13s\n",
      "Wall time: 7min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#pr_v001_003\n",
    "pr_episodes_length_v001_003, pr_penalties_v001_003,pr_rewards_v001_003, _, pr_q_table_v001_003 = transform(tl_algo= prql,\n",
    "                                                                                       episode_num = EPISODE_NUM,\n",
    "                                                                                       repeat_times = REPEAT_TIMES,\n",
    "                                                                                       target_task = \"Taxi-v003\",\n",
    "                                                                                       \n",
    "                                                                                       policy_library =  qlearn_q_table_v001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 799800\n",
      "Training finished \n",
      "\n",
      "CPU times: user 7min 14s, sys: 1min 28s, total: 8min 42s\n",
      "Wall time: 7min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#pr_v004_003\n",
    "pr_episodes_length_v004_003, pr_penalties_v004_003,pr_rewards_v004_003, _, pr_q_table_v004_003 = transform(tl_algo= prql,\n",
    "                                                                                       episode_num = EPISODE_NUM,\n",
    "                                                                                       repeat_times = REPEAT_TIMES,\n",
    "                                                                                       target_task = \"Taxi-v003\",\n",
    "                                                                                       \n",
    "                                                                                       policy_library =  qlearn_q_table_v004)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OPS-TL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4000\n",
      "Training finished\n",
      "CPU times: user 31min 22s, sys: 1min, total: 32min 23s\n",
      "Wall time: 31min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#ops_v001_003\n",
    "ops_episodes_length_v001_003, ops_penalties_v001_003,ops_rewards_v001_003, _, ops_q_table_v001_003 = transform(tl_algo= OPS_TL,\n",
    "                                                                                       episode_num = EPISODE_NUM,\n",
    "                                                                                   repeat_times = REPEAT_TIMES,\n",
    "                                                                                       target_task = \"Taxi-v003\",\n",
    "                                                                                       \n",
    "                                                                                       policy_library =  [[qfunc] for qfunc in qlearn_q_table_v001])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4000\n",
      "Training finished\n",
      "CPU times: user 47min 26s, sys: 39.8 s, total: 48min 6s\n",
      "Wall time: 47min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#ops_v004_003\n",
    "ops_episodes_length_v004_003, ops_penalties_v004_003,ops_rewards_v004_003, _, ops_q_table_v004_003 = transform(tl_algo= OPS_TL,\n",
    "                                                                                       episode_num = EPISODE_NUM,\n",
    "                                                                                       repeat_times = REPEAT_TIMES,\n",
    "                                                                                       target_task = \"Taxi-v003\",\n",
    "                                                                                       \n",
    "                                                                                       policy_library =  [[qfunc] for qfunc in qlearn_q_table_v004])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### caps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 799800\n",
      "Training finished\n",
      "CPU times: user 31min 14s, sys: 1min 18s, total: 32min 33s\n",
      "Wall time: 31min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#caps_v001_003\n",
    "caps_episodes_length_v001_003, caps_penalties_v001_003,caps_rewards_v001_003, _, caps_q_table_v001_003 = transform(tl_algo= caps,\n",
    "                                                                                       episode_num = EPISODE_NUM,\n",
    "                                                                                       repeat_times = REPEAT_TIMES,\n",
    "                                                                                       target_task = \"Taxi-v003\",\n",
    "                                                                                       \n",
    "                                                                                       policy_library =  [[qfunc] for qfunc in qlearn_q_table_v001])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 799800\n",
      "Training finished\n",
      "CPU times: user 49min 35s, sys: 44.3 s, total: 50min 20s\n",
      "Wall time: 49min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#caps_v004_003\n",
    "caps_episodes_length_v004_003, caps_penalties_v004_003,caps_rewards_v004_003, _, caps_q_table_v004_003 = transform(tl_algo= caps,\n",
    "                                                                                       episode_num = EPISODE_NUM,\n",
    "                                                                                       repeat_times = REPEAT_TIMES,\n",
    "                                                                                       target_task = \"Taxi-v003\",\n",
    "                                                                                       \n",
    "                                                                                       policy_library =  [[qfunc] for qfunc in qlearn_q_table_v004])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### qlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Transfer rate</th>\n",
       "      <th>Jumpstart</th>\n",
       "      <th>Asympotic performace</th>\n",
       "      <th>Time to threshold</th>\n",
       "      <th>Accumulated rewards</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>10.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.010</td>\n",
       "      <td>10.200</td>\n",
       "      <td>0.000</td>\n",
       "      <td>24.800</td>\n",
       "      <td>1310.200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.539</td>\n",
       "      <td>3.327</td>\n",
       "      <td>0.000</td>\n",
       "      <td>228.083</td>\n",
       "      <td>1841.795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.842</td>\n",
       "      <td>6.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-303.000</td>\n",
       "      <td>-988.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.231</td>\n",
       "      <td>8.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-90.000</td>\n",
       "      <td>118.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.084</td>\n",
       "      <td>10.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-32.500</td>\n",
       "      <td>927.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.336</td>\n",
       "      <td>12.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>155.000</td>\n",
       "      <td>2323.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.811</td>\n",
       "      <td>16.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>365.000</td>\n",
       "      <td>5114.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Transfer rate  Jumpstart  Asympotic performace  Time to threshold  \\\n",
       "count         10.000     10.000                10.000             10.000   \n",
       "mean           0.010     10.200                 0.000             24.800   \n",
       "std            0.539      3.327                 0.000            228.083   \n",
       "min           -0.842      6.000                 0.000           -303.000   \n",
       "25%           -0.231      8.000                 0.000            -90.000   \n",
       "50%           -0.084     10.000                 0.000            -32.500   \n",
       "75%            0.336     12.000                 0.000            155.000   \n",
       "max            0.811     16.000                 0.000            365.000   \n",
       "\n",
       "       Accumulated rewards  \n",
       "count               10.000  \n",
       "mean              1310.200  \n",
       "std               1841.795  \n",
       "min               -988.000  \n",
       "25%                118.000  \n",
       "50%                927.000  \n",
       "75%               2323.500  \n",
       "max               5114.000  "
      ]
     },
     "execution_count": 589,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# qlearn-Close transfer \n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "res = get_results(qlearn_rewards_v003,qlearn_rewards_v004_003)\n",
    "res = pd.DataFrame(res)\n",
    "res.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Transfer rate</th>\n",
       "      <th>Jumpstart</th>\n",
       "      <th>Asympotic performace</th>\n",
       "      <th>Time to threshold</th>\n",
       "      <th>Accumulated rewards</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>10.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-inf</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>-13.800</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-68990.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>nan</td>\n",
       "      <td>1.414</td>\n",
       "      <td>3.190</td>\n",
       "      <td>nan</td>\n",
       "      <td>2596.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-inf</td>\n",
       "      <td>-4.000</td>\n",
       "      <td>-18.000</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-72612.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-inf</td>\n",
       "      <td>-2.000</td>\n",
       "      <td>-16.000</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-71320.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-inf</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-14.000</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-69026.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>-inf</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-10.500</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-66954.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>-7.928</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-10.000</td>\n",
       "      <td>-3504.000</td>\n",
       "      <td>-64660.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Transfer rate  Jumpstart  Asympotic performace  Time to threshold  \\\n",
       "count         10.000     10.000                10.000             10.000   \n",
       "mean            -inf     -1.000               -13.800               -inf   \n",
       "std              nan      1.414                 3.190                nan   \n",
       "min             -inf     -4.000               -18.000               -inf   \n",
       "25%             -inf     -2.000               -16.000               -inf   \n",
       "50%             -inf      0.000               -14.000               -inf   \n",
       "75%             -inf      0.000               -10.500               -inf   \n",
       "max           -7.928      0.000               -10.000          -3504.000   \n",
       "\n",
       "       Accumulated rewards  \n",
       "count               10.000  \n",
       "mean            -68990.800  \n",
       "std               2596.006  \n",
       "min             -72612.000  \n",
       "25%             -71320.500  \n",
       "50%             -69026.000  \n",
       "75%             -66954.000  \n",
       "max             -64660.000  "
      ]
     },
     "execution_count": 590,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# qlearn-Far transfer\n",
    "res = get_results(qlearn_rewards_v003,qlearn_rewards_v001_003)\n",
    "res = pd.DataFrame(res)\n",
    "res.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PRQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Transfer rate</th>\n",
       "      <th>Jumpstart</th>\n",
       "      <th>Asympotic performace</th>\n",
       "      <th>Time to threshold</th>\n",
       "      <th>Accumulated rewards</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>10.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.910</td>\n",
       "      <td>-0.400</td>\n",
       "      <td>0.000</td>\n",
       "      <td>385.800</td>\n",
       "      <td>5854.600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.016</td>\n",
       "      <td>1.578</td>\n",
       "      <td>0.000</td>\n",
       "      <td>59.243</td>\n",
       "      <td>829.821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.886</td>\n",
       "      <td>-4.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>314.000</td>\n",
       "      <td>4970.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.898</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>338.000</td>\n",
       "      <td>5212.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.912</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>390.000</td>\n",
       "      <td>5705.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.917</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>412.500</td>\n",
       "      <td>6268.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.934</td>\n",
       "      <td>2.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>491.000</td>\n",
       "      <td>7528.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Transfer rate  Jumpstart  Asympotic performace  Time to threshold  \\\n",
       "count         10.000     10.000                10.000             10.000   \n",
       "mean           0.910     -0.400                 0.000            385.800   \n",
       "std            0.016      1.578                 0.000             59.243   \n",
       "min            0.886     -4.000                 0.000            314.000   \n",
       "25%            0.898      0.000                 0.000            338.000   \n",
       "50%            0.912      0.000                 0.000            390.000   \n",
       "75%            0.917      0.000                 0.000            412.500   \n",
       "max            0.934      2.000                 0.000            491.000   \n",
       "\n",
       "       Accumulated rewards  \n",
       "count               10.000  \n",
       "mean              5854.600  \n",
       "std                829.821  \n",
       "min               4970.000  \n",
       "25%               5212.500  \n",
       "50%               5705.000  \n",
       "75%               6268.500  \n",
       "max               7528.000  "
      ]
     },
     "execution_count": 591,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pr-Close transfer\n",
    "res = get_results(qlearn_rewards_v003,pr_rewards_v004_003)\n",
    "res = pd.DataFrame(res)\n",
    "res.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Transfer rate</th>\n",
       "      <th>Jumpstart</th>\n",
       "      <th>Asympotic performace</th>\n",
       "      <th>Time to threshold</th>\n",
       "      <th>Accumulated rewards</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>10.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.397</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>0.000</td>\n",
       "      <td>171.000</td>\n",
       "      <td>2371.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.105</td>\n",
       "      <td>1.476</td>\n",
       "      <td>0.000</td>\n",
       "      <td>66.483</td>\n",
       "      <td>1034.359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.226</td>\n",
       "      <td>-4.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>92.000</td>\n",
       "      <td>1198.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.359</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>140.000</td>\n",
       "      <td>1890.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.388</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>155.000</td>\n",
       "      <td>2155.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.399</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>182.750</td>\n",
       "      <td>2335.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.637</td>\n",
       "      <td>2.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>335.000</td>\n",
       "      <td>4742.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Transfer rate  Jumpstart  Asympotic performace  Time to threshold  \\\n",
       "count         10.000     10.000                10.000             10.000   \n",
       "mean           0.397     -0.200                 0.000            171.000   \n",
       "std            0.105      1.476                 0.000             66.483   \n",
       "min            0.226     -4.000                 0.000             92.000   \n",
       "25%            0.359      0.000                 0.000            140.000   \n",
       "50%            0.388      0.000                 0.000            155.000   \n",
       "75%            0.399      0.000                 0.000            182.750   \n",
       "max            0.637      2.000                 0.000            335.000   \n",
       "\n",
       "       Accumulated rewards  \n",
       "count               10.000  \n",
       "mean              2371.000  \n",
       "std               1034.359  \n",
       "min               1198.000  \n",
       "25%               1890.000  \n",
       "50%               2155.000  \n",
       "75%               2335.500  \n",
       "max               4742.000  "
      ]
     },
     "execution_count": 592,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pr-Far transfer\n",
    "res = get_results(qlearn_rewards_v003,pr_rewards_v001_003)\n",
    "res = pd.DataFrame(res)\n",
    "res.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### OPS-TL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ops_rewards_v004_003' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-229-3b1f5059d321>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# ops-Close transfer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqlearn_rewards_v003\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mops_rewards_v004_003\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ops_rewards_v004_003' is not defined"
     ]
    }
   ],
   "source": [
    "# ops-Close transfer\n",
    "res = get_results(qlearn_rewards_v003,ops_rewards_v004_003)\n",
    "res = pd.DataFrame(res)\n",
    "res.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Transfer rate</th>\n",
       "      <th>Jumpstart</th>\n",
       "      <th>Asympotic performace</th>\n",
       "      <th>Time to threshold</th>\n",
       "      <th>Accumulated rewards</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>10.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.469</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>0.000</td>\n",
       "      <td>200.800</td>\n",
       "      <td>2892.600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.142</td>\n",
       "      <td>1.647</td>\n",
       "      <td>0.000</td>\n",
       "      <td>69.723</td>\n",
       "      <td>976.823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.272</td>\n",
       "      <td>-4.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>98.000</td>\n",
       "      <td>1606.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.384</td>\n",
       "      <td>-1.500</td>\n",
       "      <td>0.000</td>\n",
       "      <td>153.000</td>\n",
       "      <td>2026.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.460</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>222.500</td>\n",
       "      <td>3138.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.577</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>247.750</td>\n",
       "      <td>3570.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.681</td>\n",
       "      <td>2.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>301.000</td>\n",
       "      <td>4226.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Transfer rate  Jumpstart  Asympotic performace  Time to threshold  \\\n",
       "count         10.000     10.000                10.000             10.000   \n",
       "mean           0.469     -0.600                 0.000            200.800   \n",
       "std            0.142      1.647                 0.000             69.723   \n",
       "min            0.272     -4.000                 0.000             98.000   \n",
       "25%            0.384     -1.500                 0.000            153.000   \n",
       "50%            0.460      0.000                 0.000            222.500   \n",
       "75%            0.577      0.000                 0.000            247.750   \n",
       "max            0.681      2.000                 0.000            301.000   \n",
       "\n",
       "       Accumulated rewards  \n",
       "count               10.000  \n",
       "mean              2892.600  \n",
       "std                976.823  \n",
       "min               1606.000  \n",
       "25%               2026.000  \n",
       "50%               3138.000  \n",
       "75%               3570.500  \n",
       "max               4226.000  "
      ]
     },
     "execution_count": 594,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ops-Far transfer\n",
    "res = get_results(qlearn_rewards_v003,ops_rewards_v001_003)\n",
    "res = pd.DataFrame(res)\n",
    "res.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### caps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Transfer rate</th>\n",
       "      <th>Jumpstart</th>\n",
       "      <th>Asympotic performace</th>\n",
       "      <th>Time to threshold</th>\n",
       "      <th>Accumulated rewards</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>10.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.904</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>383.400</td>\n",
       "      <td>5813.600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.021</td>\n",
       "      <td>1.886</td>\n",
       "      <td>0.000</td>\n",
       "      <td>61.446</td>\n",
       "      <td>845.584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.877</td>\n",
       "      <td>-4.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>308.000</td>\n",
       "      <td>4908.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.889</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>335.500</td>\n",
       "      <td>5190.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.901</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>381.000</td>\n",
       "      <td>5567.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.915</td>\n",
       "      <td>1.500</td>\n",
       "      <td>0.000</td>\n",
       "      <td>411.250</td>\n",
       "      <td>6331.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.943</td>\n",
       "      <td>2.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>489.000</td>\n",
       "      <td>7450.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Transfer rate  Jumpstart  Asympotic performace  Time to threshold  \\\n",
       "count         10.000     10.000                10.000             10.000   \n",
       "mean           0.904      0.000                 0.000            383.400   \n",
       "std            0.021      1.886                 0.000             61.446   \n",
       "min            0.877     -4.000                 0.000            308.000   \n",
       "25%            0.889      0.000                 0.000            335.500   \n",
       "50%            0.901      0.000                 0.000            381.000   \n",
       "75%            0.915      1.500                 0.000            411.250   \n",
       "max            0.943      2.000                 0.000            489.000   \n",
       "\n",
       "       Accumulated rewards  \n",
       "count               10.000  \n",
       "mean              5813.600  \n",
       "std                845.584  \n",
       "min               4908.000  \n",
       "25%               5190.500  \n",
       "50%               5567.000  \n",
       "75%               6331.000  \n",
       "max               7450.000  "
      ]
     },
     "execution_count": 595,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# caps-Close transfer\n",
    "res = get_results(qlearn_rewards_v003,caps_rewards_v004_003)\n",
    "res = pd.DataFrame(res)\n",
    "res.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Transfer rate</th>\n",
       "      <th>Jumpstart</th>\n",
       "      <th>Asympotic performace</th>\n",
       "      <th>Time to threshold</th>\n",
       "      <th>Accumulated rewards</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>10.000</td>\n",
       "      <td>10.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.436</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.000</td>\n",
       "      <td>190.900</td>\n",
       "      <td>2839.400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.205</td>\n",
       "      <td>2.503</td>\n",
       "      <td>0.000</td>\n",
       "      <td>106.820</td>\n",
       "      <td>1560.574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.154</td>\n",
       "      <td>-4.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>59.000</td>\n",
       "      <td>322.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.279</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>102.500</td>\n",
       "      <td>1780.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.439</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>181.500</td>\n",
       "      <td>2611.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.632</td>\n",
       "      <td>2.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>287.750</td>\n",
       "      <td>4118.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.693</td>\n",
       "      <td>4.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>343.000</td>\n",
       "      <td>5212.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Transfer rate  Jumpstart  Asympotic performace  Time to threshold  \\\n",
       "count         10.000     10.000                10.000             10.000   \n",
       "mean           0.436      0.600                 0.000            190.900   \n",
       "std            0.205      2.503                 0.000            106.820   \n",
       "min            0.154     -4.000                 0.000             59.000   \n",
       "25%            0.279      0.000                 0.000            102.500   \n",
       "50%            0.439      0.000                 0.000            181.500   \n",
       "75%            0.632      2.000                 0.000            287.750   \n",
       "max            0.693      4.000                 0.000            343.000   \n",
       "\n",
       "       Accumulated rewards  \n",
       "count               10.000  \n",
       "mean              2839.400  \n",
       "std               1560.574  \n",
       "min                322.000  \n",
       "25%               1780.000  \n",
       "50%               2611.000  \n",
       "75%               4118.000  \n",
       "max               5212.000  "
      ]
     },
     "execution_count": 597,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# caps-Far transfer\n",
    "res = get_results(qlearn_rewards_v003,caps_rewards_v001_003)\n",
    "res = pd.DataFrame(res)\n",
    "res.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### OC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Transfer Setting|**Transfer Rate**|**Jumpstart(100 epsidoe)**|**Asymptotic Performance**|**Time to Threshold**|**Accumulated Rewards**|\n",
    "|---|---|---|---|---|---|\n",
    "|***Close transfer***|p =0.000|p =0.000|p =0.000|p =0.000|p =0.000|\n",
    "|Q-table|0.010$\\pm$0.539|8.400$\\pm$3.978|0.000$\\pm$0.000|24.800$\\pm$228.083|1310.200$\\pm$1841.795|\n",
    "|PRQL|**0.910**$\\pm$0.016|**17.600**$\\pm$1.578|0.000$\\pm$0.000|**385.800**$\\pm$59.243|**5854.600**$\\pm$829.821|\n",
    "|OPS-TL|0.883$\\pm$0.032|17.400$\\pm$1.647|0.000$\\pm$0.000|375.000$\\pm$63.821|5682.600$\\pm$904.087|\n",
    "|caps|0.904$\\pm$0.021|**17.600**$\\pm$1.578|0.000$\\pm$0.000|383.400$\\pm$61.446|5813.600$\\pm$845.584|\n",
    "|***Far transfer***|\n",
    "|Q-table|-4.08$\\pm$0.234|-2.400$\\pm$1.578|-13.800$\\pm$3.190|-inf$\\pm$nan|-68990.800$\\pm$2596.006|\n",
    "|PRQL|0.397$\\pm$0.105|1.000$\\pm$2.160|0.000$\\pm$0.000|171.000$\\pm$66.483|2371.000$\\pm$1034.359|\n",
    "|OPS-TL|**0.469**$\\pm$0.142|4.600$\\pm$5.502|0.000$\\pm$0.000|**200.800**$\\pm$69.723|**2892.600**$\\pm$976.823|\n",
    "|caps|0.436$\\pm$0.205|**2.400**$\\pm$3.627|0.000$\\pm$0.000|190.900$\\pm$106.820|2839.400$\\pm$1560.574|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qlearn\n",
      "Two-sample ks-test statistic =  1.000, p-value = 0.0000\n",
      "pr\n",
      "Two-sample ks-test statistic =  0.092, p-value = 0.0000\n",
      "ops\n",
      "Two-sample ks-test statistic =  0.068, p-value = 0.0000\n",
      "caps\n",
      "Two-sample ks-test statistic =  0.112, p-value = 0.0000\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "import scipy.optimize as opt\n",
    "algs = ['qlearn','pr','ops','caps']\n",
    "level=['Close_transfer_', 'Far_transfer_']\n",
    "from scipy.stats import ks_2samp, kstest   \n",
    "for alg in algs:\n",
    "    print(alg)\n",
    "    stat_val, p_val = stats.ks_2samp(np.mean(all_data[level[0]+alg],axis=0), np.mean(all_data[level[1]+alg],axis=0))\n",
    "    #看看两个分布在均值上有没有显著差异\n",
    "    #注意，这里我们生成的第二组数据样本大小、方差和第一组均不相等，在运用t检验时需要使用Welch's t-test\n",
    "    #即指定ttest_ind中的equal_var=False。\n",
    "    print ('Two-sample ks-test statistic = %6.3f, p-value = %6.4f' % (stat_val, p_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 799800\n",
      "Training finished\n",
      "CPU times: user 12min 34s, sys: 3min 3s, total: 15min 37s\n",
      "Wall time: 13min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "option_policies, option_terminations, policy_over_options, nrewards,critic = [ [None]*4 for i in range(5) ] \n",
    "env_list = [ \"Taxi-v001\", \"Taxi-v003\", \"Taxi-v004\"]\n",
    "\n",
    "for i in range(len(env_list)):\n",
    "    _, _ ,nrewards[i] ,option_policies[i], option_terminations[i], policy_over_options[i],critic[i] = oc(env = gym.make(env_list[i]),episode_num = 4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 799800\n",
      "Training finished\n",
      "CPU times: user 2h 50min 45s, sys: 43min 35s, total: 3h 34min 20s\n",
      "Wall time: 3h 6min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "oc_rewards_v001_003, oc_rewards_v004_003 = np.zeros([10,4000]), np.zeros([10,4000])\n",
    "for i in range(10):\n",
    "    _, _ ,oc_rewards_v001_003[i] ,_, _, _,_ = oc(env = gym.make( \"Taxi-v003\"),episode_num = 4000,option_policies_lib=option_policies[0],option_terminations_lib =option_terminations[0] )\n",
    "    _, _ ,oc_rewards_v004_003[i] ,_, _, _,_ = oc(env = gym.make(\"Taxi-v003\"),episode_num = 4000,option_policies_lib=option_policies[2],option_terminations_lib =option_terminations[2] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'oc_rewards_v001_003' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-163-e271d37a9eaf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moc_rewards_v001_003\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'oc_rewards_v001_003' is not defined"
     ]
    }
   ],
   "source": [
    "oc_rewards_v001_003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Close_transfer_\n",
      "qlearn\n",
      "|0.106$\\pm$0.431|9.800$\\pm$2.898|0.000$\\pm$0.000|50.400$\\pm$179.353|2054.800$\\pm$1465.836|\n",
      "pr\n",
      "|0.915$\\pm$0.015|-0.800$\\pm$1.932|0.000$\\pm$0.000|411.400$\\pm$85.930|6599.200$\\pm$1285.746|\n",
      "ops\n",
      "|0.890$\\pm$0.029|0.200$\\pm$2.201|0.000$\\pm$0.000|400.600$\\pm$87.983|6427.200$\\pm$1286.940|\n",
      "caps\n",
      "|0.910$\\pm$0.017|-0.400$\\pm$2.066|0.000$\\pm$0.000|409.000$\\pm$85.825|6558.200$\\pm$1260.261|\n",
      "Far_transfer_\n",
      "qlearn\n",
      "|-inf$\\pm$nan|-1.400$\\pm$1.350|-13.800$\\pm$3.190|-inf$\\pm$nan|-68246.200$\\pm$2539.523|\n",
      "pr\n",
      "|0.418$\\pm$0.154|-0.600$\\pm$1.897|0.000$\\pm$0.000|196.600$\\pm$105.695|3115.600$\\pm$1626.796|\n",
      "ops\n",
      "|0.497$\\pm$0.127|-1.000$\\pm$1.944|0.000$\\pm$0.000|226.400$\\pm$83.228|3637.200$\\pm$1187.541|\n",
      "caps\n",
      "|0.477$\\pm$0.162|0.200$\\pm$2.394|0.000$\\pm$0.000|216.500$\\pm$90.092|3584.000$\\pm$1006.378|\n"
     ]
    }
   ],
   "source": [
    "algs = ['qlearn','pr','ops','caps']\n",
    "for level in ['Close_transfer_', 'Far_transfer_']:\n",
    "    print(level)\n",
    "    for alg in algs:\n",
    "        print(alg)\n",
    "        res = get_results(qlearn_rewards_v003,all_data[level+alg])\n",
    "        res = pd.DataFrame(res)\n",
    "        for i in range(len(res.std())):\n",
    "            print(\"|{:.3f}$\\pm${:.3f}\".format(round(res.mean()[i],3),round(res.std()[i],3)),end='')\n",
    "        print(\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Separated figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac71f5ff76f4440392a19386c1df762f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc93285b008e4d3eb55582148df24fb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e03de35c46f54c648a51dbe5cc73ff96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11c9dc581038455abb563597dc999b47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4827a1eef35d453bae3352a876cbd61f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1be74b6e2df41f6a5862f5e9931c197",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29e93994623f40bd9b4e55c4e5e40c4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5e594c0d3614bb3a9a89d65e2a7a342",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34979c06586a43edbc159853d7607de1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f6308749f0b4243afa434d14d4ff793",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "plt.rcParams['savefig.dpi'] = 600\n",
    "algs = [\"qtable\",'stable', \"prql\", \"ops\", \"caps\"]\n",
    "data =[None] *len(algs)\n",
    "for i,alg in enumerate(algs):\n",
    "    data[i] = cache(ex = \"similarity_diff\", tl_alg = alg,rl_alg='qlearn')\n",
    "    plot_result(data[i],1000,smooth_radius=1)\n",
    "#     plt.savefig(fname = './combined_figure_similarity_diff_'+ alg ,dpi = 600)\n",
    "    plot_result(data[i],500,combine_figures =False,average_group=False,figsize=(9,12),smooth_radius=1) \n",
    "#     plt.savefig(fname = './separated_figure_similarity_diff_'+ alg ,dpi = 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### combined figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Close_transfer_caps': array([[ 0.,  0.,  6., ..., 20., 20., 20.],\n",
       "        [ 0.,  0.,  4., ..., 20., 20., 20.],\n",
       "        [ 0.,  0.,  0., ..., 20., 20., 20.],\n",
       "        ...,\n",
       "        [ 2.,  2.,  6., ..., 20., 20., 20.],\n",
       "        [ 4.,  0.,  2., ..., 20., 20., 20.],\n",
       "        [ 0.,  8.,  8., ..., 20., 20., 20.]]),\n",
       " 'Close_transfer_ops': array([[ 0.,  2.,  4., ..., 20., 20., 20.],\n",
       "        [ 0.,  0.,  0., ..., 20., 20., 20.],\n",
       "        [ 0.,  0.,  4., ..., 20., 20., 20.],\n",
       "        ...,\n",
       "        [ 0.,  0.,  0., ..., 20., 20., 20.],\n",
       "        [ 0.,  0.,  2., ..., 20., 20., 20.],\n",
       "        [ 0.,  4.,  0., ..., 20., 20., 20.]]),\n",
       " 'Close_transfer_prql': array([[ 0.,  0.,  0., ..., 20., 20., 20.],\n",
       "        [ 0.,  2.,  0., ..., 20., 20., 20.],\n",
       "        [ 4.,  2.,  2., ..., 20., 20., 20.],\n",
       "        ...,\n",
       "        [ 0.,  0.,  0., ..., 20., 20., 20.],\n",
       "        [ 0.,  0.,  0., ..., 20., 20., 20.],\n",
       "        [ 0.,  2.,  6., ..., 20., 20., 20.]]),\n",
       " 'Far_transfer_caps': array([[ 2.,  2.,  4., ..., 20., 20., 20.],\n",
       "        [ 2.,  0.,  2., ..., 20., 20., 20.],\n",
       "        [ 6.,  2.,  4., ..., 20., 20., 20.],\n",
       "        ...,\n",
       "        [ 4.,  0.,  2., ..., 20., 20., 20.],\n",
       "        [ 0.,  2.,  0., ..., 20., 20., 20.],\n",
       "        [ 0.,  0.,  4., ..., 20., 20., 20.]]),\n",
       " 'Far_transfer_ops': array([[ 2.,  2.,  0., ..., 20., 20., 20.],\n",
       "        [ 2.,  0.,  0., ..., 20., 20., 20.],\n",
       "        [ 0.,  0.,  0., ..., 20., 20., 20.],\n",
       "        ...,\n",
       "        [ 0.,  0.,  8., ..., 20., 20., 20.],\n",
       "        [ 2.,  4.,  4., ..., 20., 20., 20.],\n",
       "        [ 0.,  8.,  2., ..., 20., 20., 20.]]),\n",
       " 'Far_transfer_prql': array([[ 0.,  4.,  0., ..., 20., 20., 20.],\n",
       "        [ 0.,  0.,  4., ..., 20., 20., 20.],\n",
       "        [ 2.,  4.,  0., ..., 20., 20., 20.],\n",
       "        ...,\n",
       "        [ 0.,  2.,  0., ..., 20., 20., 20.],\n",
       "        [ 0.,  0.,  0., ..., 20., 20., 20.],\n",
       "        [ 0.,  0.,  6., ..., 20., 20., 20.]]),\n",
       " 'No_transfer_caps': array([[ 0.,  2.,  0., ..., 20., 20., 20.],\n",
       "        [ 2.,  0.,  0., ..., 20., 20., 20.],\n",
       "        [ 0.,  2.,  0., ..., 20., 20., 20.],\n",
       "        ...,\n",
       "        [ 2.,  0.,  0., ..., 20., 20., 20.],\n",
       "        [ 0.,  0.,  2., ..., 20., 20., 20.],\n",
       "        [ 0.,  0.,  2., ..., 20., 20., 20.]]),\n",
       " 'No_transfer_ops': array([[ 0.,  2.,  0., ..., 20., 20., 20.],\n",
       "        [ 2.,  0.,  0., ..., 20., 20., 20.],\n",
       "        [ 0.,  2.,  0., ..., 20., 20., 20.],\n",
       "        ...,\n",
       "        [ 2.,  0.,  0., ..., 20., 20., 20.],\n",
       "        [ 0.,  0.,  2., ..., 20., 20., 20.],\n",
       "        [ 0.,  0.,  2., ..., 20., 20., 20.]]),\n",
       " 'No_transfer_prql': array([[ 0.,  2.,  0., ..., 20., 20., 20.],\n",
       "        [ 2.,  0.,  0., ..., 20., 20., 20.],\n",
       "        [ 0.,  2.,  0., ..., 20., 20., 20.],\n",
       "        ...,\n",
       "        [ 2.,  0.,  0., ..., 20., 20., 20.],\n",
       "        [ 0.,  0.,  2., ..., 20., 20., 20.],\n",
       "        [ 0.,  0.,  2., ..., 20., 20., 20.]])}"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache(all_data = all_data,rl_alg ='sarsa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Close_transfer_caps': array([[ 0.,  0.,  4., ..., 20., 20., 20.],\n",
       "        [ 0.,  0.,  2., ..., 20., 20., 20.],\n",
       "        [ 0.,  0.,  2., ..., 20., 20., 20.],\n",
       "        ...,\n",
       "        [ 0.,  0.,  0., ..., 20., 20., 20.],\n",
       "        [ 2.,  0.,  0., ..., 20., 20., 20.],\n",
       "        [ 2.,  2.,  0., ..., 20., 20., 20.]]),\n",
       " 'Close_transfer_ops': array([[ 2.,  0.,  0., ..., 20., 20., 20.],\n",
       "        [ 0.,  0.,  0., ..., 20., 20., 20.],\n",
       "        [ 2.,  0.,  4., ..., 20., 20., 20.],\n",
       "        ...,\n",
       "        [ 2.,  0.,  0., ..., 20., 20., 20.],\n",
       "        [ 2.,  4.,  0., ..., 20., 20., 20.],\n",
       "        [ 4.,  2.,  0., ..., 20., 20., 20.]]),\n",
       " 'Close_transfer_pr': array([[ 0.,  4.,  0., ..., 20., 20., 20.],\n",
       "        [ 0.,  0.,  2., ..., 20., 20., 20.],\n",
       "        [ 0.,  0.,  0., ..., 20., 20., 20.],\n",
       "        ...,\n",
       "        [ 0.,  0.,  2., ..., 20., 20., 20.],\n",
       "        [ 0.,  2.,  2., ..., 20., 20., 20.],\n",
       "        [ 2.,  0.,  2., ..., 20., 20., 20.]]),\n",
       " 'Close_transfer_qlearn': array([[12., 18.,  8., ..., 20., 20., 20.],\n",
       "        [12., 10., 10., ..., 20., 20., 20.],\n",
       "        [10.,  8., 14., ..., 20., 20., 20.],\n",
       "        ...,\n",
       "        [14.,  4., 10., ..., 20., 20., 20.],\n",
       "        [16., 18., 10., ..., 20., 20., 20.],\n",
       "        [ 8., 10.,  6., ..., 20., 20., 20.]]),\n",
       " 'Far_transfer_caps': array([[ 2.,  0.,  0., ..., 20., 20., 20.],\n",
       "        [ 0.,  0.,  0., ..., 20., 20., 20.],\n",
       "        [ 0.,  2.,  0., ..., 20., 20., 20.],\n",
       "        ...,\n",
       "        [ 0.,  2.,  2., ..., 20., 20., 20.],\n",
       "        [ 2.,  0.,  0., ..., 20., 20., 20.],\n",
       "        [ 0.,  0.,  0., ..., 20., 20., 20.]]),\n",
       " 'Far_transfer_ops': array([[ 0.,  2.,  2., ..., 20., 20., 20.],\n",
       "        [ 0.,  0.,  0., ..., 20., 20., 20.],\n",
       "        [ 0.,  2.,  0., ..., 20., 20., 20.],\n",
       "        ...,\n",
       "        [ 0.,  2.,  0., ..., 20., 20., 20.],\n",
       "        [ 0.,  2.,  2., ..., 20., 20., 20.],\n",
       "        [ 0.,  2.,  2., ..., 20., 20., 20.]]),\n",
       " 'Far_transfer_pr': array([[ 0.,  2.,  2., ..., 20., 20., 20.],\n",
       "        [ 0.,  2.,  2., ..., 20., 20., 20.],\n",
       "        [ 0.,  2.,  0., ..., 20., 20., 20.],\n",
       "        ...,\n",
       "        [ 2.,  2.,  0., ..., 20., 20., 20.],\n",
       "        [ 0.,  0.,  2., ..., 20., 20., 20.],\n",
       "        [ 2.,  2.,  0., ..., 20., 20., 20.]]),\n",
       " 'Far_transfer_qlearn': array([[ 0.,  0.,  0., ...,  4.,  4.,  4.],\n",
       "        [ 0.,  0.,  0., ...,  6.,  6.,  2.],\n",
       "        [ 0.,  0.,  0., ...,  8.,  0.,  6.],\n",
       "        ...,\n",
       "        [ 0.,  0.,  0., ...,  4.,  2., 10.],\n",
       "        [ 0.,  0.,  0., ...,  4., 10.,  4.],\n",
       "        [ 0.,  0.,  0., ...,  2.,  6., 10.]]),\n",
       " 'No_transfer_qlearn': array([[ 0.,  0.,  0., ..., 20., 20., 20.],\n",
       "        [ 4.,  0.,  0., ..., 20., 20., 20.],\n",
       "        [ 0.,  2.,  0., ..., 20., 20., 20.],\n",
       "        ...,\n",
       "        [ 0.,  0.,  0., ..., 20., 20., 20.],\n",
       "        [ 0.,  2.,  0., ..., 20., 20., 20.],\n",
       "        [ 2.,  4.,  0., ..., 20., 20., 20.]])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algs = ['qlearn','pr','ops','caps']\n",
    "data = [None] * len(algs)\n",
    "all_data = {}\n",
    "for i,alg in enumerate(algs):\n",
    "    data[i] = loadData('./past_ex_data/' + \"similarity_diff\" + '_' + alg + '_'  + '20210429' )\n",
    "    all_data.update(data[i])\n",
    "all_data    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "algs = ['prql','ops','caps']\n",
    "data = [None] * len(algs)\n",
    "all_data = {}\n",
    "for i,alg in enumerate(algs):\n",
    "    data[i] = cache(tl_alg =alg,rl_alg ='sarsa')\n",
    "    all_data.update(data[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['qlearn', 'pr', 'ops', 'caps']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11b0ee2ba42544f4a8861e8b79fc5bc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['qlearn', 'pr', 'ops', 'caps']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9923a90c8a8440049cf293f6c91b37d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plot_result(all_data,4000,title = \"3 similarity + 4 algs : 4000 episode-rewareds graph\")\n",
    "#plt.savefig('./3similarity+4algs:4000episode',dpi =  600)\n",
    "plot_result(all_data,1000,title = \"3 similarity + 4 algs : 1000 episode-rewareds graph\", show_std= False ,smooth_radius = 20)\n",
    "#plt.savefig('./3similarity+4algs:1000episode',dpi =  600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Exp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 155800\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-abc5b5252d29>\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(target_rl_algo, episode_num, repeat_times, target_task, source_task, source_rl_algo, policy_library)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mepisodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpenalties\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mknowledge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_rl_algo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-c83c33ebef2c>\u001b[0m in \u001b[0;36mq_learning\u001b[0;34m(env, train_episodes, init_q_table)\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m200\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0;31m#evaluate policy for learning curve after each episode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                 \u001b[0mepisode_rewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_penalties\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_copy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_table\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEVALUATION_TIMES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0mall_episodes_length\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-4d8b12d466fa>\u001b[0m in \u001b[0;36mpolicy_evaluate\u001b[0;34m(env, policy, times)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAX_EPISODE_LEN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg_max\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;31m#update data for learning curve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gym/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gym/gym/envs/toy_text/discrete.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mtransitions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcategorical_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransitions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtransitions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gym/gym/envs/toy_text/discrete.py\u001b[0m in \u001b[0;36mcategorical_sample\u001b[0;34m(prob_n, np_random)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprob_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mcsprob_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcsprob_n\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mnp_random\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# training Taxi-v3 from scratch\n",
    "qlearn_episodes_length_v3, qlearn_penalties_v3, qlearn_rewards_v3, _, qlearn_q_table_v3 = transform(tl_algo = q_learning,\n",
    "                                                                                     episode_num = EPISODE_NUM, \n",
    "                                                                                     repeat_times = REPEAT_TIMES,\n",
    "                                                                                     target_task = \"Taxi-v3\",\n",
    "                                                                                     source_task = None\n",
    "                                                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 799800\n",
      "Training finished\n",
      "CPU times: user 8min 48s, sys: 1min 21s, total: 10min 9s\n",
      "Wall time: 9min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# training Taxi-v1 from scratch\n",
    "qlearn_episodes_length_v1, qlearn_penalties_v1, qlearn_rewards_v1,_, qlearn_q_table_v1 = transform(tl_algo = q_learning,\n",
    "                                                                                     episode_num = EPISODE_NUM, \n",
    "                                                                                     repeat_times = REPEAT_TIMES,\n",
    "                                                                                     target_task = \"Taxi-v1\",\n",
    "                                                                                     source_task = None\n",
    "                                                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 799800\n",
      "Training finished\n",
      "CPU times: user 9min 6s, sys: 1min 13s, total: 10min 19s\n",
      "Wall time: 9min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# training Taxi-v4 from scratch\n",
    "qlearn_episodes_length_v4, qlearn_penalties_v4, qlearn_rewards_v4,_, qlearn_q_table_v4 = transform(tl_algo = q_learning,\n",
    "                                                                                     episode_num = EPISODE_NUM, \n",
    "                                                                                     repeat_times = REPEAT_TIMES,\n",
    "                                                                                     target_task = \"Taxi-v4\",\n",
    "                                                                                     source_task = None\n",
    "                                                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 799800\n",
      "Training finished\n",
      "CPU times: user 9min 5s, sys: 1min 23s, total: 10min 28s\n",
      "Wall time: 9min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# training Taxi-v03 from scratch\n",
    "qlearn_episodes_length_v03, qlearn_penalties_v03, qlearn_rewards_v03, _, qlearn_q_table_v03 = transform(tl_algo = q_learning,\n",
    "                                                                                     episode_num = EPISODE_NUM, \n",
    "                                                                                     repeat_times = REPEAT_TIMES,\n",
    "                                                                                     target_task = \"Taxi-v03\",\n",
    "                                                                                     source_task = None\n",
    "                                                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 799800\n",
      "Training finished\n",
      "CPU times: user 8min 48s, sys: 1min 21s, total: 10min 9s\n",
      "Wall time: 9min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# training Taxi-v01 from scratch\n",
    "qlearn_episodes_length_v01, qlearn_penalties_v01, qlearn_rewards_v01,_, qlearn_q_table_v01 = transform(tl_algo = q_learning,\n",
    "                                                                                     episode_num = EPISODE_NUM, \n",
    "                                                                                     repeat_times = REPEAT_TIMES,\n",
    "                                                                                     target_task = \"Taxi-v01\",\n",
    "                                                                                     source_task = None\n",
    "                                                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 799800\n",
      "Training finished\n",
      "CPU times: user 9min 6s, sys: 1min 13s, total: 10min 19s\n",
      "Wall time: 9min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# training Taxi-v04 from scratch\n",
    "qlearn_episodes_length_v04, qlearn_penalties_v04, qlearn_rewards_v04,_, qlearn_q_table_v04 = transform(tl_algo = q_learning,\n",
    "                                                                                     episode_num = EPISODE_NUM, \n",
    "                                                                                     repeat_times = REPEAT_TIMES,\n",
    "                                                                                     target_task = \"Taxi-v04\",\n",
    "                                                                                     source_task = None\n",
    "                                                                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Exp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 154400\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-9fc5cc68f80b>\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(target_rl_algo, episode_num, repeat_times, target_task, source_task, source_rl_algo, policy_library)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mpast_policies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_library\u001b[0m \u001b[0;31m# policy_library = [ [[policy11][policy21]...] [[policy12][policy22]...]...]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtarget_rl_algo\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mCAPS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m                 \u001b[0mepisodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpenalties\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfrequency\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mknowledge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_rl_algo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_episodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast_policies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mepisodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpenalties\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mknowledge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_rl_algo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_episodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast_policies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-0307c0ff7df1>\u001b[0m in \u001b[0;36mCAPS\u001b[0;34m(env, train_episodes, rng, past_policies)\u001b[0m\n\u001b[1;32m     53\u001b[0m             episode_penalties, episode_rewards, episode_length,i = policy_reuse(timesteps, env, q_table, state, i,\n\u001b[1;32m     54\u001b[0m                                                                               \u001b[0mall_episodes_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mall_penalties\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mall_rewards\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m                                                                               past_policy = past_policies[j])\n\u001b[0m\u001b[1;32m     56\u001b[0m             \u001b[0mpolicy_expect\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpolicy_expect\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpolicy_times\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mepisode_rewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mpolicy_times\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mpolicy_times\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-5d9eb6ae084e>\u001b[0m in \u001b[0;36mpolicy_reuse\u001b[0;34m(timesteps, env, q_table, initial_state, i, all_episodes_length, all_penalties, all_rewards, past_policy, fi, mu, alpha, gamma)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# update q-value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mq_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mq_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mq_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mamax\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spinningup/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mamax\u001b[0;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2704\u001b[0m     \"\"\"\n\u001b[1;32m   2705\u001b[0m     return _wrapreduction(a, np.maximum, 'max', axis, None, out,\n\u001b[0;32m-> 2706\u001b[0;31m                           keepdims=keepdims, initial=initial, where=where)\n\u001b[0m\u001b[1;32m   2707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spinningup/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#caps_v014_003\n",
    "caps_episodes_length_v014_003, caps_penalties_v014_003,caps_rewards_v014_003,caps_fre_v014_003, caps_q_table_v014_003 = transform(tl_algo= caps,\n",
    "                                                                                       episode_num = EPISODE_NUM,\n",
    "                                                                                       repeat_times = REPEAT_TIMES,\n",
    "                                                                                       target_task = \"Taxi-v003\",\n",
    "                                                                                      \n",
    "                                                                                       policy_library = [[q_func1,q_func2,q_func3 ] for q_func1,q_func2,q_func3 in zip(qlearn_q_table_v000, qlearn_q_table_v001, qlearn_q_table_v004)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 's_table' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-9fc5cc68f80b>\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(target_rl_algo, episode_num, repeat_times, target_task, source_task, source_rl_algo, policy_library)\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0mepisodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpenalties\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfrequency\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mknowledge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_rl_algo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_episodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast_policies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m                 \u001b[0mepisodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpenalties\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mknowledge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_rl_algo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_episodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast_policies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;31m#Situation 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0msource_task\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-720e1c574ecd>\u001b[0m in \u001b[0;36mprql\u001b[0;34m(env, train_episodes, rng, past_policy)\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m200\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                 \u001b[0;31m#evaluate policy for learning curve after each episode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m                 \u001b[0mepisode_rewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_penalties\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_copy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_table\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEVALUATION_TIMES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mall_episodes_length\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 's_table' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#pr_v014_003\n",
    "pr_episodes_length_v014_003, pr_penalties_v014_003,pr_rewards_v014_003, _, pr_q_table_v014_003 = transform(tl_algo= prql,\n",
    "                                                                                       episode_num = EPISODE_NUM,\n",
    "                                                                                       repeat_times = REPEAT_TIMES,\n",
    "                                                                                       target_task = \"Taxi-v003\",\n",
    "                                                                                                                                                                            policy_library =  qlearn_q_table_v004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 112000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-9fc5cc68f80b>\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(target_rl_algo, episode_num, repeat_times, target_task, source_task, source_rl_algo, policy_library)\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0mepisodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpenalties\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfrequency\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mknowledge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_rl_algo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_episodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast_policies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m                 \u001b[0mepisodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpenalties\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mknowledge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_rl_algo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_episodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast_policies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;31m#Situation 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0msource_task\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-50-0c97c6ab7cc8>\u001b[0m in \u001b[0;36mOPS_TL\u001b[0;34m(env, train_episodes, rng, past_policies)\u001b[0m\n\u001b[1;32m     50\u001b[0m             episode_penalties, episode_rewards, episode_length,i = policy_reuse(timesteps, env, q_table, state, i,\n\u001b[1;32m     51\u001b[0m                                                                               \u001b[0mall_episodes_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mall_penalties\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mall_rewards\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                                                                               past_policy = past_policies[j])\n\u001b[0m\u001b[1;32m     53\u001b[0m             \u001b[0mpolicy_expect\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpolicy_expect\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpolicy_times\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mepisode_rewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mpolicy_times\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mpolicy_times\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-5d9eb6ae084e>\u001b[0m in \u001b[0;36mpolicy_reuse\u001b[0;34m(timesteps, env, q_table, initial_state, i, all_episodes_length, all_penalties, all_rewards, past_policy, fi, mu, alpha, gamma)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mepisode_penalties\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_rewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0m_env_copy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtimesteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mfi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spinningup/lib/python3.6/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    178\u001b[0m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_reconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;31m# If is its own copy, don't memoize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spinningup/lib/python3.6/copy.py\u001b[0m in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdeep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__setstate__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spinningup/lib/python3.6/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spinningup/lib/python3.6/copy.py\u001b[0m in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0mmemo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spinningup/lib/python3.6/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    178\u001b[0m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_reconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;31m# If is its own copy, don't memoize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spinningup/lib/python3.6/copy.py\u001b[0m in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdeep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__setstate__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spinningup/lib/python3.6/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spinningup/lib/python3.6/copy.py\u001b[0m in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0mmemo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spinningup/lib/python3.6/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spinningup/lib/python3.6/copy.py\u001b[0m in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0mmemo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spinningup/lib/python3.6/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spinningup/lib/python3.6/copy.py\u001b[0m in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0mmemo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spinningup/lib/python3.6/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/spinningup/lib/python3.6/copy.py\u001b[0m in \u001b[0;36m_deepcopy_list\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0mmemo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0mappend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m         \u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#ops_v014_003\n",
    "tl_episodes_length_v014_003, tl_penalties_v014_003,tl_rewards_v014_003,tl_fre_v014_003, tl_q_table_v014_003 = transform(tl_algo= OPS_TL,\n",
    "                                                                                       episode_num = EPISODE_NUM,\n",
    "                                                                                       repeat_times = REPEAT_TIMES,\n",
    "                                                                                       target_task = \"Taxi-v003\",\n",
    "                                                                                      \n",
    "                                                                                       policy_library = [[q_func1,q_func2,q_func3 ] for q_func1,q_func2,q_func3 in zip(qlearn_q_table_v000, qlearn_q_table_v001, qlearn_q_table_v004)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "taxiEnvs = [gym.make(\"Taxi-v000\"),gym.make(\"Taxi-v001\"),None,gym.make(\"Taxi-v003\"),gym.make(\"Taxi-v004\")]\n",
    "for i in [0,1,3,4]:\n",
    "    _, _ ,nrewards[i] ,option_policies[i], option_terminations[i], policy_over_options[i],critic[i]= oc( taxiEnvs[i], 4000, noptions=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "source_lib = [0,1,4]\n",
    "_,_, trans_rewards_012, _, _, _,_= oc( four_room_envs[3], 4000, [option_policies[i][0] for i in source_lib], noptions=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(run_times, q_table = None ,task = \"Taxi-v003\", mode = 'learning agent', show_frame = True):\n",
    "    \"\"\"Evaluate agent's performance after policy updates\"\"\"\n",
    "    env = gym.make(task)\n",
    "    if q_table is None:\n",
    "        list_length = 1\n",
    "    else:\n",
    "        list_length = len(q_table)\n",
    "    all_epochs, all_penalties, all_rewards, all_success_rate =  np.zeros(list_length), np.zeros(list_length),np.zeros(list_length),np.zeros(list_length)\n",
    "    all_epochs_std, all_penalties_std, all_rewards_std = np.zeros(list_length), np.zeros(list_length),np.zeros(list_length)\n",
    "    \n",
    "    \n",
    "        \n",
    "    for i in range( list_length ):\n",
    "        table_epochs, table_penalties, table_rewards = np.zeros(run_times), np.zeros(run_times),np.zeros(run_times)\n",
    "        if q_table is not None:\n",
    "            table = q_table[i]\n",
    "        success = 0\n",
    "        for j in range(run_times):\n",
    "            state = env.reset()\n",
    "            epochs, penalties, reward, sum_reward = 0, 0, 0, 0\n",
    "\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "                if mode == 'learning agent':\n",
    "                    action = arg_max(table[state])\n",
    "                elif mode == 'random agent':\n",
    "                    action = np.random.randint(0,env.action_space.n)\n",
    "                state, reward, done, info = env.step(action)\n",
    "\n",
    "                if reward == -10:\n",
    "                    penalties += 1\n",
    "                elif reward == 20:\n",
    "                    success += 1\n",
    "\n",
    "                epochs += 1\n",
    "                sum_reward += reward\n",
    "                if show_frame:\n",
    "                    show_frames(env,j, epochs,sum_reward )\n",
    "            table_penalties[j] = penalties\n",
    "            table_epochs[j] = epochs\n",
    "            table_rewards[j] = sum_reward\n",
    "\n",
    "        all_success_rate[i] = success/run_times    \n",
    "        all_epochs[i], all_penalties[i], all_rewards[i]  = np.mean(table_epochs), np.mean(table_penalties), np.mean(table_rewards)\n",
    "        all_epochs_std[i], all_penalties_std[i], all_rewards_std[i] = np.std(table_epochs), np.std(table_penalties), np.std(table_rewards)\n",
    "      \n",
    "    print(f\"Results over {run_times} evaluating episodes:\")\n",
    "    print(f\"Success rate : {np.mean(all_success_rate)}\")\n",
    "    print(f\"Average  episode length : {np.mean(all_epochs)} ± {np.mean(all_epochs_std)}\")\n",
    "    print(f\"Average penalties per episode: {np.mean(all_penalties)} ± {np.mean(all_penalties_std)}\")\n",
    "    print(f\"Average rewards per episode: {np.mean(all_rewards)} ± {np.mean(all_rewards_std)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def op_evaluate(run_times,option_policies, option_terminations, policy_over_options, q_table = None ,task = \"Taxi-v003\", mode = 'learning agent'):\n",
    "    \"\"\"Evaluate agent's performance after policy updates\"\"\"\n",
    "    env = gym.make(task)\n",
    "    if q_table is None:\n",
    "        list_length = 1\n",
    "    else:\n",
    "        list_length = len(q_table)\n",
    "    all_epochs, all_penalties, all_rewards, all_success_rate =  np.zeros(list_length), np.zeros(list_length),np.zeros(list_length),np.zeros(list_length)\n",
    "    all_epochs_std, all_penalties_std, all_rewards_std = np.zeros(list_length), np.zeros(list_length),np.zeros(list_length)\n",
    "    \n",
    "    \n",
    "        \n",
    "    for i in range( list_length ):\n",
    "        table_epochs, table_penalties, table_rewards = np.zeros(run_times), np.zeros(run_times),np.zeros(run_times)\n",
    "        if q_table is not None:\n",
    "            table = q_table[i]\n",
    "        success = 0\n",
    "        for j in range(run_times):\n",
    "            state = env.reset()\n",
    "            epochs, penalties, reward, sum_reward = 0, 0, 0, 0\n",
    "\n",
    "            done = False\n",
    "            option = policy_over_options.evaluate(state)\n",
    "            while not done:\n",
    "                if mode == 'learning agent':\n",
    "                    if option_terminations[option].sample(state):\n",
    "                        option = policy_over_options.evaluate(state)\n",
    "\n",
    "                    action = option_policies[option].evaluate(state)\n",
    "                    \n",
    "                    \n",
    "                elif mode == 'random agent':\n",
    "                    action = np.random.randint(0,env.action_space.n)\n",
    "                state, reward, done, info = env.step(action)\n",
    "\n",
    "                if reward == -10:\n",
    "                    penalties += 1\n",
    "                elif reward == 20:\n",
    "                    success += 1\n",
    "\n",
    "                epochs += 1\n",
    "                sum_reward += reward\n",
    "                show_frames(env,j,epochs,sum_reward )\n",
    "            table_penalties[j] = penalties\n",
    "            table_epochs[j] = epochs\n",
    "            table_rewards[j] = sum_reward\n",
    "\n",
    "        all_success_rate[i] = success/run_times    \n",
    "        all_epochs[i], all_penalties[i], all_rewards[i]  = np.mean(table_epochs), np.mean(table_penalties), np.mean(table_rewards)\n",
    "        all_epochs_std[i], all_penalties_std[i], all_rewards_std[i] = np.std(table_epochs), np.std(table_penalties), np.std(table_rewards)\n",
    "      \n",
    "    print(f\"Results after {run_times} runs:\")\n",
    "    print(f\"Success rate : {np.mean(all_success_rate)}\")\n",
    "    print(f\"Average  episode length : {np.mean(all_epochs)} ± {np.mean(all_epochs_std)}\")\n",
    "    print(f\"Average penalties per episode: {np.mean(all_penalties)} ± {np.mean(all_penalties_std)}\")\n",
    "    print(f\"Average rewards per episode: {np.mean(all_rewards)} ± {np.mean(all_rewards_std)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 79800\n",
      "Training finished\n",
      "CPU times: user 12.3 s, sys: 675 ms, total: 13 s\n",
      "Wall time: 12.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#ops_v0_003\n",
    "ops_episodes_length_v0_003, ops_penalties_v0_003,ops_rewards_v0_003,ops_fre_v0_003, ops_q_table_v0_003 = transform(tl_algo= caps,\n",
    "                                                                                       episode_num = EPISODE_NUM//10,\n",
    "                                                                                       repeat_times = REPEAT_TIMES,\n",
    "                                                                                       target_task = \"Taxi-v003\",\n",
    "                                                                                      \n",
    "                                                                                       policy_library = [[q_func1] for q_func1 in qlearn_q_table_v000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 79800\n",
      "Training finished\n",
      "CPU times: user 13 s, sys: 527 ms, total: 13.5 s\n",
      "Wall time: 13.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#ops_v1_003\n",
    "ops_episodes_length_v1_003, ops_penalties_v1_003,ops_rewards_v1_003,ops_fre_v1_003, ops_q_table_v1_003 = transform(tl_algo= caps,\n",
    "                                                                                       episode_num = EPISODE_NUM//10,\n",
    "                                                                                       repeat_times = REPEAT_TIMES,\n",
    "                                                                                       target_task = \"Taxi-v003\",\n",
    "                                                                                      \n",
    "                                                                                       policy_library = [[q_func1] for q_func1 in qlearn_q_table_v001])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 399\n",
      "Training finished \n",
      "\n",
      "CPU times: user 2.21 s, sys: 106 µs, total: 2.21 s\n",
      "Wall time: 2.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pr_episodes_length_v1_003, pr_penalties_v1_003,pr_rewards_v1_003, _, pr_q_table_v1_003 = transform(tl_algo= prql,\n",
    "                                                                                       episode_num = EPISODE_NUM//10,\n",
    "                                                                                       repeat_times = REPEAT_TIMES,\n",
    "                                                                                       target_task = \"Taxi-v003\",\n",
    "                                                                                       \n",
    "                                                                                       policy_library =  qlearn_q_table_v001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps: 79800\n",
      "Training finished\n",
      "CPU times: user 3.7 s, sys: 455 ms, total: 4.15 s\n",
      "Wall time: 3.81 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "qlearn_episodes_length_v3, qlearn_penalties_v3, qlearn_rewards_v3, _, qlearn_q_table_v3 = transform(tl_algo = q_learning,\n",
    "                                                                                     episode_num = EPISODE_NUM//10, \n",
    "                                                                                     repeat_times = 1,\n",
    "                                                                                     target_task = \"Taxi-v003\",\n",
    "                                                                                     source_task = None\n",
    "                                                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56d5b099cde549529106f6714c0b7732",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show>"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "RATIO = 10\n",
    "SMOOTH_RADIUS = 20\n",
    "plt.plot(range(EPISODE_NUM//RATIO),smooth(qlearn_rewards_v3[:EPISODE_NUM//RATIO],SMOOTH_RADIUS), label = 'qlearn_v3')\n",
    "\n",
    "plt.plot(range(EPISODE_NUM//RATIO),smooth(pr_rewards_v1_003[:EPISODE_NUM//RATIO],SMOOTH_RADIUS), label = 'pr_v1_3')\n",
    "plt.plot(range(EPISODE_NUM//RATIO),smooth(ops_rewards_v1_003[:EPISODE_NUM//RATIO],SMOOTH_RADIUS), label = 'ops_v1_3')\n",
    "\n",
    "plt.title(\"episode-penalties graph\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"penalties\")\n",
    "plt.legend()\n",
    "plt.show\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results over 1000 evaluating episodes:\n",
      "Success rate : 0.779\n",
      "Average  episode length : 84.576 ± 75.92020958875179\n",
      "Average penalties per episode: 0.0 ± 0.0\n",
      "Average rewards per episode: 15.58 ± 8.298409486160585\n"
     ]
    }
   ],
   "source": [
    "evaluate(1000,qlearn_q_table_v3,show_frame = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "episode 3  step 6  rewards=20\n",
      "Results over 3 evaluating episodes:\n",
      "Success rate : 1.0\n",
      "Average  episode length : 17.0 ± 9.41629792788369\n",
      "Average penalties per episode: 0.0 ± 0.0\n",
      "Average rewards per episode: 20.0 ± 0.0\n"
     ]
    }
   ],
   "source": [
    "evaluate(3,ops_q_table_v1_003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results over 1000 evaluating episodes:\n",
      "Success rate : 0.988\n",
      "Average  episode length : 18.847 ± 25.006750908504685\n",
      "Average penalties per episode: 0.0 ± 0.0\n",
      "Average rewards per episode: 19.76 ± 2.1777052142105915\n"
     ]
    }
   ],
   "source": [
    "evaluate(3,pr_q_table_v1_003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results over 1000 evaluating episodes:\n",
      "Success rate : 0.706\n",
      "Average  episode length : 85.098 ± 82.33279052722554\n",
      "Average penalties per episode: 0.0 ± 0.0\n",
      "Average rewards per episode: 14.12 ± 9.111838453352869\n"
     ]
    }
   ],
   "source": [
    "evaluate(1000,pr_q_table_v1_003,show_frame = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(1000, mode = 'random agent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# training Taxi-v3 reusing policy from Taxi-v1\n",
    "pr1_rewards_v1_3 = np.zeros(10000)\n",
    "for i in range(10):\n",
    "    pr1_episodes_length_v1_3, pr1_penalties_v1_3,rewards, pr1_q_table_v1_3 = prql(gym.make(\"Taxi-v3\"), qlearn_q_table_v1[i], 10000)\n",
    "    pr1_rewards_v1_3 += rewards\n",
    "pr1_rewards_v1_3/=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# training Taxi-v3 reusing policy from Taxi-v1\n",
    "pr1_rewards_v4_3 = np.zeros(10000)\n",
    "for i in range(10):\n",
    "    pr1_episodes_length_v4_3, pr1_penalties_v4_3,rewards, pr1_q_table_v4_3 = prql(gym.make(\"Taxi-v3\"), qlearn_q_table_v4[i], 10000)\n",
    "    pr1_rewards_v4_3 += rewards\n",
    "pr1_rewards_v4_3/=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "env = gym.make(\"Taxi-v4\")\n",
    "# training Taxi-v4 from scratch\n",
    "lprofiler = LineProfiler(q_learning)\n",
    "lprofiler.run('qlearn_episodes_length_v41, qlearn_penalties_v41, qlearn_rewards_v41, qlearn_q_table_v41 = q_learning(env,np.zeros([env.observation_space.n,env.action_space.n]),EPISODE_NUM)')\n",
    "lprofiler.print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spinningup",
   "language": "python",
   "name": "spinningup"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
